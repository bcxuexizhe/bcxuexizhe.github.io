<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AI大模型探索之路-训练篇16：大语言模型预训练-微调技术之LoRA - 编程学习者</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcxuexizhe.github.io/posts/4942899091c7fb3946e0f9ab17765ece/">
  <meta property="og:site_name" content="编程学习者">
  <meta property="og:title" content="AI大模型探索之路-训练篇16：大语言模型预训练-微调技术之LoRA">
  <meta property="og:description" content="系列篇章💥 AI大模型探索之路-训练篇1：大语言模型微调基础认知
AI大模型探索之路-训练篇2：大语言模型预训练基础认知
AI大模型探索之路-训练篇3：大语言模型全景解读
AI大模型探索之路-训练篇4：大语言模型训练数据集概览
AI大模型探索之路-训练篇5：大语言模型预训练数据准备-词元化
AI大模型探索之路-训练篇6：大语言模型预训练数据准备-预处理
AI大模型探索之路-训练篇7：大语言模型Transformer库之HuggingFace介绍
AI大模型探索之路-训练篇8：大语言模型Transformer库-预训练流程编码体验
AI大模型探索之路-训练篇9：大语言模型Transformer库-Pipeline组件实践
AI大模型探索之路-训练篇10：大语言模型Transformer库-Tokenizer组件实践
AI大模型探索之路-训练篇11：大语言模型Transformer库-Model组件实践
AI大模型探索之路-训练篇12：语言模型Transformer库-Datasets组件实践
AI大模型探索之路-训练篇13：大语言模型Transformer库-Evaluate组件实践
AI大模型探索之路-训练篇14：大语言模型Transformer库-Trainer组件实践
AI大模型探索之路-训练篇15：大语言模型预训练之全量参数微调
目录 系列篇章💥前言一、微调技术分类二、LoRA原理三、在哪儿增加旁路四、为什么微调少量参数就可以五、如何对A和B进行初始化六、增加旁路会增加推理时间吗？七、R值为多少合适八、如何注入LoRA九、LoRA代码实践学术资源加速步骤1 导入相关包步骤2 加载数据集步骤3 数据集预处理步骤4 创建模型1、PEFT 步骤1 配置文件2、PEFT 步骤2 创建模型 步骤5 配置训练参数步骤6 创建训练器步骤7 模型训练步骤8 模型推理 十、主路合并旁路1、加载基础模型2、加载LoRA模型3、模型推理4、模型合并5、模型推理6、完整模型保存 总结 前言 在自然语言处理领域，大语言模型的预训练-微调技术已经成为一种常见的方法。其中，LoRA（Low-Rank Adaptation）是一种新颖的微调技术，通过引入低秩矩阵来调整模型的行为，以提高模型在新任务上的表现。本文将对LoRA的原理、优势以及应用进行详细介绍。
一、微调技术分类 微调技术主要分为以下几类：
1）增加额外参数（A）：这种方法是在原有的预训练模型的基础上增加一些额外的参数，以改变模型的行为。
2）选取一部分参数更新（S）：这种方法是在微调过程中只更新模型的一部分参数，而不是所有参数。这可以减少计算量，提高微调效率。
3）引入重参数化（R）：这种方法是在模型的参数空间中引入一些新的变化，通常是一些线性变换或非线性变换，以改变模型的行为。这种方法可以使模型在新任务上有更好的表现。
常见的参数高效微调技术有Prefix Tuning、Prompt Tuning、P-Tuning、Adapter Tuning、LoRA等
二、LoRA原理 LoRA（Low-Rank Adaptation:低秩的适配器）是一种新颖的微调技术，它通过引入低秩矩阵来调整模型的行为，以提高模型在新任务上的表现。具体来说，LoRA在原有的预训练模型中增加了两个旁路矩阵A和B，这两个矩阵的维度远小于原始模型的输入输出维度，从而实现了参数的高效微调。
三、在哪儿增加旁路 在原有的预训练模型中，可以选择在任意两个相邻层之间增加旁路矩阵A和B。这样，模型在前向传播过程中，可以通过这两个旁路矩阵来引入新的信息，从而改变模型的行为。
四、为什么微调少量参数就可以 A的输入维度和B的输出维度分别与原始模型的输入输出维度相同，而A的输出维度和B的输入维度是一个远小于原始模型输入输出维度的值，这就是low-rank的体现，可以极大地减少待训练的参数
秩表示的是矩阵的信息量，这里的“秩”特指引入的旁路矩阵的规模，即它们的行数和列数。
在LoRA技术中，我们通过引入低秩矩阵来调整预训练模型的行为，同时保留大部分原有的参数不变。这样做可以在不牺牲太多性能的前提下，显著降低模型微调时的计算成本和内存需求。
通俗化解释：“秩”:
想象一下你有一个很大的包裹，你需要通过一个小门把它送出去。但是门太小了，你必须把包裹拆成几个小包裹才能通过。在这个比喻中，大包裹就像模型的权重矩阵，小门就像我们新增的低秩矩阵，而“秩”就是这些小包裹的数量。在LoRA中，我们通过创建一些小的（低秩）矩阵来传递信息，而不是使用原始的大矩阵。这样做的好处是我们可以只关注那些最重要的信息，忽略掉不重要的信息，从而减少计算量和内存需求。
五、如何对A和B进行初始化 A和B如何初始化？
对A采用高斯初始化，对B采用零初始化的目的是，让训练刚开始时的值为0，这样不会给模型带来额外的噪声。
六、增加旁路会增加推理时间吗？ 虽然增加了旁路矩阵A和B，但是由于它们的维度远小于原始模型的输入输出维度，因此在推理过程中，计算量的增加是非常有限的。
七、R值为多少合适 R值表示的是旁路矩阵A和B的秩。一般来说，R值的选择需要根据具体任务和模型结构来确定。在实际应用中，可以尝试不同的R值，以找到最佳的设置。
八、如何注入LoRA 要将LoRA应用于现有的预训练模型中，首先需要在相邻层之间插入旁路矩阵A和B。然后，在微调过程中，只需要调整这两个旁路矩阵的参数即可。这样，就可以实现模型行为的高效调整。
如上图中定义一个简单的3层的神经网络，在第1层增加旁路后效果如下：
九、LoRA代码实践 PEFT文档资料地址
1）文档地址：https://huggingface.co/docs/peft/index">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-07T09:17:52+08:00">
    <meta property="article:modified_time" content="2024-05-07T09:17:52+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程学习者" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程学习者</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AI大模型探索之路-训练篇16：大语言模型预训练-微调技术之LoRA</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_2"></a>系列篇章💥</h2> 
<p><a href="https://xundaomalu.blog.csdn.net/article/details/138107946" rel="nofollow">AI大模型探索之路-训练篇1：大语言模型微调基础认知</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138143923" rel="nofollow">AI大模型探索之路-训练篇2：大语言模型预训练基础认知</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138161057" rel="nofollow">AI大模型探索之路-训练篇3：大语言模型全景解读</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138205204" rel="nofollow">AI大模型探索之路-训练篇4：大语言模型训练数据集概览</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138225299" rel="nofollow">AI大模型探索之路-训练篇5：大语言模型预训练数据准备-词元化</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138267915" rel="nofollow">AI大模型探索之路-训练篇6：大语言模型预训练数据准备-预处理</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138294519" rel="nofollow">AI大模型探索之路-训练篇7：大语言模型Transformer库之HuggingFace介绍</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138348834" rel="nofollow">AI大模型探索之路-训练篇8：大语言模型Transformer库-预训练流程编码体验</a><br> <a href="https://blog.csdn.net/xiaobing259/article/details/138373677">AI大模型探索之路-训练篇9：大语言模型Transformer库-Pipeline组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138391592" rel="nofollow">AI大模型探索之路-训练篇10：大语言模型Transformer库-Tokenizer组件实践</a><br> <a href="https://blog.csdn.net/xiaobing259/article/details/138424867">AI大模型探索之路-训练篇11：大语言模型Transformer库-Model组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138426216" rel="nofollow">AI大模型探索之路-训练篇12：语言模型Transformer库-Datasets组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138448172" rel="nofollow">AI大模型探索之路-训练篇13：大语言模型Transformer库-Evaluate组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138448511" rel="nofollow">AI大模型探索之路-训练篇14：大语言模型Transformer库-Trainer组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138472105" rel="nofollow">AI大模型探索之路-训练篇15：大语言模型预训练之全量参数微调</a></p> 
<hr> 
<p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#_2" rel="nofollow">系列篇章💥</a></li><li><a href="#_23" rel="nofollow">前言</a></li><li><a href="#_25" rel="nofollow">一、微调技术分类</a></li><li><a href="#LoRA_35" rel="nofollow">二、LoRA原理</a></li><li><a href="#_39" rel="nofollow">三、在哪儿增加旁路</a></li><li><a href="#_44" rel="nofollow">四、为什么微调少量参数就可以</a></li><li><a href="#AB_58" rel="nofollow">五、如何对A和B进行初始化</a></li><li><a href="#_64" rel="nofollow">六、增加旁路会增加推理时间吗？</a></li><li><a href="#R_69" rel="nofollow">七、R值为多少合适</a></li><li><a href="#LoRA_73" rel="nofollow">八、如何注入LoRA</a></li><li><a href="#LoRA_80" rel="nofollow">九、LoRA代码实践</a></li><li><ul><li><a href="#_93" rel="nofollow">学术资源加速</a></li><li><a href="#1__108" rel="nofollow">步骤1 导入相关包</a></li><li><a href="#2__117" rel="nofollow">步骤2 加载数据集</a></li><li><a href="#3__147" rel="nofollow">步骤3 数据集预处理</a></li><li><a href="#4__215" rel="nofollow">步骤4 创建模型</a></li><li><ul><li><a href="#1PEFT_1__575" rel="nofollow">1、PEFT 步骤1 配置文件</a></li><li><a href="#2PEFT_2__586" rel="nofollow">2、PEFT 步骤2 创建模型</a></li></ul> 
   </li><li><a href="#5__665" rel="nofollow">步骤5 配置训练参数</a></li><li><a href="#6__679" rel="nofollow">步骤6 创建训练器</a></li><li><a href="#7__691" rel="nofollow">步骤7 模型训练</a></li><li><a href="#8__699" rel="nofollow">步骤8 模型推理</a></li></ul> 
  </li><li><a href="#_717" rel="nofollow">十、主路合并旁路</a></li><li><ul><li><a href="#1_718" rel="nofollow">1、加载基础模型</a></li><li><a href="#2LoRA_729" rel="nofollow">2、加载LoRA模型</a></li><li><a href="#3_794" rel="nofollow">3、模型推理</a></li><li><a href="#4_806" rel="nofollow">4、模型合并</a></li><li><a href="#5_841" rel="nofollow">5、模型推理</a></li><li><a href="#6_851" rel="nofollow">6、完整模型保存</a></li></ul> 
  </li><li><a href="#_858" rel="nofollow">总结</a></li></ul> 
</div> 
<p></p> 
<hr> 
<h2><a id="_23"></a>前言</h2> 
<p>在自然语言处理领域，大语言模型的预训练-微调技术已经成为一种常见的方法。其中，LoRA（Low-Rank Adaptation）是一种新颖的微调技术，通过引入低秩矩阵来调整模型的行为，以提高模型在新任务上的表现。本文将对LoRA的原理、优势以及应用进行详细介绍。</p> 
<h2><a id="_25"></a>一、微调技术分类</h2> 
<p>微调技术主要分为以下几类：<br> <strong>1）增加额外参数（A</strong>）：这种方法是在原有的预训练模型的基础上增加一些额外的参数，以改变模型的行为。<br> <strong>2）选取一部分参数更新（S）</strong>：这种方法是在微调过程中只更新模型的一部分参数，而不是所有参数。这可以减少计算量，提高微调效率。<br> <strong>3）引入重参数化（R）</strong>：这种方法是在模型的参数空间中引入一些新的变化，通常是一些线性变换或非线性变换，以改变模型的行为。这种方法可以使模型在新任务上有更好的表现。</p> 
<blockquote> 
 <p>常见的参数高效微调技术有Prefix Tuning、Prompt Tuning、P-Tuning、Adapter Tuning、LoRA等<br> <img src="https://images2.imgbox.com/a3/51/6XTc00l0_o.png" alt="在这里插入图片描述"></p> 
</blockquote> 
<h2><a id="LoRA_35"></a>二、LoRA原理</h2> 
<p>LoRA（Low-Rank Adaptation:低秩的适配器）是一种新颖的微调技术，它通过引入低秩矩阵来调整模型的行为，以提高模型在新任务上的表现。具体来说，LoRA在原有的预训练模型中增加了两个旁路矩阵A和B，这两个矩阵的维度远小于原始模型的输入输出维度，从而实现了参数的高效微调。<br> <img src="https://images2.imgbox.com/70/79/1OWOrCDO_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_39"></a>三、在哪儿增加旁路</h2> 
<p>在原有的预训练模型中，可以选择在任意两个相邻层之间增加旁路矩阵A和B。这样，模型在前向传播过程中，可以通过这两个旁路矩阵来引入新的信息，从而改变模型的行为。<br> <img src="https://images2.imgbox.com/8b/d5/twoiMw7I_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_44"></a>四、为什么微调少量参数就可以</h2> 
<p><img src="https://images2.imgbox.com/6b/47/jETLyFDa_o.png" alt="在这里插入图片描述"><br> A的输入维度和B的输出维度分别与原始模型的输入输出维度相同，而A的输出维度和B的输入维度是一个远小于原始模型输入输出维度的值，这就是low-rank的体现，可以极大地减少待训练的参数<br> <img src="https://images2.imgbox.com/4d/25/8bT2Tz9T_o.png" alt="在这里插入图片描述"></p> 
<p><strong>秩</strong>表示的是矩阵的信息量，这里的“<strong>秩</strong>”特指引入的旁路矩阵的规模，即它们的行数和列数。<br> <img src="https://images2.imgbox.com/dc/0c/TfXHy883_o.png" alt="在这里插入图片描述"></p> 
<p>在LoRA技术中，我们通过引入低秩矩阵来调整预训练模型的行为，同时保留大部分原有的参数不变。这样做可以在不牺牲太多性能的前提下，显著降低模型微调时的计算成本和内存需求。</p> 
<blockquote> 
 <p><strong>通俗化解释：“秩”</strong>:<br> 想象一下你有一个很大的包裹，你需要通过一个小门把它送出去。但是门太小了，你必须把包裹拆成几个小包裹才能通过。在这个比喻中，大包裹就像模型的权重矩阵，小门就像我们新增的低秩矩阵，而“秩”就是这些小包裹的数量。在LoRA中，我们通过创建一些小的（低秩）矩阵来传递信息，而不是使用原始的大矩阵。这样做的好处是我们可以只关注那些最重要的信息，忽略掉不重要的信息，从而减少计算量和内存需求。</p> 
</blockquote> 
<h2><a id="AB_58"></a>五、如何对A和B进行初始化</h2> 
<p>A和B如何初始化？<br> 对A采用高斯初始化，对B采用零初始化的目的是，让训练刚开始时的值为0，这样不会给模型带来额外的噪声。</p> 
<p><img src="https://images2.imgbox.com/10/d3/43d4w64Y_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_64"></a>六、增加旁路会增加推理时间吗？</h2> 
<p>虽然增加了旁路矩阵A和B，但是由于它们的维度远小于原始模型的输入输出维度，因此在推理过程中，计算量的增加是非常有限的。<br> <img src="https://images2.imgbox.com/fa/bb/Fgo5KfS5_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="R_69"></a>七、R值为多少合适</h2> 
<p>R值表示的是旁路矩阵A和B的秩。一般来说，R值的选择需要根据具体任务和模型结构来确定。在实际应用中，可以尝试不同的R值，以找到最佳的设置。</p> 
<p><img src="https://images2.imgbox.com/12/70/piGzvdBW_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="LoRA_73"></a>八、如何注入LoRA</h2> 
<p>要将LoRA应用于现有的预训练模型中，首先需要在相邻层之间插入旁路矩阵A和B。然后，在微调过程中，只需要调整这两个旁路矩阵的参数即可。这样，就可以实现模型行为的高效调整。<br> <img src="https://images2.imgbox.com/69/aa/PH5GP0yD_o.png" alt="在这里插入图片描述"></p> 
<p>如上图中定义一个简单的3层的神经网络，在第1层增加旁路后效果如下：<br> <img src="https://images2.imgbox.com/a5/0c/DoFz7WIB_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="LoRA_80"></a>九、LoRA代码实践</h2> 
<p>PEFT文档资料地址<br> 1）文档地址：https://huggingface.co/docs/peft/index<br> 2）Github地址：https://github.com/huggingface/peft<br> <a href="https://huggingface.co/docs/peft/index" rel="nofollow">PEFT</a>（Parameter-Efficient Fine-Tuning）库是一个用于参数高效微调预训练语言模型的库，旨在降低大规模模型微调的计算和存储成本。<br> PEFT库的核心优势在于它能够仅通过微调少量额外模型参数来适应各种下游任务，避免了对整个大模型参数进行微调的需求。这种方法不仅降低了资源消耗，而且在很多情况下能达到与完全微调相当的性能<br> <img src="https://images2.imgbox.com/64/d0/B03xau7r_o.png" alt="在这里插入图片描述"></p> 
<p>PEFT技术的支持：<br> <img src="https://images2.imgbox.com/91/d5/7ETPRfwU_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_93"></a>学术资源加速</h3> 
<p>方便从huggingface下载模型，这云平台<a href="https://www.autodl.com/" rel="nofollow">autodl</a>提供的，仅适用于autodl。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> subprocess
<span class="token keyword">import</span> os

result <span class="token operator">=</span> subprocess<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token string">'bash -c "source /etc/network_turbo &amp;&amp; env | grep proxy"'</span><span class="token punctuation">,</span> shell<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> capture_output<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> result<span class="token punctuation">.</span>stdout
<span class="token keyword">for</span> line <span class="token keyword">in</span> output<span class="token punctuation">.</span>splitlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token string">'='</span> <span class="token keyword">in</span> line<span class="token punctuation">:</span>
        var<span class="token punctuation">,</span> value <span class="token operator">=</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'='</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span>var<span class="token punctuation">]</span> <span class="token operator">=</span> value
</code></pre> 
<h3><a id="1__108"></a>步骤1 导入相关包</h3> 
<p>开始之前，我们需要导入适用于模型训练和推理的必要库，如transformers。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> datasets <span class="token keyword">import</span> Dataset
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForCausalLM<span class="token punctuation">,</span> DataCollatorForSeq2Seq<span class="token punctuation">,</span> TrainingArguments<span class="token punctuation">,</span> Trainer
</code></pre> 
<h3><a id="2__117"></a>步骤2 加载数据集</h3> 
<p>使用适当的数据加载器，例如datasets库，来加载预处理过的指令遵循性任务数据集。</p> 
<pre><code class="prism language-python">ds <span class="token operator">=</span> Dataset<span class="token punctuation">.</span>load_from_disk<span class="token punctuation">(</span><span class="token string">"/root/tuning/lesson01/data/alpaca_data_zh/"</span><span class="token punctuation">)</span>
ds
</code></pre> 
<p>输出</p> 
<pre><code class="prism language-python">Dataset<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span>
    features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'output'</span><span class="token punctuation">,</span> <span class="token string">'input'</span><span class="token punctuation">,</span> <span class="token string">'instruction'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    num_rows<span class="token punctuation">:</span> <span class="token number">26858</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> 
<p>数据查看</p> 
<pre><code class="prism language-python">ds<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span>
</code></pre> 
<p>输出</p> 
<pre><code class="prism language-python"><span class="token punctuation">{<!-- --></span><span class="token string">'output'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'以下是保持健康的三个提示：\n\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\n\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\n\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 <span class="token string">'input'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">''</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 <span class="token string">'instruction'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'保持健康的三个提示。'</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
</code></pre> 
<h3><a id="3__147"></a>步骤3 数据集预处理</h3> 
<p>利用预训练模型的分词器（Tokenizer）对原始文本进行编码，并生成相应的输入ID、注意力掩码和标签。<br> 1）获取分词器</p> 
<pre><code class="prism language-python">tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"Langboat/bloom-1b4-zh"</span><span class="token punctuation">)</span>
tokenizer
</code></pre> 
<p><img src="https://images2.imgbox.com/44/50/3Nxbw6uo_o.png" alt="在这里插入图片描述"></p> 
<p>输出：</p> 
<pre><code class="prism language-python">BloomTokenizerFast<span class="token punctuation">(</span>name_or_path<span class="token operator">=</span><span class="token string">'Langboat/bloom-1b4-zh'</span><span class="token punctuation">,</span> vocab_size<span class="token operator">=</span><span class="token number">46145</span><span class="token punctuation">,</span> model_max_length<span class="token operator">=</span><span class="token number">1000000000000000019884624838656</span><span class="token punctuation">,</span> is_fast<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> padding_side<span class="token operator">=</span><span class="token string">'left'</span><span class="token punctuation">,</span> truncation_side<span class="token operator">=</span><span class="token string">'right'</span><span class="token punctuation">,</span> special_tokens<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token string">'bos_token'</span><span class="token punctuation">:</span> <span class="token string">'&lt;s&gt;'</span><span class="token punctuation">,</span> <span class="token string">'eos_token'</span><span class="token punctuation">:</span> <span class="token string">'&lt;/s&gt;'</span><span class="token punctuation">,</span> <span class="token string">'unk_token'</span><span class="token punctuation">:</span> <span class="token string">'&lt;unk&gt;'</span><span class="token punctuation">,</span> <span class="token string">'pad_token'</span><span class="token punctuation">:</span> <span class="token string">'&lt;pad&gt;'</span><span class="token punctuation">}</span><span class="token punctuation">,</span> clean_up_tokenization_spaces<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  added_tokens_decoder<span class="token operator">=</span><span class="token punctuation">{<!-- --></span>
	<span class="token number">0</span><span class="token punctuation">:</span> AddedToken<span class="token punctuation">(</span><span class="token string">"&lt;unk&gt;"</span><span class="token punctuation">,</span> rstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> lstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> single_word<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> normalized<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> special<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
	<span class="token number">1</span><span class="token punctuation">:</span> AddedToken<span class="token punctuation">(</span><span class="token string">"&lt;s&gt;"</span><span class="token punctuation">,</span> rstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> lstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> single_word<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> normalized<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> special<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
	<span class="token number">2</span><span class="token punctuation">:</span> AddedToken<span class="token punctuation">(</span><span class="token string">"&lt;/s&gt;"</span><span class="token punctuation">,</span> rstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> lstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> single_word<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> normalized<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> special<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
	<span class="token number">3</span><span class="token punctuation">:</span> AddedToken<span class="token punctuation">(</span><span class="token string">"&lt;pad&gt;"</span><span class="token punctuation">,</span> rstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> lstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> single_word<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> normalized<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> special<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>2）定义数据处理函数</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">process_func</span><span class="token punctuation">(</span>example<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 设置最大长度为256</span>
    MAX_LENGTH <span class="token operator">=</span> <span class="token number">256</span>
    <span class="token comment"># 初始化输入ID、注意力掩码和标签列表</span>
    input_ids<span class="token punctuation">,</span> attention_mask<span class="token punctuation">,</span> labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token comment"># 对指令和输入进行编码</span>
    instruction <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"Human: "</span> <span class="token operator">+</span> example<span class="token punctuation">[</span><span class="token string">"instruction"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> example<span class="token punctuation">[</span><span class="token string">"input"</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"\n\nAssistant: "</span><span class="token punctuation">)</span>
    <span class="token comment"># 对输出进行编码，并添加结束符</span>
    response <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>example<span class="token punctuation">[</span><span class="token string">"output"</span><span class="token punctuation">]</span> <span class="token operator">+</span> tokenizer<span class="token punctuation">.</span>eos_token<span class="token punctuation">)</span>
    <span class="token comment"># 将指令和响应的输入ID拼接起来</span>
    input_ids <span class="token operator">=</span> instruction<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span> <span class="token operator">+</span> response<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span>
    <span class="token comment"># 将指令和响应的注意力掩码拼接起来</span>
    attention_mask <span class="token operator">=</span> instruction<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span> <span class="token operator">+</span> response<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span>
    <span class="token comment"># 将指令的标签设置为-100，表示不计算损失；将响应的输入ID作为标签</span>
    labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">100</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>instruction<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> response<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span>
    <span class="token comment"># 如果输入ID的长度超过最大长度，截断输入ID、注意力掩码和标签</span>
    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span> <span class="token operator">&gt;</span> MAX_LENGTH<span class="token punctuation">:</span>
        input_ids <span class="token operator">=</span> input_ids<span class="token punctuation">[</span><span class="token punctuation">:</span>MAX_LENGTH<span class="token punctuation">]</span>
        attention_mask <span class="token operator">=</span> attention_mask<span class="token punctuation">[</span><span class="token punctuation">:</span>MAX_LENGTH<span class="token punctuation">]</span>
        labels <span class="token operator">=</span> labels<span class="token punctuation">[</span><span class="token punctuation">:</span>MAX_LENGTH<span class="token punctuation">]</span>
    <span class="token comment"># 返回处理后的数据</span>
    <span class="token keyword">return</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">"input_ids"</span><span class="token punctuation">:</span> input_ids<span class="token punctuation">,</span>
        <span class="token string">"attention_mask"</span><span class="token punctuation">:</span> attention_mask<span class="token punctuation">,</span>
        <span class="token string">"labels"</span><span class="token punctuation">:</span> labels
    <span class="token punctuation">}</span>
</code></pre> 
<p>3）对数据进行预处理</p> 
<pre><code class="prism language-python">tokenized_ds <span class="token operator">=</span> ds<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>process_func<span class="token punctuation">,</span> remove_columns<span class="token operator">=</span>ds<span class="token punctuation">.</span>column_names<span class="token punctuation">)</span>
tokenized_ds
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">Dataset<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span>
    features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">,</span> <span class="token string">'attention_mask'</span><span class="token punctuation">,</span> <span class="token string">'labels'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    num_rows<span class="token punctuation">:</span> <span class="token number">26858</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="4__215"></a>步骤4 创建模型</h3> 
<p>然后，我们实例化一个预训练模型，这个模型将作为微调的基础。对于大型模型，我们可能还需要进行一些特定的配置，以适应可用的计算资源。</p> 
<pre><code class="prism language-python"><span class="token comment">#这行代码从Hugging Face Model Hub加载了一个预训练的Bloom模型，模型名称为"Langboat/bloom-1b4-zh"，并且设置了low_cpu_mem_usage=True以减少CPU内存使用。</span>
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"Langboat/bloom-1b4-zh"</span><span class="token punctuation">,</span> low_cpu_mem_usage<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
<p>查看总共有哪些层，可以基于这些层添加LoRA</p> 
<pre><code class="prism language-python"><span class="token keyword">for</span> name<span class="token punctuation">,</span> parameter <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">)</span>
</code></pre> 
<p>输出</p> 
<pre><code class="prism language-python">base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>word_embeddings<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>word_embeddings_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>word_embeddings_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>ln_f<span class="token punctuation">.</span>weight
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>ln_f<span class="token punctuation">.</span>bias
</code></pre> 
<p><strong>LoRA相关的配置（下面2个部分是LoRA相关的配置，其他的和全量微调代码一样）。</strong></p> 
<h4><a id="1PEFT_1__575"></a>1、PEFT 步骤1 配置文件</h4> 
<p>在使用PEFT进行微调时，我们首先需要创建一个配置文件，该文件定义了微调过程中的各种设置，如学习率调度、优化器选择等。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> peft <span class="token keyword">import</span> LoraConfig<span class="token punctuation">,</span> TaskType<span class="token punctuation">,</span> get_peft_model
config <span class="token operator">=</span> LoraConfig<span class="token punctuation">(</span>task_type<span class="token operator">=</span>TaskType<span class="token punctuation">.</span>CAUSAL_LM<span class="token punctuation">)</span>
<span class="token comment">##也可以不使用默认的，自己指定， 目标层 target_modules=["query_key_value"],秩 r=8</span>
<span class="token comment">#config = LoraConfig(task_type=TaskType.CAUSAL_LM,r=8, target_modules=['query_key_value','dense_4h_to_h'])</span>
config
</code></pre> 
<h4><a id="2PEFT_2__586"></a>2、PEFT 步骤2 创建模型</h4> 
<p>接下来，我们使用PEFT和预训练模型来创建一个微调模型。<font color="red">这个模型将包含原始的预训练模型以及由PEFT引入的低秩参数。</font></p> 
<pre><code class="prism language-python">model <span class="token operator">=</span> get_peft_model<span class="token punctuation">(</span>model<span class="token punctuation">,</span> config<span class="token punctuation">)</span>
model
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">PeftModelForCausalLM<span class="token punctuation">(</span>
  <span class="token punctuation">(</span>base_model<span class="token punctuation">)</span><span class="token punctuation">:</span> LoraModel<span class="token punctuation">(</span>
    <span class="token punctuation">(</span>model<span class="token punctuation">)</span><span class="token punctuation">:</span> PeftModelForCausalLM<span class="token punctuation">(</span>
      <span class="token punctuation">(</span>base_model<span class="token punctuation">)</span><span class="token punctuation">:</span> LoraModel<span class="token punctuation">(</span>
        <span class="token punctuation">(</span>model<span class="token punctuation">)</span><span class="token punctuation">:</span> BloomForCausalLM<span class="token punctuation">(</span>
          <span class="token punctuation">(</span>transformer<span class="token punctuation">)</span><span class="token punctuation">:</span> BloomModel<span class="token punctuation">(</span>
            <span class="token punctuation">(</span>word_embeddings<span class="token punctuation">)</span><span class="token punctuation">:</span> Embedding<span class="token punctuation">(</span><span class="token number">46145</span><span class="token punctuation">,</span> <span class="token number">2048</span><span class="token punctuation">)</span>
            <span class="token punctuation">(</span>word_embeddings_layernorm<span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
            <span class="token punctuation">(</span>h<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleList<span class="token punctuation">(</span>
              <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">-</span><span class="token number">23</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token number">24</span> x BloomBlock<span class="token punctuation">(</span>
                <span class="token punctuation">(</span>input_layernorm<span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
                <span class="token punctuation">(</span>self_attention<span class="token punctuation">)</span><span class="token punctuation">:</span> BloomAttention<span class="token punctuation">(</span>
                  <span class="token punctuation">(</span>query_key_value<span class="token punctuation">)</span><span class="token punctuation">:</span> lora<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>
                    <span class="token punctuation">(</span>base_layer<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">6144</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
                    <span class="token punctuation">(</span>lora_dropout<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleDict<span class="token punctuation">(</span>
                      <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token punctuation">:</span> Identity<span class="token punctuation">(</span><span class="token punctuation">)</span>
                    <span class="token punctuation">)</span>
                    <span class="token punctuation">(</span>lora_A<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleDict<span class="token punctuation">(</span>
                      <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
                    <span class="token punctuation">)</span>
                    <span class="token punctuation">(</span>lora_B<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleDict<span class="token punctuation">(</span>
                      <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">6144</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
                    <span class="token punctuation">)</span>
                    <span class="token punctuation">(</span>lora_embedding_A<span class="token punctuation">)</span><span class="token punctuation">:</span> ParameterDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
                    <span class="token punctuation">(</span>lora_embedding_B<span class="token punctuation">)</span><span class="token punctuation">:</span> ParameterDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
                  <span class="token punctuation">)</span>
                  <span class="token punctuation">(</span>dense<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
                  <span class="token punctuation">(</span>attention_dropout<span class="token punctuation">)</span><span class="token punctuation">:</span> Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
                <span class="token punctuation">)</span>
                <span class="token punctuation">(</span>post_attention_layernorm<span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
                <span class="token punctuation">(</span>mlp<span class="token punctuation">)</span><span class="token punctuation">:</span> BloomMLP<span class="token punctuation">(</span>
                  <span class="token punctuation">(</span>dense_h_to_4h<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">8192</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
                  <span class="token punctuation">(</span>gelu_impl<span class="token punctuation">)</span><span class="token punctuation">:</span> BloomGelu<span class="token punctuation">(</span><span class="token punctuation">)</span>
                  <span class="token punctuation">(</span>dense_4h_to_h<span class="token punctuation">)</span><span class="token punctuation">:</span> lora<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>
                    <span class="token punctuation">(</span>base_layer<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">8192</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
                    <span class="token punctuation">(</span>lora_dropout<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleDict<span class="token punctuation">(</span>
                      <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token punctuation">:</span> Identity<span class="token punctuation">(</span><span class="token punctuation">)</span>
                    <span class="token punctuation">)</span>
                    <span class="token punctuation">(</span>lora_A<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleDict<span class="token punctuation">(</span>
                      <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">8192</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
                    <span class="token punctuation">)</span>
                    <span class="token punctuation">(</span>lora_B<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleDict<span class="token punctuation">(</span>
                      <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
                    <span class="token punctuation">)</span>
                    <span class="token punctuation">(</span>lora_embedding_A<span class="token punctuation">)</span><span class="token punctuation">:</span> ParameterDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
                    <span class="token punctuation">(</span>lora_embedding_B<span class="token punctuation">)</span><span class="token punctuation">:</span> ParameterDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
                  <span class="token punctuation">)</span>
                <span class="token punctuation">)</span>
              <span class="token punctuation">)</span>
            <span class="token punctuation">)</span>
            <span class="token punctuation">(</span>ln_f<span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
          <span class="token punctuation">)</span>
          <span class="token punctuation">(</span>lm_head<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">46145</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
      <span class="token punctuation">)</span>
    <span class="token punctuation">)</span>
  <span class="token punctuation">)</span>
<span class="token punctuation">)</span>
</code></pre> 
<p>查看配置</p> 
<pre><code class="prism language-python">config
</code></pre> 
<p>输出</p> 
<pre><code class="prism language-python">LoraConfig<span class="token punctuation">(</span>peft_type<span class="token operator">=</span><span class="token operator">&lt;</span>PeftType<span class="token punctuation">.</span>LORA<span class="token punctuation">:</span> <span class="token string">'LORA'</span><span class="token operator">&gt;</span><span class="token punctuation">,</span> auto_mapping<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> base_model_name_or_path<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> revision<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> task_type<span class="token operator">=</span><span class="token operator">&lt;</span>TaskType<span class="token punctuation">.</span>CAUSAL_LM<span class="token punctuation">:</span> <span class="token string">'CAUSAL_LM'</span><span class="token operator">&gt;</span><span class="token punctuation">,</span> inference_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> r<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> target_modules<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token string">'query_key_value'</span><span class="token punctuation">,</span> <span class="token string">'dense_4h_to_h'</span><span class="token punctuation">}</span><span class="token punctuation">,</span> lora_alpha<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> lora_dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> fan_in_fan_out<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">,</span> modules_to_save<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> init_lora_weights<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> layers_to_transform<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> layers_pattern<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> rank_pattern<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">,</span> alpha_pattern<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">,</span> megatron_config<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> megatron_core<span class="token operator">=</span><span class="token string">'megatron.core'</span><span class="token punctuation">,</span> loftq_config<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="5__665"></a>步骤5 配置训练参数</h3> 
<p>定义训练参数，包括输出目录、学习率、批次大小、梯度累积步数、优化器选择等。</p> 
<pre><code class="prism language-python">args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
    output_dir<span class="token operator">=</span><span class="token string">"/root/autodl-tmp/tuningdata/lora"</span><span class="token punctuation">,</span><span class="token comment"># 指定模型训练结果的输出目录。</span>
    per_device_train_batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token comment"># 指定每个设备（如GPU）上的批次大小</span>
    gradient_accumulation_steps<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token comment"># 指定梯度累积步数。在本例子中，每8个步骤进行一次梯度更新。</span>
    logging_steps<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token comment">#指定日志记录的频率。在本例子中，每20个步骤记录一次日志</span>
    num_train_epochs<span class="token operator">=</span><span class="token number">4</span> <span class="token comment">#指定训练的总轮数</span>
<span class="token punctuation">)</span>

</code></pre> 
<h3><a id="6__679"></a>步骤6 创建训练器</h3> 
<p>最后，我们创建一个训练器实例，它封装了训练循环。训练器将负责运行训练过程，并根据我们之前定义的参数进行优化。</p> 
<pre><code class="prism language-python">trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>
    model<span class="token operator">=</span>model<span class="token punctuation">,</span><span class="token comment">#指定训练模型</span>
    args<span class="token operator">=</span>args<span class="token punctuation">,</span> <span class="token comment">#指定训练参数</span>
    train_dataset<span class="token operator">=</span>tokenized_ds<span class="token punctuation">,</span> <span class="token comment">#指定数据集</span>
    data_collator<span class="token operator">=</span>DataCollatorForSeq2Seq<span class="token punctuation">(</span>tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment">#指定数据收集器。其中tokenizer是分词器，padding=True表示对输入进行填充以保持批次大小一致。</span>
<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="7__691"></a>步骤7 模型训练</h3> 
<p>通过调用训练器的<code>train()</code>方法，我们启动模型的训练过程。</p> 
<pre><code class="prism language-python">trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="8__699"></a>步骤8 模型推理</h3> 
<p>训练完成后，我们可以使用训练好的模型进行推理。这通常涉及到使用模型的<code>inference</code>方法，输入经过适当处理的问题，并得到模型的输出。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

pipe <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"text-generation"</span><span class="token punctuation">,</span> model<span class="token operator">=</span>model<span class="token punctuation">,</span> tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

ipt <span class="token operator">=</span> <span class="token string">"Human: {}\n{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token string">"如何写好一个简历？"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"\n\nAssistant: "</span>
pipe<span class="token punctuation">(</span>ipt<span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> do_sample<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token punctuation">)</span>
</code></pre> 
<p>输出</p> 
<pre><code class="prism language-python"><span class="token punctuation">[</span><span class="token punctuation">{<!-- --></span><span class="token string">'generated_text'</span><span class="token punctuation">:</span> <span class="token string">'Human: 如何写好一个简历？\n\nAssistant: 一篇好的简历应包含以下内容：个人信息（姓名，出生日期，出生地，教育经历，工作经历）、求职理由、个人能力（如语言能力，英语水平，操作技能，编程能力，市场营销能力，分析归纳能力等）、学习经历、实践经历和经验、荣誉奖项、相关证书和荣誉、个人兴趣爱好以及在工作中遇到的瓶颈和障碍。\n\n在书写时，应注意文字简洁、条理清晰，突出重点，语言流畅。您也可以在简历中附上一些相关的个人照片或照片资料以供他人参考。如果您有任何疑问，请随时与我联系。'</span><span class="token punctuation">}</span><span class="token punctuation">]</span>
</code></pre> 
<h2><a id="_717"></a>十、主路合并旁路</h2> 
<h3><a id="1_718"></a>1、加载基础模型</h3> 
<pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForCausalLM<span class="token punctuation">,</span> AutoTokenizer

<span class="token keyword">from</span> peft <span class="token keyword">import</span> PeftModel

model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"Langboat/bloom-1b4-zh"</span><span class="token punctuation">,</span> low_cpu_mem_usage<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"Langboat/bloom-1b4-zh"</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="2LoRA_729"></a>2、加载LoRA模型</h3> 
<pre><code class="prism language-python">p_model <span class="token operator">=</span> PeftModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model<span class="token punctuation">,</span> model_id<span class="token operator">=</span><span class="token string">"/root/autodl-tmp/tuningdata/lora/checkpoint-500"</span><span class="token punctuation">)</span>
p_model
</code></pre> 
<p>输出</p> 
<pre><code class="prism language-python">PeftModelForCausalLM<span class="token punctuation">(</span>
  <span class="token punctuation">(</span>base_model<span class="token punctuation">)</span><span class="token punctuation">:</span> LoraModel<span class="token punctuation">(</span>
    <span class="token punctuation">(</span>model<span class="token punctuation">)</span><span class="token punctuation">:</span> BloomForCausalLM<span class="token punctuation">(</span>
      <span class="token punctuation">(</span>transformer<span class="token punctuation">)</span><span class="token punctuation">:</span> BloomModel<span class="token punctuation">(</span>
        <span class="token punctuation">(</span>word_embeddings<span class="token punctuation">)</span><span class="token punctuation">:</span> Embedding<span class="token punctuation">(</span><span class="token number">46145</span><span class="token punctuation">,</span> <span class="token number">2048</span><span class="token punctuation">)</span>
        <span class="token punctuation">(</span>word_embeddings_layernorm<span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token punctuation">(</span>h<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleList<span class="token punctuation">(</span>
          <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">-</span><span class="token number">23</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token number">24</span> x BloomBlock<span class="token punctuation">(</span>
            <span class="token punctuation">(</span>input_layernorm<span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
            <span class="token punctuation">(</span>self_attention<span class="token punctuation">)</span><span class="token punctuation">:</span> BloomAttention<span class="token punctuation">(</span>
              <span class="token punctuation">(</span>query_key_value<span class="token punctuation">)</span><span class="token punctuation">:</span> lora<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>
                <span class="token punctuation">(</span>base_layer<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">6144</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
                <span class="token punctuation">(</span>lora_dropout<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleDict<span class="token punctuation">(</span>
                  <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token punctuation">:</span> Identity<span class="token punctuation">(</span><span class="token punctuation">)</span>
                <span class="token punctuation">)</span>
                <span class="token punctuation">(</span>lora_A<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleDict<span class="token punctuation">(</span>
                  <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
                <span class="token punctuation">)</span>
                <span class="token punctuation">(</span>lora_B<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleDict<span class="token punctuation">(</span>
                  <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">6144</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
                <span class="token punctuation">)</span>
                <span class="token punctuation">(</span>lora_embedding_A<span class="token punctuation">)</span><span class="token punctuation">:</span> ParameterDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
                <span class="token punctuation">(</span>lora_embedding_B<span class="token punctuation">)</span><span class="token punctuation">:</span> ParameterDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
              <span class="token punctuation">)</span>
              <span class="token punctuation">(</span>dense<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
              <span class="token punctuation">(</span>attention_dropout<span class="token punctuation">)</span><span class="token punctuation">:</span> Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
            <span class="token punctuation">)</span>
            <span class="token punctuation">(</span>post_attention_layernorm<span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
            <span class="token punctuation">(</span>mlp<span class="token punctuation">)</span><span class="token punctuation">:</span> BloomMLP<span class="token punctuation">(</span>
              <span class="token punctuation">(</span>dense_h_to_4h<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">8192</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
              <span class="token punctuation">(</span>gelu_impl<span class="token punctuation">)</span><span class="token punctuation">:</span> BloomGelu<span class="token punctuation">(</span><span class="token punctuation">)</span>
              <span class="token punctuation">(</span>dense_4h_to_h<span class="token punctuation">)</span><span class="token punctuation">:</span> lora<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>
                <span class="token punctuation">(</span>base_layer<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">8192</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
                <span class="token punctuation">(</span>lora_dropout<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleDict<span class="token punctuation">(</span>
                  <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token punctuation">:</span> Identity<span class="token punctuation">(</span><span class="token punctuation">)</span>
                <span class="token punctuation">)</span>
                <span class="token punctuation">(</span>lora_A<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleDict<span class="token punctuation">(</span>
                  <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">8192</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
                <span class="token punctuation">)</span>
                <span class="token punctuation">(</span>lora_B<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleDict<span class="token punctuation">(</span>
                  <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
                <span class="token punctuation">)</span>
                <span class="token punctuation">(</span>lora_embedding_A<span class="token punctuation">)</span><span class="token punctuation">:</span> ParameterDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
                <span class="token punctuation">(</span>lora_embedding_B<span class="token punctuation">)</span><span class="token punctuation">:</span> ParameterDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
              <span class="token punctuation">)</span>
            <span class="token punctuation">)</span>
          <span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        <span class="token punctuation">(</span>ln_f<span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
      <span class="token punctuation">)</span>
      <span class="token punctuation">(</span>lm_head<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">46145</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span>
  <span class="token punctuation">)</span>
<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="3_794"></a>3、模型推理</h3> 
<pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

pipe <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"text-generation"</span><span class="token punctuation">,</span> model<span class="token operator">=</span>p_model<span class="token punctuation">,</span> tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
ipt <span class="token operator">=</span> <span class="token string">"Human: {}\n{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token string">"如何写好一个简历？"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"\n\nAssistant: "</span>
pipe<span class="token punctuation">(</span>ipt<span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> do_sample<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token punctuation">)</span>
</code></pre> 
<h3><a id="4_806"></a>4、模型合并</h3> 
<pre><code class="prism language-python">merge_model <span class="token operator">=</span> p_model<span class="token punctuation">.</span>merge_and_unload<span class="token punctuation">(</span><span class="token punctuation">)</span>
merge_model
</code></pre> 
<p>输出</p> 
<pre><code class="prism language-python">BloomForCausalLM<span class="token punctuation">(</span>
  <span class="token punctuation">(</span>transformer<span class="token punctuation">)</span><span class="token punctuation">:</span> BloomModel<span class="token punctuation">(</span>
    <span class="token punctuation">(</span>word_embeddings<span class="token punctuation">)</span><span class="token punctuation">:</span> Embedding<span class="token punctuation">(</span><span class="token number">46145</span><span class="token punctuation">,</span> <span class="token number">2048</span><span class="token punctuation">)</span>
    <span class="token punctuation">(</span>word_embeddings_layernorm<span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token punctuation">(</span>h<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleList<span class="token punctuation">(</span>
      <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">-</span><span class="token number">23</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token number">24</span> x BloomBlock<span class="token punctuation">(</span>
        <span class="token punctuation">(</span>input_layernorm<span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token punctuation">(</span>self_attention<span class="token punctuation">)</span><span class="token punctuation">:</span> BloomAttention<span class="token punctuation">(</span>
          <span class="token punctuation">(</span>query_key_value<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">6144</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
          <span class="token punctuation">(</span>dense<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
          <span class="token punctuation">(</span>attention_dropout<span class="token punctuation">)</span><span class="token punctuation">:</span> Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        <span class="token punctuation">(</span>post_attention_layernorm<span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token punctuation">(</span>mlp<span class="token punctuation">)</span><span class="token punctuation">:</span> BloomMLP<span class="token punctuation">(</span>
          <span class="token punctuation">(</span>dense_h_to_4h<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">8192</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
          <span class="token punctuation">(</span>gelu_impl<span class="token punctuation">)</span><span class="token punctuation">:</span> BloomGelu<span class="token punctuation">(</span><span class="token punctuation">)</span>
          <span class="token punctuation">(</span>dense_4h_to_h<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">8192</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
      <span class="token punctuation">)</span>
    <span class="token punctuation">)</span>
    <span class="token punctuation">(</span>ln_f<span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
  <span class="token punctuation">)</span>
  <span class="token punctuation">(</span>lm_head<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">46145</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="5_841"></a>5、模型推理</h3> 
<pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

pipe <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"text-generation"</span><span class="token punctuation">,</span> model<span class="token operator">=</span>merge_model<span class="token punctuation">,</span> tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
ipt <span class="token operator">=</span> <span class="token string">"Human:如何写好一个简历？\n\nAssistant: "</span>
pipe<span class="token punctuation">(</span>ipt<span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="6_851"></a>6、完整模型保存</h3> 
<p>模型训练完后，可以将合并的模型进行保存到本地，进行备用</p> 
<pre><code class="prism language-python">merge_model<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span><span class="token string">"/root/autodl-tmp/tuningdata/merge_model"</span><span class="token punctuation">)</span>
</code></pre> 
<hr> 
<h2><a id="_858"></a>总结</h2> 
<p>LoRA是一种新颖的微调技术，通过引入低秩矩阵来调整模型的行为，以提高模型在新任务上的表现。它具有参数高效、计算复杂度低等优点，因此在自然语言处理领域具有广泛的应用前景。</p> 
<p><img src="https://images2.imgbox.com/bb/e7/UeuHars5_o.png" alt="在这里插入图片描述"></p> 
<p>🎯🔖更多专栏系列文章：<a href="https://blog.csdn.net/xiaobing259/category_12628007.html?spm=1001.2014.3001.5482"><strong>AIGC-AI大模型探索之路</strong></a></p> 
<blockquote> 
 <p>如果文章内容对您有所触动，别忘了<font color="red"><strong>点赞、⭐关注，收藏</strong></font>！加入我，让我们携手同行AI的探索之旅，一起开启智能时代的大门！</p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e3f0e947814c486c131605818f9370ac/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">2024年网络安全最全凯哥带你从零学大数据系列之Java篇---第十七章 集合(List)(1)，2024年最新带你轻松理解网络安全-Hook机制</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b4c594d4a2ce56f5eb1dd1e647b2b8ca/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【Java】第二讲：字符串相关类</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程学习者.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>