<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Linux快速部署大语言模型LLaMa3，Web可视化j交互（Ollama&#43;Open Web UI） - 编程学习者</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcxuexizhe.github.io/posts/4066d175d95361b97a9e97e7c49ff403/">
  <meta property="og:site_name" content="编程学习者">
  <meta property="og:title" content="Linux快速部署大语言模型LLaMa3，Web可视化j交互（Ollama&#43;Open Web UI）">
  <meta property="og:description" content="本文在个人博客同步发布，前往阅读
1 介绍 本文将介绍使用开源工具Ollama(60.6k⭐)部署LLaMa大模型，以及使用Open WebUI搭建前端Web交互界面的方法。
我们先来过一遍几个相关的概念，对这块比较熟悉的朋友可跳过。
1.1 大规模语言模型 大规模语言模型（Large Language Models, LLMs），顾名思义是指在大量语料数据的基础上训练成的模型，能够模拟人类的语言风格生成较为生动的文本。这类模型的主要特征有：
规模大：训练所使用的数据量非常庞大，有时超过1000亿个参数。复杂性高：模型结构比较复杂具有较好的上下文理解能力：大规模语言模型可以理解文本的上下文和细微差别 1.2 LLaMa LLaMA是一种大规模语言模型，由Meta AI基于Transformer深度学习框架开发。该模型旨在生成各种风格的高质量文本（例如创意写作、对话甚至诗歌），能够胜任以下工作：
自然语言处理（NLP）：理解和生成自然语言。机器学习：根据数据和算法学习新的信息和技能。对话生成：可以与用户进行对话，并根据情况生成合适的回应。 1.3 Ollama 官网：Ollama
API文档：ollama/docs/api.md at main · ollama/ollama (github.com)
支持的模型列表：library
一款可以快速部署大模型的工具。
1.4 Open WebUI 官网：Open WebUI
相关介绍及源码：open-webui/open-webui: User-friendly WebUI for LLMs (Formerly Ollama WebUI) (github.com)
Open WebUI 是一个可视化的Web交互环境，它拥有清新简约的UI风格，具有可扩展、功能丰富、用户友好、自托管的特点，可以完全离线运行。它支持各种 LLM 运行程序，包括 Ollama 和 OpenAI 兼容的 API。
2 部署LLM服务 本文介绍的方法使用于Linux系统，同样适用于Windows系统的WSL（安装方法可参见我的这篇文章）。
2.1 部署Ollama 1、下载Ollama
Linux系统的安装命令如下：
curl -fsSL https://ollama.com/install.sh | sh ※此外官方还提供了macOS和Windows的下载方式。
2、下载llama3模型
ollama run llama3 ※在这里可以看到该命令的相关介绍。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-26T20:28:37+08:00">
    <meta property="article:modified_time" content="2024-04-26T20:28:37+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程学习者" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程学习者</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Linux快速部署大语言模型LLaMa3，Web可视化j交互（Ollama&#43;Open Web UI）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p>本文在个人博客同步发布，<a href="http://janborn.fun/2024/04/26/linux%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8Bllama3%EF%BC%8C%E6%90%AD%E5%BB%BAweb%E5%8F%AF%E8%A7%86%E5%8C%96ui%EF%BC%88ollamaopen-web-ui%EF%BC%89/" rel="nofollow">前往阅读</a></p> 
</blockquote> 
<h2><a id="1__2"></a>1 介绍</h2> 
<p>本文将介绍使用开源工具Ollama(60.6k⭐)部署LLaMa大模型，以及使用Open WebUI搭建前端Web交互界面的方法。</p> 
<p>我们先来过一遍几个相关的概念，对这块比较熟悉的朋友可跳过。</p> 
<h3><a id="11__8"></a>1.1 大规模语言模型</h3> 
<p>大规模语言模型（Large Language Models, LLMs），顾名思义是指在大量语料数据的基础上训练成的模型，能够模拟人类的语言风格生成较为生动的文本。这类模型的主要特征有：</p> 
<ul><li>规模大：训练所使用的数据量非常庞大，有时超过1000亿个参数。</li><li>复杂性高：模型结构比较复杂</li><li>具有较好的上下文理解能力：大规模语言模型可以理解文本的上下文和细微差别</li></ul> 
<h3><a id="12_LLaMa_16"></a>1.2 LLaMa</h3> 
<p>LLaMA是一种大规模语言模型，由Meta AI基于Transformer深度学习框架开发。该模型旨在生成各种风格的高质量文本（例如创意写作、对话甚至诗歌），能够胜任以下工作：</p> 
<ul><li>自然语言处理（NLP）：理解和生成自然语言。</li><li>机器学习：根据数据和算法学习新的信息和技能。</li><li>对话生成：可以与用户进行对话，并根据情况生成合适的回应。</li></ul> 
<h3><a id="13_Ollama_24"></a>1.3 Ollama</h3> 
<blockquote> 
 <p>官网：<a href="https://ollama.com/" rel="nofollow">Ollama</a></p> 
 <p>API文档：<a href="https://github.com/ollama/ollama/blob/main/docs/api.md">ollama/docs/api.md at main · ollama/ollama (github.com)</a></p> 
 <p>支持的模型列表：<a href="https://ollama.com/library" rel="nofollow">library</a></p> 
</blockquote> 
<p>一款可以快速部署大模型的工具。</p> 
<h3><a id="14_Open_WebUI_34"></a>1.4 Open WebUI</h3> 
<blockquote> 
 <p>官网：<a href="https://openwebui.com/" rel="nofollow">Open WebUI</a></p> 
 <p>相关介绍及源码：<a href="https://github.com/open-webui/open-webui">open-webui/open-webui: User-friendly WebUI for LLMs (Formerly Ollama WebUI) (github.com)</a></p> 
</blockquote> 
<p>Open WebUI 是一个可视化的Web交互环境，它拥有清新简约的UI风格，具有可扩展、功能丰富、用户友好、自托管的特点，可以完全离线运行。它支持各种 LLM 运行程序，包括 Ollama 和 OpenAI 兼容的 API。</p> 
<h2><a id="2_LLM_44"></a>2 部署LLM服务</h2> 
<p>本文介绍的方法使用于Linux系统，同样适用于Windows系统的WSL（安装方法可参见我的<a href="https://blog.csdn.net/mustuo/article/details/133960230?">这篇文章</a>）。</p> 
<h3><a id="21_Ollama_48"></a>2.1 部署Ollama</h3> 
<p>1、下载Ollama</p> 
<p>Linux系统的安装命令如下：</p> 
<pre><code class="prism language-shell"><span class="token function">curl</span> <span class="token parameter variable">-fsSL</span> https://ollama.com/install.sh <span class="token operator">|</span> <span class="token function">sh</span>
</code></pre> 
<p>※此外<a href="https://ollama.com/download" rel="nofollow">官方</a>还提供了macOS和Windows的下载方式。</p> 
<p>2、下载llama3模型</p> 
<pre><code class="prism language-shell">ollama run llama3
</code></pre> 
<p>※在<a href="https://ollama.com/blog/llama3" rel="nofollow">这里</a>可以看到该命令的相关介绍。</p> 
<p>上述命令将自动拉取模型，并进行sha256验签。处理完毕后自动进入llama3的运行环境，可以使用中文或英文进行提问，<kbd>ctrl</kbd>+<kbd>D</kbd>退出。</p> 
<p>3、配置服务</p> 
<p>为使外网环境能够访问到服务，需要对HOST进行配置。</p> 
<p>打开配置文件：<code>vim /etc/systemd/system/ollama.service</code>，根据情况修改变量<code>Environment</code>：</p> 
<ul><li>服务器环境下：<code>Environment="OLLAMA_HOST=0.0.0.0:11434"</code></li><li>虚拟机环境下：<code>Environment="OLLAMA_HOST=服务器内网IP地址:11434"</code></li></ul> 
<p>3、启动服务</p> 
<p>启动服务的命令：<code>ollama serve</code></p> 
<p>首次启动可能会出现以下两个提示：</p> 
<blockquote> 
 <p>Couldn’t find ‘/home/用户名/.ollama/id_ed25519’. Generating new private key.</p> 
</blockquote> 
<p>该提示表示文件系统中不存在ssh私钥文件，此时命令将自动帮我们生成该文件，并在命令行中打印相应的公钥。</p> 
<blockquote> 
 <p>Error: listen tcp 127.0.0.1:11434: bind: address already in use</p> 
</blockquote> 
<p>看到该提示，大概率服务已在运行中，可以通过<code>netstat -tulpn | grep 11434</code>命令进行确认。</p> 
<ul><li>若命令输出的最后一列包含“ollama”字样，则表示服务已启动，无需做额外处理。</li><li>否则，可尝试执行下列命令重启ollama：</li></ul> 
<pre><code class="prism language-shell"><span class="token comment"># ubuntu/debian</span>
<span class="token function">sudo</span> <span class="token function">apt</span> update
<span class="token function">sudo</span> <span class="token function">apt</span> <span class="token function">install</span> <span class="token function">lsof</span>
stop ollama
<span class="token function">lsof</span> <span class="token parameter variable">-i</span> :11434
<span class="token function">kill</span> <span class="token operator">&lt;</span>PID<span class="token operator">&gt;</span>
ollama serve

<span class="token comment"># centos</span>
<span class="token function">sudo</span> yum update
<span class="token function">sudo</span> yum <span class="token function">install</span> <span class="token function">lsof</span>
stop ollama
<span class="token function">lsof</span> <span class="token parameter variable">-i</span> :11434
<span class="token function">kill</span> <span class="token operator">&lt;</span>PID<span class="token operator">&gt;</span>
ollama serve
</code></pre> 
<p>如果您使用的是MacOS，可在<a href="https://github.com/ollama/ollama/issues/707">🔗这里</a>找到解决方法。</p> 
<p>4、在外网环境验证连接</p> 
<p>方法一：执行<code>curl http://ip:11434</code>命令，若返回“Ollama is running”，则表示连接正常。</p> 
<p>方法二：在浏览器访问http://ip:11434，若页面显示文本“Ollama is running”，则表示连接正常。</p> 
<h3><a id="22_Ollama_122"></a>2.2 Ollama常用命令</h3> 
<p>1、进入llama3运行环境：<code>ollama run llama3</code></p> 
<p>2、启动服务：<code>ollama serve</code></p> 
<p>3、重启ollama</p> 
<pre><code class="prism language-shell">systemctl daemon-reload
systemctl restart ollama
</code></pre> 
<p>4、重启ollama服务</p> 
<pre><code class="prism language-shell"><span class="token comment"># ubuntu/debian</span>
<span class="token function">sudo</span> <span class="token function">apt</span> update
<span class="token function">sudo</span> <span class="token function">apt</span> <span class="token function">install</span> <span class="token function">lsof</span>
stop ollama
<span class="token function">lsof</span> <span class="token parameter variable">-i</span> :11434
<span class="token function">kill</span> <span class="token operator">&lt;</span>PID<span class="token operator">&gt;</span>
ollama serve

<span class="token comment"># centos</span>
<span class="token function">sudo</span> yum update
<span class="token function">sudo</span> yum <span class="token function">install</span> <span class="token function">lsof</span>
stop ollama
<span class="token function">lsof</span> <span class="token parameter variable">-i</span> :11434
<span class="token function">kill</span> <span class="token operator">&lt;</span>PID<span class="token operator">&gt;</span>
ollama serve
</code></pre> 
<p>5、确认服务端口状态：<code>netstat -tulpn | grep 11434</code></p> 
<h2><a id="3_Open_WebUI_157"></a>3 部署Open WebUI</h2> 
<p>1、下载Open WebUI</p> 
<p>Open WebUI基于docker部署，docker的安装方法可以参考<a href="https://zhuanlan.zhihu.com/p/651148141" rel="nofollow">这篇知乎文章</a>。</p> 
<p>Open WebUI既可以部署在服务端，也可以部署在客户端：</p> 
<pre><code class="prism language-shell"><span class="token comment"># 若部署在客户端，执行：</span>
<span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token parameter variable">-p</span> <span class="token number">3000</span>:8080 --add-host<span class="token operator">=</span>host.docker.internal:host-gateway <span class="token parameter variable">-v</span> open-webui:/app/backend/data <span class="token parameter variable">--name</span> open-webui <span class="token parameter variable">--restart</span> always ghcr.io/open-webui/open-webui:main

<span class="token comment"># 若部署在服务端，执行：</span>
<span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token parameter variable">-p</span> <span class="token number">3000</span>:8080 <span class="token parameter variable">-e</span> <span class="token assign-left variable">OLLAMA_BASE_URL</span><span class="token operator">=</span>https://example.com <span class="token parameter variable">-v</span> open-webui:/app/backend/data <span class="token parameter variable">--name</span> open-webui <span class="token parameter variable">--restart</span> always ghcr.io/open-webui/open-webui:main
</code></pre> 
<p>如果您的机器在国内，建议将<code>--restart</code>的参数值替换为<code>ghcr.nju.edu.cn/open-webui/open-webui:main</code>，下载速度会快非常多（见up主小杨生存日记的<a href="https://www.bilibili.com/read/cv32462618/" rel="nofollow">这篇文章</a>）。</p> 
<p>2、检查相关配置</p> 
<p>下载完之后，就可以在浏览器访问了，地址为<code>http://loacalhost:3000</code>（客户端部署）或<code>http://服务器ip:3000</code>。</p> 
<p>页面加载完成后（这个过程可能需要一些时间），新注册一个账号并登录。</p> 
<p>登录之后，点击页面顶端的齿轮⚙图标进入设置：</p> 
<ol><li>侧边导航栏-General，将语言设置为中文</li><li>侧边导航栏-连接，若“Ollama 基础 URL”这一项为<code>http://host.docker.internal:11434</code>，则表示ollama服务正常且连接成功；如果是空的，则需要回头检查一下ollama服务了</li><li>侧边导航栏-模型，一般会自动拉取ollama服务上部署好的模型，可选模型参看<a href="https://ollama.com/library/llama3" rel="nofollow">官方的这篇文档</a></li><li>其它的项目根据需要设置即可</li></ol> 
<p>3、选择模型</p> 
<p>在顶端下拉框选择好模型，就可以开始提问啦！<br> <img src="https://images2.imgbox.com/26/52/5iYWUwX8_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_196"></a>参考文章</h2> 
<ul><li><a href="https://juejin.cn/post/7359470175761350690" rel="nofollow">macOS + Ollama + Enchanted，本地部署最新 Llama3 - 掘金 (juejin.cn)</a></li><li><a href="https://www.bilibili.com/read/cv32462618/" rel="nofollow">服务器部署开源大模型完整教程 Ollama+Gemma+open-webui - 哔哩哔哩 (bilibili.com)</a></li><li><a href="https://zhuanlan.zhihu.com/p/686952702" rel="nofollow">Ollama管理本地开源大模型，用Open WebUI访问Ollama接口 - 知乎 (zhihu.com)</a></li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2da9598ede93b27434bbafa2a45d9a8c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">伪分布式数据库搭建（hadoop&#43;spark&#43;scala）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/92c72a90cad7cb2911bd035aba8990d3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Kafka 3.x.x 入门到精通（03）——Kafka基础生产消息</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程学习者.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>