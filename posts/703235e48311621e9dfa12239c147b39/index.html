<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>本地搭建AI环境 - 编程学习者</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcxuexizhe.github.io/posts/703235e48311621e9dfa12239c147b39/">
  <meta property="og:site_name" content="编程学习者">
  <meta property="og:title" content="本地搭建AI环境">
  <meta property="og:description" content="本地搭建AI 这几天刚刚看到好兄弟分享的一段关于本地搭建AI的短视频，于是我按照视频里的讲解，进行了实践。感觉非常棒！！，马上整理成文字与大家分享一下。
在本地启动并运行大型语言模型，运行llama3、phi3和其他模型，自定义并创建您自己的。
安装Ollama 首先进入Ollama网站，在浏览器的地址栏输入：https://ollama.com/，直接进入网站，如下图所示：
根据你的操作系统，点击Download进行下载。
本人的操作系统是macOS，芯片是M3。下载之后，直接进行安装。
检验是否安装成功 输入 Ollama 命令，正常的得出命令行输出，表示已经安装成功，下面有 ollama 的常用命令
localhost:~ root# ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model from a Modelfile show Show information for a model run Run a model pull Pull a model from a registry push Push a model to a registry list List models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use &#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-09T05:00:00+08:00">
    <meta property="article:modified_time" content="2024-05-09T05:00:00+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程学习者" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程学习者</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">本地搭建AI环境</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="AI_0"></a>本地搭建AI</h2> 
<p>这几天刚刚看到好兄弟分享的一段关于本地搭建AI的短视频，于是我按照视频里的讲解，进行了实践。感觉非常棒！！，马上整理成文字与大家分享一下。</p> 
<p>在本地启动并运行大型语言模型，运行llama3、phi3和其他模型，自定义并创建您自己的。</p> 
<h3><a id="Ollama_6"></a>安装Ollama</h3> 
<p>首先进入Ollama网站，在浏览器的地址栏输入：https://ollama.com/，直接进入网站，如下图所示：</p> 
<p><img src="https://images2.imgbox.com/71/c8/u4fBCpws_o.png" alt="Screenshot 2024-05-08 at 19.10.04"></p> 
<p>根据你的操作系统，点击Download进行下载。</p> 
<p>本人的操作系统是macOS，芯片是M3。下载之后，直接进行安装。</p> 
<h3><a id="_16"></a>检验是否安装成功</h3> 
<p>输入 Ollama 命令，正常的得出命令行输出，表示已经安装成功，下面有 ollama 的常用命令</p> 
<pre><code class="prism language-shell">localhost:~ root<span class="token comment"># ollama</span>
Usage:
  ollama <span class="token punctuation">[</span>flags<span class="token punctuation">]</span>
  ollama <span class="token punctuation">[</span>command<span class="token punctuation">]</span>

Available Commands:
  serve       Start ollama
  create      Create a model from a Modelfile
  show        Show information <span class="token keyword">for</span> a model
  run         Run a model
  pull        Pull a model from a registry
  push        Push a model to a registry
  list        List models
  <span class="token function">cp</span>          Copy a model
  <span class="token function">rm</span>          Remove a model
  <span class="token builtin class-name">help</span>        Help about any <span class="token builtin class-name">command</span>

Flags:
  -h, <span class="token parameter variable">--help</span>      <span class="token builtin class-name">help</span> <span class="token keyword">for</span> ollama
  -v, <span class="token parameter variable">--version</span>   Show version information

Use <span class="token string">"ollama [command] --help"</span> <span class="token keyword">for</span> <span class="token function">more</span> information about a command.
</code></pre> 
<h3><a id="ollama_45"></a>ollama模型库</h3> 
<p>跳转到https://ollama.com/library。进入到模型库主页，如下图所示：</p> 
<p><img src="https://images2.imgbox.com/76/eb/yOfGbZWe_o.png" alt="Screenshot 2024-05-08 at 19.36.45"></p> 
<h3><a id="_51"></a>运行模型库</h3> 
<p>本案例利用llama3模型，在Terminal窗口下，在命令行输入如下命令：</p> 
<pre><code class="prism language-shell">localhost:~ root<span class="token comment"># ollama run llama3</span>
</code></pre> 
<p>运行结果如下：</p> 
<pre><code class="prism language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> Send a message <span class="token punctuation">(</span><span class="token operator">/</span>? <span class="token keyword">for</span> <span class="token builtin">help</span><span class="token punctuation">)</span>
</code></pre> 
<p>然后输入如下问题：</p> 
<pre><code class="prism language-shell"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> 你是谁？请用中文回答
我是一個基於人工智慧的聊天機器人，旨在與使用者進行自然语言对话。我可以理解和生成人类语言，可以回答问题、提供信息、甚至参与创作等。我的目的是为了帮助用户更好地了解我是一個基於人工智慧的聊天機器人，旨在與使用者進行自然语言对话。我可以理解和生成人类语言，可以回答问题、提供信息、甚至参与创作等。我的目的是为了帮助用户更好地了解世界，并且与我交流思想和经验。

（Note：I'm a chatbot powered by artificial intelligence, designed to engage <span class="token keyword">in</span> 
natural language conversations with users. I can understand and generate 
human-like language, answer questions, provide information, and even participate
<span class="token keyword">in</span> creative activities. My goal is to <span class="token builtin class-name">help</span> <span class="token function">users</span> better understand the world and
exchange thoughts and experiences with me.<span class="token punctuation">]</span>
</code></pre> 
<pre><code class="prism language-shell"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> 请问你都有什么功能？用中文回答
我是一個多功能的chatbot，以下是我的一些主要功能：

<span class="token number">1</span>. **对话生成**：我可以和用户进行自然语言对话，回答问题、提供信息、甚至参与创作等。
<span class="token number">2</span>. **知识分享**：我拥有广泛的知识数据库，可以分享历史、科学、技术、文化等领域的知识。
<span class="token number">3</span>. **语言理解**：我可以理解用户的语言，并根据情况回答或回应。
<span class="token number">4</span>. **情感识别**：我可以识别用户的情感状态，提供合适的支持和建议。
<span class="token number">5</span>. **问题解决**：我可以帮助用户解决问题，找到答案，或者提供相关信息。
<span class="token number">6</span>. **创作协作**：我可以与用户合作创作文本、诗歌、短剧等内容。
<span class="token number">7</span>. **游戏互动**：我可以和用户一起玩游戏，如20 Questions、Hangman、Word Association 等。
<span class="token number">8</span>. **幽默分享**：我可以分享幽默和笑话，帮助用户度过难关的时刻。

这些功能让我能够与用户进行更加紧密的交流，提供更加个性化的服务。
</code></pre> 
<p>可以看到llama3的功能还是很强大的。**注意：请在问题后面加上用中文回答。否则回答的是英文答案。**当然，如果你英文很好，可以忽略。</p> 
<h3><a id="_96"></a>部署大模型前端界面</h3> 
<p>上面的步骤介绍的是在命令行下面直接运行模型，感觉界面不够友好。接下来教你搭建一个美观、友好的前端页面，这样用起来更方便，更高大上。然后就可以完全免费的享用大模型，想想都很爽。这个美观的大模型界面就是开源的LobeChat。</p> 
<h4><a id="1_docker_100"></a>1. 安装docker</h4> 
<h5><a id="docker_102"></a>什么是docker</h5> 
<p>Docker 是一个开源的容器化平台，可以将应用程序和服务打包到一个轻量级的容器中，使得开发、测试、生产环境中的应用程序之间实现了更好的隔离和重用。</p> 
<p>Docker 的主要特点有：</p> 
<ol><li><strong>容器化</strong>：Docker 将应用程序和服务打包到一个容器中，这个容器可以包含所需的库、依赖项、配置文件等。</li><li><strong>轻量级</strong>：Docker 容器非常轻量，通常只有几十兆到几百兆大小，可以快速启动和关闭。</li><li><strong>可移植性</strong>：Docker 容器可以在不同的环境中运行，无需担心不同操作系统、库版本等问题。</li><li><strong>高效率</strong>：Docker 可以提高开发和部署速度，因为它可以快速创建和销毁容器。</li></ol> 
<p>Docker 的使用场景非常广泛，包括：</p> 
<ol><li><strong>微服务架构</strong>：Docker 可以帮助实现微服务架构，使得应用程序变得更加灵活、可维护和可扩展。</li><li><strong>DevOps</strong>：Docker 可以帮助实现 DevOpsWorkflow，使得开发、测试、生产环境中的应用程序之间实现了更好的隔离和重用。</li><li><strong>云计算</strong>：Docker 可以帮助实现云计算，提供一个灵活的容器化平台，使得应用程序可以快速部署到不同的云环境中。</li></ol> 
<h5><a id="docker_119"></a>下载docker</h5> 
<p>docker下载地址：https://www.docker.com/products/docker-desktop/，下载界面如下图所示：</p> 
<p><img src="https://images2.imgbox.com/48/6a/R4EiB2AR_o.png" alt="Screenshot 2024-05-08 at 20.52.44"></p> 
<h5><a id="docker_125"></a>验证安装docker成功</h5> 
<p>只需要运行下面的命令</p> 
<pre><code class="prism language-shell"><span class="token function">docker</span> run hello-world
</code></pre> 
<p>如果返回消息中带有：成功，表明安装成功。</p> 
<h4><a id="2_dockerlobechat_135"></a>2. docker部署lobechat</h4> 
<p>通过docker部署，只需要一两行命令，非常简单。启动docker，界面如下：</p> 
<p><img src="https://images2.imgbox.com/7e/ec/jesEm2x6_o.png" alt="Screenshot 2024-05-08 at 21.21.16"></p> 
<p>输入如下命令：</p> 
<pre><code class="prism language-shell"><span class="token function">docker</span> pull lobehub/lobe-chat:latest
</code></pre> 
<p><code>docker pull</code> 命令用于从 Docker Hub 或其他 Registry 中拉取镜像。lobehub/lobe-chat:这是Docker的镜像的名称，其中lobehub是镜像仓库的名称，lobe-chat是具体的镜像名称。latest：表示拉取该镜像的最新版本（tag）。如果没有指定版本标签，Docker默认会拉取latest标签的版本。执行这条命令后， Docker会将lobehub/lobe-chat镜像的最新版本下载到你的本地系统，以便你可以使用它创建和运行Docker容器。然后再运行一条命令就可以了：</p> 
<pre><code class="prism language-shell"><span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token parameter variable">--name</span> lobe-chat <span class="token parameter variable">-p</span> <span class="token number">10084</span>:3210 <span class="token parameter variable">-e</span> <span class="token assign-left variable">ACCESS_CODE</span><span class="token operator">=</span>lobe66 lobehub/lobe-chat:latest
</code></pre> 
<p>解释一下这条命令，这个命令是使用 Docker 运行一个容器的命令。下面将分解该命令：</p> 
<ul><li><code>-d</code>： detached 模式，即在后台运行容器。这意味着容器将独立于终端会话运行。</li><li><code>--name lobe-chat</code>：为容器命名为 “lobe-chat”。这使得后期管理和识别容器变得更加容易。</li><li><code>-p 10084:3210</code>：将主机的端口 10084 映射到容器中的端口 3210。这允许你从外部访问容器中的服务，使用主机的 IP 地址和端口 10084。</li><li><code>-e ACCESS_CODE=lobe66</code>：设置一个环境变量 <code>ACCESS__CODE</code>，其值为 <code>lobe66</code>。这将为容器中的应用程序提供一个配置参数。</li></ul> 
<p>这个命令创建并启动一个名为 “lobe-chat” 的容器，从 <code>lobehub/lobe-chat:latest</code> 镜像中运行。该容器在 detached 模式下运行，端口 10084 在主机上映射到容器中的端口 3210，并将环境变量 <code>ACCESS_ CODE</code> 设置为 <code>lobe66</code>。这个lobe66在后面启动网页界面时会用到。到这里，已经安装部署完成lobechat。</p> 
<p>结果，容器将在后台运行，你可以使用其他 Docker 命令（例如 <code>docker ps</code> 或 <code>docker logs</code>）来管理和检查容器。</p> 
<p><strong>注意</strong></p> 
<ul><li>如果你已经拉取了该镜像，可以使用 <code>docker images</code> 命令查看当前可用的镜像。</li><li>如果你想更新已有的镜像，可以使用 <code>docker pull</code> 命令重新拉取最新版本的镜像。</li></ul> 
<h4><a id="3__169"></a>3. 体验</h4> 
<p>打开浏览器，在地址栏输入：localhost:10084，如果从其他电脑登录，直接输入http://+你的本机ip地址:10084。进入之后，界面布局如下图所示：</p> 
<p><img src="https://images2.imgbox.com/4d/4b/8O8y5qoh_o.png" alt="Screenshot 2024-05-08 at 21.44.22"></p> 
<p>点击上图箭头所指的Start Now按钮，进入如下界面：</p> 
<p><img src="https://images2.imgbox.com/c1/c7/GQ7p9irJ_o.png" alt=""></p> 
<p>本案例选择Llama3模型，如果没有安装，直接按照提示点击下载，这样就可以免费使用Llama3模型了。如下图所示：</p> 
<p><img src="https://images2.imgbox.com/45/92/SW9RSlr3_o.png" alt="Screenshot 2024-05-08 at 22.07.37"></p> 
<p>以上就是完整的搭建过程。请大家快去体验吧，搭建自己的AI本地环境。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9b62ded5ae193ea8ebb0d61a394adb83/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Python版【植物大战僵尸 &#43;源码】</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/095362abd7ffb4fdb98a938d64bb4400/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">JavaScript 垃圾回收机制深度解析：内存管理的艺术</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程学习者.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>