<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【AIGC调研系列】embeding模型有哪些，各有什么优势 - 编程学习者</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcxuexizhe.github.io/posts/11288ceae09af81d676498f1f40707c5/">
  <meta property="og:site_name" content="编程学习者">
  <meta property="og:title" content="【AIGC调研系列】embeding模型有哪些，各有什么优势">
  <meta property="og:description" content="在AIGC中，Embedding模型是一种将文本数据转换为多维向量数组的技术，这些向量可以代表任何事物，如文本、音乐、视频等[2]。有几种不同的Embedding模型，它们各有其优势和应用领域。
Word2vec：这是一种经典的嵌入模型，通过学习单词之间的长距离依赖关系来生成向量。它的优势在于能够有效地捕捉到词语之间的关系，使得模型能够理解和处理复杂的文本结构[4][5]。众安Embedding模型：在中文通用FAQ数据集上，众安Embedding模型表现出色，尽管优势已不如以前那么大[3]。这表明该模型在中文数据集上具有较好的性能。Nomic Embed：这是首个开源、开放数据、开放权重、开放训练代码、完全可复现和可审核的嵌入模型。它的上下文长度为8192，显示出在特定场景下的高效性[18]。OpenAI Embedding Models：OpenAI推出了一系列新型嵌入模型，包括更小、高效的text-embedding-3-small模型和更大、更强大的text-embedding-3-large模型。这些模型具有更低的定价，适用于表示自然语言或代码等内容中的概念[21]。 每种Embedding模型都有其独特的优势，例如Word2vec擅长处理复杂的文本结构，而众安Embedding模型在中文数据集上表现优异。Nomic Embed则以其开源和可复现的特性受到开发者的青睐。OpenAI的嵌入模型则提供了更多样化的模型选择，满足不同应用场景的需求。
Embedding模型在处理中文数据时的具体优势和挑战是什么？ 语义理解能力：Embedding模型通过词向量来表示文本，能够捕捉到词汇之间的语义联系，相比之下，基于关键词的检索往往关注字面匹配，可能忽略了词语之间的语义联系[24]。这一点在搜索引擎、构建私有知识问答系统、内容推荐系统等应用中尤为重要[27]。容错性：基于Embedding的方法能够理解词汇之间的关系，从而提高了模型的容错性[24]。在面对错误或不完整的数据时，Embedding模型能够更好地保持其准确性和可靠性。支持中文：一些Embedding模型，如BGE，对中文数据的支持效果较好，是中文embedding模型中为数不多的优质选择[28]。这表明在中文数据处理方面，Embedding模型具有较强的应用潜力。 然而，Embedding模型在处理中文数据时也面临着挑战：
性能差异：尽管有些模型在中文处理上有优势，但不同模型的性能可能存在显著差异。例如，text2vec在STS-B测试集中的效果优势下降，与MiniLM效果相近[23]。这意味着在选择Embedding模型时，需要考虑到不同模型的性能表现。模型部署和优化：虽然有些模型已经开源并方便本地私有化部署[26]，但在实际应用中，如何有效地部署和优化这些模型，以适应特定的业务需求，仍然是一个挑战。特定领域的适用性：特定领域的专有模型通常比通用模型表现更好，尤其是当模型的参数量较小时[29]。这提示我们在处理特定任务时，可能需要考虑使用专门针对该任务的Embedding模型，而不是通用的Embedding模型。 Embedding模型在处理中文数据时的优势在于其强大的语义理解能力、高容错性以及对中文数据的良好支持。然而，选择合适的模型、有效部署和优化模型以及根据特定任务选择专有模型等挑战，也是处理中文数据过程中需要注意的问题。
Word2vec与其他Embedding模型（如Nomic Embed和OpenAI Embedding Models）在性能上的比较研究有哪些？ 模型架构和训练方式的差异：Word2vec和其他模型如Bert、GPT-3等，虽然都利用了单词的周边信息，但它们在使用周边信息的方式、模型架构以及训练方式上存在显著差异。这些差异导致了对单词的表征效果有所不同[32]。例如，Word2vec模型结构相对简单，主要通过连续词袋模型（CBOW）和Skip-gram模型来处理输入数据，以降低模型复杂度并在大规模数据上进行训练[38][39]。而OpenAI的文本嵌入模型，如OpenAI的text embedding 002，通常基于更复杂的深度学习模型[38]。性能的具体比较：尽管没有直接的性能比较结果被明确提及，但是从现有的研究中可以推断出不同模型的性能比较。例如，有研究对比了GPT-3、Bert、GloVe与Word2vec在性能上的差异[37]。此外，OpenAI公布的embedding endpoint也是基于神经网络模型，将文本和代码转换为向量表示，嵌入到高维空间中[33]。这表明OpenAI的文本嵌入模型在技术实现上可能更为先进。应用场景的差异：Word2vec因其简单的模型结构和高效的训练方法，特别适合于需要大规模处理能力的应用场景，如文本分类、情感分析等[34]。而OpenAI的文本嵌入模型则更侧重于链接大模型与外部知识，适用于需要处理复杂语义信息和跨领域知识的应用场景[37]。 Word2vec与其他Embedding模型在性能上的比较研究显示了各自的优势和局限性。Word2vec以其简单高效的特点，在特定的应用场景下表现出色；而OpenAI的文本嵌入模型则在技术实现和应用范围上展现出更多的灵活性和复杂性。
如何优化Embedding模型以提高其在特定领域的应用效果？ 领域特定模型训练：针对特定领域（如医疗、法律）训练Embedding模型，以提高在特定上下文中的准确性[42]。这意味着在训练Embedding模型时，需要考虑到特定领域的特定需求和特点，以确保模型能够更好地适应这些领域。基于组合的方法优化：不应局限于使用q和r两个矩阵，而是可以采用q，r，z，k等多个矩阵的组合方式来优化embedding层。这种方法虽然能降低embedding的参数量，但可能会显著影响模型的性能[41]。因此，选择合适的矩阵组合方式对于模型的优化至关重要。利用开源模型资源：参考全球权威Embedding评测榜单上排名靠前的开源模型，如数元灵开源的Embedding模型，可以为AI Native应用开发提供强大的支持[43]。开源资源通常经过了广泛的应用验证，能够有效提高模型的应用效果。优化算法的选择：在广告/推荐领域，可以使用针对性的优化算法，如FTRL（Fine-tuning with Regularization），这种方法适用于对高维稀疏模型进行训练[44]。此外，Deep Hash Embedding (DHE)也是一种有效的优化方法，特别是当字典大小过大时，DHE能够有效压缩Embedding[45]。fine-tuning技术应用：通过使用特定领域的标注数据，对模型的权重进行微调，可以使模型更好地理解和处理特定领域的文本数据[46]。这种技术的应用可以显著提升模型在特定领域中的表现。稀疏特征的优化表示：对于推荐系统中存在海量稀疏特征的问题，可以通过优化表示方法来解决。例如，通过手工测试来寻找好的Embedding大小，或者采用其他稀疏特征Embedding的优化方法[49]。RAG和微调技术的结合使用：在提升大语言模型性能的过程中，检索增强生成（RAG）和微调（Fine-tuning）两种方法都有其优势。根据微软的指南，在建设特定领域的应用时，可以根据具体情况选择更高效的方法[50]。 优化Embedding模型的关键在于深入理解特定领域的需求，合理选择和组合优化方法，以及利用开源资源和先进的技术手段。通过这些方法，可以有效提高Embedding模型在特定领域的应用效果。
Embedding模型在自然语言处理以外的应用案例有哪些？ Embedding模型在自然语言处理（NLP）之外的应用案例主要包括计算机视觉（CV）领域。Embedding模型能够将高维度的数据转化为低维度的向量空间，这一特性使得它在处理图像数据时表现出色[52]。例如，在计算机视觉中，Embedding模型可以用于图像分类、目标检测、图像分割等任务中。通过将图像中的特征映射到低维度的向量空间，Embedding模型能够捕捉到图像的深层信息，从而提高这些任务的性能[53]。
此外，Embedding模型在其他领域的应用也逐渐增多，尽管文献中没有直接提及具体的应用案例，但根据其在自然语言处理和计算机视觉中的应用背景，我们可以推断出Embedding模型在图像识别、视频分析等领域也有潜在的应用价值。例如，在图像识别领域，Embedding模型可以用于训练模型，以识别和分类不同类型的图像。在视频分析中，Embedding模型可以用于理解和分析视频内容，如动作识别、情感分析等[54]。
虽然文献中没有直接列出Embedding模型在除自然语言处理外的其他应用案例，但根据其在自然语言处理和计算机视觉中的应用背景，我们可以合理推测Embedding模型在图像识别、视频分析等领域也有广泛的应用潜力。
最新的Embedding模型技术发展趋势是什么？ 多阶段训练过程的引入：BGE M3-Embedding采用了多阶段训练过程，结合自动编码和弱监督对比学习，这种方式不仅可以提高模型的性能，还能避免预训练方式之间的冲突，预示着未来Embedding模型技术可能会发展出更多创新的训练方式[59]。模型大小的优化：OpenAI推出的text-embedding-3-small模型和text-embedding-3-large模型，分别代表了更小且高效以及更大且更强大的文本嵌入模型，这表明了在嵌入模型设计上追求性能和效率的平衡[60]。技术和成本的权衡：OpenAI新模型使用的嵌入技术允许开发人员在使用嵌入时权衡使用嵌入的性能和成本，通过在dimensions API参数中传递嵌入而不丢失其概念，这种方法有助于开发者更灵活地应用嵌入技术[61]。多模态模型的发展：GPT-4作为GPT系列中的最新版本，支持图像和文本类型的输入，这是一个重要的发展趋势，标志着Embedding模型技术正在向支持更广泛输入类型（如图像）的方向发展[64]。开源和复现的重要性：Nomic Embed模型的推出，作为首个开源、开放数据、开放权重、开放训练代码的嵌入模型，强调了开源和复现在技术发展中的重要性，这对于推动技术的普及和创新具有积极作用[67][68]。 最新的Embedding模型技术发展趋势是向着多阶段训练、模型大小优化、技术与成本的权衡、多模态支持以及开源和复现的方向发展，这些趋势共同推动了Embedding模型技术的进步和应用范围的扩大。
参考资料
[1]. 【AIGC】Embedding与LLM：token长度限制困局下 - 博客园
[2]. 【AIGC】All you need is Embedding？Token长度困境下的泛长文本LLM应用 - 知乎
[3]. 优雅！比OpenAI更认真的文本嵌入模型 - 稀土掘金
[4]. AIGC｜Embeddings入门详解！手把手带你训练 - 融·项目管理
[5]. AIGC｜Embeddings入门详解!手把手带你训练 - 知乎
[6]. 大模型应用实践：AIGC探索之旅 - 网易">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-03-05T15:33:09+08:00">
    <meta property="article:modified_time" content="2024-03-05T15:33:09+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程学习者" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程学习者</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【AIGC调研系列】embeding模型有哪些，各有什么优势</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>在AIGC中，Embedding模型是一种将文本数据转换为多维向量数组的技术，这些向量可以代表任何事物，如文本、音乐、视频等[2]。有几种不同的Embedding模型，它们各有其优势和应用领域。</p> 
<ol><li><strong>Word2vec</strong>：这是一种经典的嵌入模型，通过学习单词之间的长距离依赖关系来生成向量。它的优势在于能够有效地捕捉到词语之间的关系，使得模型能够理解和处理复杂的文本结构[4][5]。</li><li><strong>众安Embedding模型</strong>：在中文通用FAQ数据集上，众安Embedding模型表现出色，尽管优势已不如以前那么大[3]。这表明该模型在中文数据集上具有较好的性能。</li><li><strong>Nomic Embed</strong>：这是首个开源、开放数据、开放权重、开放训练代码、完全可复现和可审核的嵌入模型。它的上下文长度为8192，显示出在特定场景下的高效性[18]。</li><li><strong>OpenAI Embedding Models</strong>：OpenAI推出了一系列新型嵌入模型，包括更小、高效的text-embedding-3-small模型和更大、更强大的text-embedding-3-large模型。这些模型具有更低的定价，适用于表示自然语言或代码等内容中的概念[21]。</li></ol> 
<p>每种Embedding模型都有其独特的优势，例如Word2vec擅长处理复杂的文本结构，而众安Embedding模型在中文数据集上表现优异。Nomic Embed则以其开源和可复现的特性受到开发者的青睐。OpenAI的嵌入模型则提供了更多样化的模型选择，满足不同应用场景的需求。</p> 
<h3>Embedding模型在处理中文数据时的具体优势和挑战是什么？</h3> 
<ol><li><strong>语义理解能力</strong>：Embedding模型通过词向量来表示文本，能够捕捉到词汇之间的语义联系，相比之下，基于关键词的检索往往关注字面匹配，可能忽略了词语之间的语义联系[24]。这一点在搜索引擎、构建私有知识问答系统、内容推荐系统等应用中尤为重要[27]。</li><li><strong>容错性</strong>：基于Embedding的方法能够理解词汇之间的关系，从而提高了模型的容错性[24]。在面对错误或不完整的数据时，Embedding模型能够更好地保持其准确性和可靠性。</li><li><strong>支持中文</strong>：一些Embedding模型，如BGE，对中文数据的支持效果较好，是中文embedding模型中为数不多的优质选择[28]。这表明在中文数据处理方面，Embedding模型具有较强的应用潜力。</li></ol> 
<p>然而，Embedding模型在处理中文数据时也面临着挑战：</p> 
<ol><li><strong>性能差异</strong>：尽管有些模型在中文处理上有优势，但不同模型的性能可能存在显著差异。例如，text2vec在STS-B测试集中的效果优势下降，与MiniLM效果相近[23]。这意味着在选择Embedding模型时，需要考虑到不同模型的性能表现。</li><li><strong>模型部署和优化</strong>：虽然有些模型已经开源并方便本地私有化部署[26]，但在实际应用中，如何有效地部署和优化这些模型，以适应特定的业务需求，仍然是一个挑战。</li><li><strong>特定领域的适用性</strong>：特定领域的专有模型通常比通用模型表现更好，尤其是当模型的参数量较小时[29]。这提示我们在处理特定任务时，可能需要考虑使用专门针对该任务的Embedding模型，而不是通用的Embedding模型。</li></ol> 
<p>Embedding模型在处理中文数据时的优势在于其强大的语义理解能力、高容错性以及对中文数据的良好支持。然而，选择合适的模型、有效部署和优化模型以及根据特定任务选择专有模型等挑战，也是处理中文数据过程中需要注意的问题。</p> 
<h3>Word2vec与其他Embedding模型（如Nomic Embed和OpenAI Embedding Models）在性能上的比较研究有哪些？</h3> 
<ol><li><strong>模型架构和训练方式的差异</strong>：Word2vec和其他模型如Bert、GPT-3等，虽然都利用了单词的周边信息，但它们在使用周边信息的方式、模型架构以及训练方式上存在显著差异。这些差异导致了对单词的表征效果有所不同[32]。例如，Word2vec模型结构相对简单，主要通过连续词袋模型（CBOW）和Skip-gram模型来处理输入数据，以降低模型复杂度并在大规模数据上进行训练[38][39]。而OpenAI的文本嵌入模型，如OpenAI的text embedding 002，通常基于更复杂的深度学习模型[38]。</li><li><strong>性能的具体比较</strong>：尽管没有直接的性能比较结果被明确提及，但是从现有的研究中可以推断出不同模型的性能比较。例如，有研究对比了GPT-3、Bert、GloVe与Word2vec在性能上的差异[37]。此外，OpenAI公布的embedding endpoint也是基于神经网络模型，将文本和代码转换为向量表示，嵌入到高维空间中[33]。这表明OpenAI的文本嵌入模型在技术实现上可能更为先进。</li><li><strong>应用场景的差异</strong>：Word2vec因其简单的模型结构和高效的训练方法，特别适合于需要大规模处理能力的应用场景，如文本分类、情感分析等[34]。而OpenAI的文本嵌入模型则更侧重于链接大模型与外部知识，适用于需要处理复杂语义信息和跨领域知识的应用场景[37]。</li></ol> 
<p>Word2vec与其他Embedding模型在性能上的比较研究显示了各自的优势和局限性。Word2vec以其简单高效的特点，在特定的应用场景下表现出色；而OpenAI的文本嵌入模型则在技术实现和应用范围上展现出更多的灵活性和复杂性。</p> 
<h3>如何优化Embedding模型以提高其在特定领域的应用效果？</h3> 
<ol><li><strong>领域特定模型训练</strong>：针对特定领域（如医疗、法律）训练Embedding模型，以提高在特定上下文中的准确性[42]。这意味着在训练Embedding模型时，需要考虑到特定领域的特定需求和特点，以确保模型能够更好地适应这些领域。</li><li><strong>基于组合的方法优化</strong>：不应局限于使用q和r两个矩阵，而是可以采用q，r，z，k等多个矩阵的组合方式来优化embedding层。这种方法虽然能降低embedding的参数量，但可能会显著影响模型的性能[41]。因此，选择合适的矩阵组合方式对于模型的优化至关重要。</li><li><strong>利用开源模型资源</strong>：参考全球权威Embedding评测榜单上排名靠前的开源模型，如数元灵开源的Embedding模型，可以为AI Native应用开发提供强大的支持[43]。开源资源通常经过了广泛的应用验证，能够有效提高模型的应用效果。</li><li><strong>优化算法的选择</strong>：在广告/推荐领域，可以使用针对性的优化算法，如FTRL（Fine-tuning with Regularization），这种方法适用于对高维稀疏模型进行训练[44]。此外，Deep Hash Embedding (DHE)也是一种有效的优化方法，特别是当字典大小过大时，DHE能够有效压缩Embedding[45]。</li><li><strong>fine-tuning技术应用</strong>：通过使用特定领域的标注数据，对模型的权重进行微调，可以使模型更好地理解和处理特定领域的文本数据[46]。这种技术的应用可以显著提升模型在特定领域中的表现。</li><li><strong>稀疏特征的优化表示</strong>：对于推荐系统中存在海量稀疏特征的问题，可以通过优化表示方法来解决。例如，通过手工测试来寻找好的Embedding大小，或者采用其他稀疏特征Embedding的优化方法[49]。</li><li><strong>RAG和微调技术的结合使用</strong>：在提升大语言模型性能的过程中，检索增强生成（RAG）和微调（Fine-tuning）两种方法都有其优势。根据微软的指南，在建设特定领域的应用时，可以根据具体情况选择更高效的方法[50]。</li></ol> 
<p>优化Embedding模型的关键在于深入理解特定领域的需求，合理选择和组合优化方法，以及利用开源资源和先进的技术手段。通过这些方法，可以有效提高Embedding模型在特定领域的应用效果。</p> 
<h3>Embedding模型在自然语言处理以外的应用案例有哪些？</h3> 
<p>Embedding模型在自然语言处理（NLP）之外的应用案例主要包括计算机视觉（CV）领域。Embedding模型能够将高维度的数据转化为低维度的向量空间，这一特性使得它在处理图像数据时表现出色[52]。例如，在计算机视觉中，Embedding模型可以用于图像分类、目标检测、图像分割等任务中。通过将图像中的特征映射到低维度的向量空间，Embedding模型能够捕捉到图像的深层信息，从而提高这些任务的性能[53]。</p> 
<p>此外，Embedding模型在其他领域的应用也逐渐增多，尽管文献中没有直接提及具体的应用案例，但根据其在自然语言处理和计算机视觉中的应用背景，我们可以推断出Embedding模型在图像识别、视频分析等领域也有潜在的应用价值。例如，在图像识别领域，Embedding模型可以用于训练模型，以识别和分类不同类型的图像。在视频分析中，Embedding模型可以用于理解和分析视频内容，如动作识别、情感分析等[54]。</p> 
<p>虽然文献中没有直接列出Embedding模型在除自然语言处理外的其他应用案例，但根据其在自然语言处理和计算机视觉中的应用背景，我们可以合理推测Embedding模型在图像识别、视频分析等领域也有广泛的应用潜力。</p> 
<h3>最新的Embedding模型技术发展趋势是什么？</h3> 
<ol><li><strong>多阶段训练过程的引入</strong>：BGE M3-Embedding采用了多阶段训练过程，结合自动编码和弱监督对比学习，这种方式不仅可以提高模型的性能，还能避免预训练方式之间的冲突，预示着未来Embedding模型技术可能会发展出更多创新的训练方式[59]。</li><li><strong>模型大小的优化</strong>：OpenAI推出的text-embedding-3-small模型和text-embedding-3-large模型，分别代表了更小且高效以及更大且更强大的文本嵌入模型，这表明了在嵌入模型设计上追求性能和效率的平衡[60]。</li><li><strong>技术和成本的权衡</strong>：OpenAI新模型使用的嵌入技术允许开发人员在使用嵌入时权衡使用嵌入的性能和成本，通过在dimensions API参数中传递嵌入而不丢失其概念，这种方法有助于开发者更灵活地应用嵌入技术[61]。</li><li><strong>多模态模型的发展</strong>：GPT-4作为GPT系列中的最新版本，支持图像和文本类型的输入，这是一个重要的发展趋势，标志着Embedding模型技术正在向支持更广泛输入类型（如图像）的方向发展[64]。</li><li><strong>开源和复现的重要性</strong>：Nomic Embed模型的推出，作为首个开源、开放数据、开放权重、开放训练代码的嵌入模型，强调了开源和复现在技术发展中的重要性，这对于推动技术的普及和创新具有积极作用[67][68]。</li></ol> 
<p>最新的Embedding模型技术发展趋势是向着多阶段训练、模型大小优化、技术与成本的权衡、多模态支持以及开源和复现的方向发展，这些趋势共同推动了Embedding模型技术的进步和应用范围的扩大。</p> 
<p>参考资料</p> 
<p>[1]. <a href="https://www.cnblogs.com/xy1997/p/17386756.html" rel="nofollow" title="【AIGC】Embedding与LLM：token长度限制困局下 - 博客园">【AIGC】Embedding与LLM：token长度限制困局下 - 博客园</a></p> 
<p>[2]. <a href="https://zhuanlan.zhihu.com/p/629362202" rel="nofollow" title="【AIGC】All you need is Embedding？Token长度困境下的泛长文本LLM应用 - 知乎">【AIGC】All you need is Embedding？Token长度困境下的泛长文本LLM应用 - 知乎</a></p> 
<p>[3]. <a href="https://juejin.cn/post/7277887015538753573" rel="nofollow" title="优雅！比OpenAI更认真的文本嵌入模型 - 稀土掘金">优雅！比OpenAI更认真的文本嵌入模型 - 稀土掘金</a></p> 
<p>[4]. <a href="https://www.rongpm.com/column/embeddings-yhv-ai.html" rel="nofollow" title="AIGC｜Embeddings入门详解！手把手带你训练 - 融·项目管理">AIGC｜Embeddings入门详解！手把手带你训练 - 融·项目管理</a></p> 
<p>[5]. <a href="https://zhuanlan.zhihu.com/p/672281354" rel="nofollow" title="AIGC｜Embeddings入门详解!手把手带你训练 - 知乎">AIGC｜Embeddings入门详解!手把手带你训练 - 知乎</a></p> 
<p>[6]. <a href="https://www.163.com/dy/article/INKS1CF9055656SZ.html" rel="nofollow" title="大模型应用实践：AIGC探索之旅 - 网易">大模型应用实践：AIGC探索之旅 - 网易</a></p> 
<p>[7]. <a href="https://pdf.dfcfw.com/pdf/H3_AP202302131583073056_1.pdf?1676277416000.pdf" rel="nofollow" title="[PDF] AIGC 专题一：探析AIGC 的技术发展和应用">[PDF] AIGC 专题一：探析AIGC 的技术发展和应用</a></p> 
<p>[8]. <a href="https://www.aiyzh.com/aishow/aitech/125/" rel="nofollow" title="【AIGC】All you need is Embedding？Token长度困境下的泛长文本 ...">【AIGC】All you need is Embedding？Token长度困境下的泛长文本 ...</a></p> 
<p>[9]. <a href="https://cloud.tencent.com/developer/article/2392072" rel="nofollow" title="ICCV 2023 | 最全AIGC梳理，5w字30个diffusion扩散模型方向，近百篇论文!-腾讯云开发者社区-腾讯云">ICCV 2023 | 最全AIGC梳理，5w字30个diffusion扩散模型方向，近百篇论文!-腾讯云开发者社区-腾讯云</a></p> 
<p>[11]. <a href="https://aigc.luomor.com/2024/01/08/%E5%A4%A7%E6%A8%A1%E5%9E%8Brag%E9%97%AE%E7%AD%94%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84%E5%8F%8A%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97%E5%9B%9E%E9%A1%BE%EF%BC%9A%E4%BB%8Eembedding%E3%80%81prompt-embedding/" rel="nofollow" title="大模型RAG问答技术架构及核心模块回顾：从Embedding - 文心AIGC">大模型RAG问答技术架构及核心模块回顾：从Embedding - 文心AIGC</a></p> 
<p>[12]. <a href="https://www.thepaper.cn/newsDetail_forward_21645171" rel="nofollow" title="AIGC时代已来，跨模态内容生成技术发展得怎么样了_澎湃号·湃客_澎湃新闻-The Paper">AIGC时代已来，跨模态内容生成技术发展得怎么样了_澎湃号·湃客_澎湃新闻-The Paper</a></p> 
<p>[13]. <a href="https://www.modb.pro/db/1694163621601431552" rel="nofollow" title="前沿|AIGC起飞！通过数据库为AI大模型注入“持久记忆” - 墨天轮">前沿|AIGC起飞！通过数据库为AI大模型注入“持久记忆” - 墨天轮</a></p> 
<p>[14]. <a href="https://zhuanlan.zhihu.com/p/607822576" rel="nofollow" title="万字长文：Aigc技术与应用全解析 - 知乎 - 知乎专栏">万字长文：Aigc技术与应用全解析 - 知乎 - 知乎专栏</a></p> 
<p>[15]. <a href="https://cloud.tencent.com/developer/article/2307044" rel="nofollow" title="AIGC之文本和图片生成向量 - 腾讯云">AIGC之文本和图片生成向量 - 腾讯云</a></p> 
<p>[16]. <a href="https://blog.csdn.net/Taobaojishu/article/details/135376567" title="大模型应用实践：Aigc探索之旅-csdn博客">大模型应用实践：Aigc探索之旅-csdn博客</a></p> 
<p>[17]. <a href="https://blog.csdn.net/u013716859/article/details/135349595" title="[AIGC] 如何在Stable Diffusion 中使用embedding(嵌入) - CSDN博客">[AIGC] 如何在Stable Diffusion 中使用embedding(嵌入) - CSDN博客</a></p> 
<p>[18]. <a href="https://www.skycaiji.com/aigc/ai9438.html" rel="nofollow" title="数据、代码全开源，能完美复现的嵌入模型Nomic Embed来了- AIGC">数据、代码全开源，能完美复现的嵌入模型Nomic Embed来了- AIGC</a></p> 
<p>[19]. <a href="https://zhuanlan.zhihu.com/p/644893859" rel="nofollow" title="探索大模型智能：众安保险基于AIGC 的应用实践 - 知乎专栏">探索大模型智能：众安保险基于AIGC 的应用实践 - 知乎专栏</a></p> 
<p>[20]. <a href="https://testerhome.com/topics/37001?order_by=like&amp;" rel="nofollow" title="史上最全，细数AIGC 在测试领域落地的困难点 - TesterHome">史上最全，细数AIGC 在测试领域落地的困难点 - TesterHome</a></p> 
<p>[21]. <a href="https://top.aibase.com/tool/openai-embedding-models" rel="nofollow" title="OpenAI Embedding Models是一系列新型嵌入模型 - AIbase">OpenAI Embedding Models是一系列新型嵌入模型 - AIbase</a></p> 
<p>[22]. <a href="https://www.secrss.com/articles/61907" rel="nofollow" title="基于生成式因果语言模型的水印嵌入与检测 - 安全内参">基于生成式因果语言模型的水印嵌入与检测 - 安全内参</a></p> 
<p>[23]. <a href="https://zhuanlan.zhihu.com/p/635670918" rel="nofollow" title="langchain(2)—基于开源embedding模型的中文向量效果测试 - 知乎">langchain(2)—基于开源embedding模型的中文向量效果测试 - 知乎</a></p> 
<p>[24]. <a href="https://zhuanlan.zhihu.com/p/647646322" rel="nofollow" title="文本 Embedding 基本概念和应用实现原理 - 知乎 - 知乎专栏">文本 Embedding 基本概念和应用实现原理 - 知乎 - 知乎专栏</a></p> 
<p>[25]. <a href="https://www.sohu.com/a/710126518_473283" rel="nofollow" title="智源开源最强语义向量模型BGE！中英文测评全面超过OpenAI、Meta">智源开源最强语义向量模型BGE！中英文测评全面超过OpenAI、Meta</a></p> 
<p>[26]. <a href="https://www.cnblogs.com/xiaoxi666/p/18014457" rel="nofollow" title="Embedding 模型部署及效果评测- xiaoxi666 - 博客园">Embedding 模型部署及效果评测- xiaoxi666 - 博客园</a></p> 
<p>[27]. <a href="https://segmentfault.com/a/1190000044075300" rel="nofollow" title="技术分享 - 文本 Embedding 基本概念和应用实现原理 - Inside Dify - SegmentFault 思否">技术分享 - 文本 Embedding 基本概念和应用实现原理 - Inside Dify - SegmentFault 思否</a></p> 
<p>[28]. <a href="https://www.wehelpwin.com/article/4181" rel="nofollow" title="Embedding开源模型重磅玩家：北京智源人工智能研究院最新 ...">Embedding开源模型重磅玩家：北京智源人工智能研究院最新 ...</a></p> 
<p>[29]. <a href="https://xie.infoq.cn/article/56c0ecebb28fa6b8d5aeb604e" rel="nofollow" title="优雅!比 OpenAI 更认真的文本嵌入模型 - InfoQ 写作社区">优雅!比 OpenAI 更认真的文本嵌入模型 - InfoQ 写作社区</a></p> 
<p>[30]. <a href="https://zhuanlan.zhihu.com/p/657827660" rel="nofollow" title="炼不出垂直大模型? 试试垂直Embedding - 知乎专栏">炼不出垂直大模型? 试试垂直Embedding - 知乎专栏</a></p> 
<p>[31]. <a href="https://zhuanlan.zhihu.com/p/679166797" rel="nofollow" title="[NLP]中文Embedding模型优劣数据评测分析报告(超详细) - 知乎">[NLP]中文Embedding模型优劣数据评测分析报告(超详细) - 知乎</a></p> 
<p>[32]. <a href="https://zhuanlan.zhihu.com/p/352842845" rel="nofollow" title="word2vec 与bert 的embedding区别 - 知乎专栏">word2vec 与bert 的embedding区别 - 知乎专栏</a></p> 
<p>[33]. <a href="https://blog.csdn.net/deephub/article/details/128992344" title="GPT-3 vs Bert vs GloVe vs Word2vec 文本嵌入技术的性能对比测试原创">GPT-3 vs Bert vs GloVe vs Word2vec 文本嵌入技术的性能对比测试原创</a></p> 
<p>[34]. <a href="https://www.zhihu.com/question/53354714" rel="nofollow" title="word2vec和word embedding有什么区别? - 知乎">word2vec和word embedding有什么区别? - 知乎</a></p> 
<p>[35]. <a href="https://zhuanlan.zhihu.com/p/656227178" rel="nofollow" title="优雅！比OpenAI更认真的文本嵌入模型 - 知乎专栏">优雅！比OpenAI更认真的文本嵌入模型 - 知乎专栏</a></p> 
<p>[36]. <a href="https://zhuanlan.zhihu.com/p/330030367" rel="nofollow" title="论文｜万物皆可Vector之Word2vec：2个模型、2个优化及实战使用 - 知乎">论文｜万物皆可Vector之Word2vec：2个模型、2个优化及实战使用 - 知乎</a></p> 
<p>[37]. <a href="https://cloud.tencent.com/developer/news/1151010" rel="nofollow" title="链接大模型与外部知识，智源开源最强语义向量模型BGE - 腾讯云">链接大模型与外部知识，智源开源最强语义向量模型BGE - 腾讯云</a></p> 
<p>[38]. <a href="https://zhuanlan.zhihu.com/p/672817686" rel="nofollow" title="论文速读: word2vec (CBOW+Skip-gram) - 知乎专栏">论文速读: word2vec (CBOW+Skip-gram) - 知乎专栏</a></p> 
<p>[39]. <a href="https://www.cnblogs.com/gogoSandy/p/13418257.html" rel="nofollow" title="无所不能的Embedding1 - 词向量三巨头之Word2vec模型详解&amp;代码实现">无所不能的Embedding1 - 词向量三巨头之Word2vec模型详解&amp;代码实现</a></p> 
<p>[40]. <a href="https://blog.csdn.net/weixin_45123595/article/details/108217202" title="Word2vec vs Bert 系列技术要点原创 - CSDN博客">Word2vec vs Bert 系列技术要点原创 - CSDN博客</a></p> 
<p>[41]. <a href="https://zhuanlan.zhihu.com/p/516610026" rel="nofollow" title="关于embedding layer的一些优化（待续） - 知乎专栏">关于embedding layer的一些优化（待续） - 知乎专栏</a></p> 
<p>[42]. <a href="https://www.woshipm.com/ai/6003413.html" rel="nofollow" title="深度解码：产品经理如何驾驭Embedding(嵌入)技术以革新产品体验">深度解码：产品经理如何驾驭Embedding(嵌入)技术以革新产品体验</a></p> 
<p>[43]. <a href="https://zhuanlan.zhihu.com/p/680400660" rel="nofollow" title='数元灵夺得全球权威Embedding评测榜单开源模型第一名，开放大模型"知识外挂"，赋能AI Native应用开发 - 知乎'>数元灵夺得全球权威Embedding评测榜单开源模型第一名，开放大模型"知识外挂"，赋能AI Native应用开发 - 知乎</a></p> 
<p>[44]. <a href="https://qiankunli.github.io/2022/03/02/embedding.html" rel="nofollow" title="embedding的原理及实践| 李乾坤的博客">embedding的原理及实践| 李乾坤的博客</a></p> 
<p>[45]. <a href="https://www.zhihu.com/question/522006535" rel="nofollow" title="推荐系统embedding过大如何压缩一下？最近正在解决这个问题？ - 知乎">推荐系统embedding过大如何压缩一下？最近正在解决这个问题？ - 知乎</a></p> 
<p>[46]. <a href="https://developer.baidu.com/article/details/1815661" rel="nofollow" title="大模型训练中的Embedding与fine-tuning：个性化应用的关键">大模型训练中的Embedding与fine-tuning：个性化应用的关键</a></p> 
<p>[47]. <a href="https://zhuanlan.zhihu.com/p/673483110" rel="nofollow" title="高级RAG(一)：Embedding模型的选择 - 知乎 - 知乎专栏">高级RAG(一)：Embedding模型的选择 - 知乎 - 知乎专栏</a></p> 
<p>[48]. <a href="https://zhuanlan.zhihu.com/p/141517705" rel="nofollow" title="推荐系统中稀疏特征Embedding的优化表示方法 - 知乎专栏">推荐系统中稀疏特征Embedding的优化表示方法 - 知乎专栏</a></p> 
<p>[49]. <a href="https://zhuanlan.zhihu.com/p/269023909" rel="nofollow" title="稀疏特征Embedding的优化 - 知乎 - 知乎专栏">稀疏特征Embedding的优化 - 知乎 - 知乎专栏</a></p> 
<p>[50]. <a href="https://www.thepaper.cn/newsDetail_forward_26367994" rel="nofollow" title="RAG还是微调？微软出了一份特定领域大模型应用建设流程指南_澎湃号·湃客_澎湃新闻-The Paper">RAG还是微调？微软出了一份特定领域大模型应用建设流程指南_澎湃号·湃客_澎湃新闻-The Paper</a></p> 
<p>[51]. <a href="https://zhuanlan.zhihu.com/p/647280030" rel="nofollow" title="为什么Embedding模型在大语言模型中很重要？ - 知乎专栏">为什么Embedding模型在大语言模型中很重要？ - 知乎专栏</a></p> 
<p>[52]. <a href="https://fuxi.163.com/database/1076" rel="nofollow" title="embedding模型是什么？ - 网易伏羲">embedding模型是什么？ - 网易伏羲</a></p> 
<p>[53]. <a href="https://blog.csdn.net/Dreamershi/article/details/132079772" title="Embedding入门介绍以及为什么Embedding在大语言模型中很重要原创">Embedding入门介绍以及为什么Embedding在大语言模型中很重要原创</a></p> 
<p>[54]. <a href="https://zhuanlan.zhihu.com/p/628148903" rel="nofollow" title="AI大模型领域的热门技术——Embedding入门介绍以及为什么Embedding在大语言模型中很重要 - 知乎">AI大模型领域的热门技术——Embedding入门介绍以及为什么Embedding在大语言模型中很重要 - 知乎</a></p> 
<p>[55]. <a href="https://www.51cto.com/article/619937.html" rel="nofollow" title="案例详解| 基于Embedding的特征安全计算 - 51CTO">案例详解| 基于Embedding的特征安全计算 - 51CTO</a></p> 
<p>[56]. <a href="https://zhuanlan.zhihu.com/p/667877230" rel="nofollow" title="大模型里面常说的Embedding是什么？ - 知乎专栏">大模型里面常说的Embedding是什么？ - 知乎专栏</a></p> 
<p>[57]. <a href="http://onedreame.github.io/2020/08/13/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84embeddings/" rel="nofollow" title="自然语言处理中的embeddings - 杨康的博客| OD Blog">自然语言处理中的embeddings - 杨康的博客| OD Blog</a></p> 
<p>[58]. <a href="https://blog.csdn.net/dzysunshine/article/details/133674250" title="Text embedding 模型总结_bge嵌入模型-CSDN博客">Text embedding 模型总结_bge嵌入模型-CSDN博客</a></p> 
<p>[59]. <a href="https://zhuanlan.zhihu.com/p/681259219" rel="nofollow" title="BGE M3-Embedding：智源最新发布的text embedding模型，多语言检索效果吊打微软跟openai">BGE M3-Embedding：智源最新发布的text embedding模型，多语言检索效果吊打微软跟openai</a></p> 
<p>[60]. <a href="https://www.jiqizhixin.com/articles/2024-01-29-9" rel="nofollow" title="OpenAI新模型用的嵌入技术被网友扒出来了 - 机器之心">OpenAI新模型用的嵌入技术被网友扒出来了 - 机器之心</a></p> 
<p>[61]. <a href="https://cloud.tencent.com/developer/article/2385297" rel="nofollow" title="揭秘！OpenAI新模型使用的：嵌入(Embedding)技术 - 腾讯云">揭秘！OpenAI新模型使用的：嵌入(Embedding)技术 - 腾讯云</a></p> 
<p>[62]. <a href="https://cloud.tencent.com/developer/article/2363350" rel="nofollow" title="「X」Embedding in NLP｜一文读懂 2023 年最流行的 20 个 NLP 模型-腾讯云开发者社区-腾讯云">「X」Embedding in NLP｜一文读懂 2023 年最流行的 20 个 NLP 模型-腾讯云开发者社区-腾讯云</a></p> 
<p>[63]. <a href="https://zhuanlan.zhihu.com/p/684899047" rel="nofollow" title="2024年大模型最快的应用落地技术-Embedding向量优化Matryoshka">2024年大模型最快的应用落地技术-Embedding向量优化Matryoshka</a></p> 
<p>[64]. [「X」Embedding in NLP｜一文读懂2023 年最流行的20 个NLP 模型](https://zilliz.com.cn/blog/nlp models-nlp-zilliz)</p> 
<p>[65]. <a href="https://hub.baai.ac.cn/view/34151" rel="nofollow" title="大模型RAG问答技术架构及核心模块回顾：从Embedding、prompt-embedding到Reranker - 智源社区">大模型RAG问答技术架构及核心模块回顾：从Embedding、prompt-embedding到Reranker - 智源社区</a></p> 
<p>[66]. <a href="https://zhuanlan.zhihu.com/p/427435466" rel="nofollow" title="Embedding+MLP 最经典的深度学习模型（以 Deep Crossing 深度学习推荐模型为例） - 知乎">Embedding+MLP 最经典的深度学习模型（以 Deep Crossing 深度学习推荐模型为例） - 知乎</a></p> 
<p>[67]. <a href="https://finance.sina.com.cn/tech/roll/2024-02-03/doc-inafttxn3069736.shtml" rel="nofollow" title="击败OpenAI，权重、数据、代码全开源，能完美复现的嵌入模型Nomic Embed来了|基准_新浪科技_新浪网">击败OpenAI，权重、数据、代码全开源，能完美复现的嵌入模型Nomic Embed来了|基准_新浪科技_新浪网</a></p> 
<p>[68]. <a href="https://www.jiqizhixin.com/articles/2024-02-04-3" rel="nofollow" title="权重、数据、代码全开源，能完美复现的嵌入模型Nomic Embed来了">权重、数据、代码全开源，能完美复现的嵌入模型Nomic Embed来了</a></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d086a3696d3b898cef0a06e76835e81f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">mac python下载安装教程,python在mac上怎么下载</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/18a2c3d429c8785db3f4d5737a57886e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">怎样通过小红书AI绘画赚钱？AI艺术创收日入2900</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程学习者.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>