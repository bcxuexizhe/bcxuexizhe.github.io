<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>WSL &#43; Vscode一站式搭建Hadoop伪分布式 &#43; Spark环境 - 编程学习者</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcxuexizhe.github.io/posts/81d56e61afbda2c1170bb10d93ad3694/">
  <meta property="og:site_name" content="编程学习者">
  <meta property="og:title" content="WSL &#43; Vscode一站式搭建Hadoop伪分布式 &#43; Spark环境">
  <meta property="og:description" content="Wsl &#43; Vscode一站式搭建Hadoop &#43; Spark环境 想要搭建Linux、Hadoop、Spark等环境,现在通常的做法是在VM、Virtualbox等软件上安装虚拟机
本文介绍在windows子系统(Windows Subsystem for Linux)上搭建相关环境并使用vscode进行Spark程序开发
Wsl环境准备 wsl安装文档详细请见&lt;设置 WSL 开发环境 | Microsoft Learn&gt;
PowerShell中键入ubuntu即可进入wsl环境
注意到windows文件资源管理器已经多了一只小企鹅
搭建Hadoop伪分布式环境 资源准备 修改/opt目录权限:
sudo chown -R yourname /opt # 以下的用户名xuxin替换为用户姓名.
在/opt目录下新建两个文件夹module 和 software
/opt/software目录下准备jdk-1.8和hadoop-3.2.3
jdk:&lt; Java Downloads | Oracle&gt;
hadoop:&lt;Apache Hadoop&gt;
将下载好的文件直接复制粘贴进/opt/software目录下
解压文件
tar -zxvf /opt/software/jdk-8u212-linux-x64.tar.gz -C /opt/module/ tar -zxvf /opt/software/hadoop-3.2.3.tar.gz -C /opt/module/ 配置ssh服务 安装ssh server
sudo apt install openssh-server配置免密登录 cd ~/.ssh/ # 若没有该目录,先执行ssh localhost ssh-keygen -t rsa # 出现提示,全部回车即可 cat .">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-11-23T18:14:02+08:00">
    <meta property="article:modified_time" content="2023-11-23T18:14:02+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程学习者" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程学习者</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">WSL &#43; Vscode一站式搭建Hadoop伪分布式 &#43; Spark环境</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="Wsl__VscodeHadoop__Spark_0"></a>Wsl + Vscode一站式搭建Hadoop + Spark环境</h2> 
<p>想要搭建Linux、Hadoop、Spark等环境,现在通常的做法是在VM、Virtualbox等软件上安装虚拟机<br> 本文介绍在windows子系统(<em>Windows Subsystem for Linux</em>)上搭建相关环境并使用vscode进行Spark程序开发</p> 
<h3><a id="Wsl_4"></a>Wsl环境准备</h3> 
<p>wsl安装文档详细请见&lt;<a href="https://learn.microsoft.com/zh-cn/windows/wsl/setup/environment" rel="nofollow">设置 WSL 开发环境 | Microsoft Learn</a>&gt;<br> PowerShell中键入<code>ubuntu</code>即可进入wsl环境<br> 注意到windows文件资源管理器已经多了一只小企鹅<br> <img src="https://images2.imgbox.com/57/e0/D9YJslYJ_o.png" alt="文件资源管理器"></p> 
<h3><a id="Hadoop_10"></a>搭建Hadoop伪分布式环境</h3> 
<h4><a id="_11"></a>资源准备</h4> 
<ul><li> <p>修改/opt目录权限:<br> <code>sudo chown -R yourname /opt # 以下的用户名xuxin替换为用户姓名.</code></p> </li><li> <p>在/opt目录下新建两个文件夹<strong>module</strong> 和 <strong>software</strong><br> /opt/software目录下准备<strong>jdk-1.8</strong>和<strong>hadoop-3.2.3</strong><br> <strong>jdk</strong>:&lt; <a href="https://www.oracle.com/java/technologies/downloads/" rel="nofollow">Java Downloads | Oracle</a>&gt;<br> <strong>hadoop</strong>:&lt;<a href="https://hadoop.apache.org/" rel="nofollow">Apache Hadoop</a>&gt;</p> </li><li> <p>将下载好的文件直接复制粘贴进/opt/software目录下<br> <img src="https://images2.imgbox.com/64/a3/J3WTWSDS_o.png" alt="![[Pasted image 20231112164154.png]]"></p> </li><li> <p>解压文件</p> </li></ul> 
<pre><code>tar -zxvf /opt/software/jdk-8u212-linux-x64.tar.gz -C /opt/module/
tar -zxvf /opt/software/hadoop-3.2.3.tar.gz -C /opt/module/
</code></pre> 
<h4><a id="ssh_29"></a>配置ssh服务</h4> 
<ul><li>安装<strong>ssh server</strong><br> <code>sudo apt install openssh-server</code></li><li>配置免密登录</li></ul> 
<pre><code>cd ~/.ssh/    # 若没有该目录,先执行ssh localhost
ssh-keygen -t rsa    # 出现提示,全部回车即可
cat ./id_rsa.pub &gt;&gt; ./authorized_keys
</code></pre> 
<h4><a id="Java_38"></a>配置Java环境</h4> 
<ul><li>修改.bashrc文件<br> <code>vim ~/.bashrc</code><br> 添加以下内容</li></ul> 
<pre><code>export JAVA_HOME=/opt/module/jdk1.8.0_212
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
export PATH=${JAVA_HOME}/bin:$PATH
</code></pre> 
<ul><li> <p>使配置文件生效<br> <code>source ~/.bashrc</code></p> </li><li> <p>出现以下内容表示java环境搭建完成<br> <code>java -version</code><br> <img src="https://images2.imgbox.com/4d/75/tEyGZpb9_o.png" alt="java"></p> </li></ul> 
<h4><a id="hadoop_57"></a>配置hadoop环境</h4> 
<ul><li>修改<strong>core-site.xml</strong>文件</li></ul> 
<pre><code>cd /opt/module/hadoop-3.2.3
vim ./etc/hadoop/core-site.xml
</code></pre> 
<ul><li>改为如下配置</li></ul> 
<pre><code>&lt;configuration&gt;

&lt;property&gt;
&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
&lt;value&gt;file:/opt/module/hadoop-3.2.3/tmp&lt;/value&gt;
&lt;description&gt;Abase for other temporary directories.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
&lt;name&gt;fs.defaultFS&lt;/name&gt;
&lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
&lt;name&gt;hadooop.http.staticuser.user&lt;/name&gt;
&lt;value&gt;xuxin&lt;/value&gt;
&lt;/property&gt;

&lt;/configuration&gt;
</code></pre> 
<ul><li>同样修改hdfs.xml文件</li></ul> 
<pre><code>cd /opt/module/hadoop-3.2.3
vim ./etc/hadoop/hdfs-site.xml
</code></pre> 
<ul><li>改为如下配置</li></ul> 
<pre><code>&lt;configuration&gt;

&lt;property&gt;
&lt;name&gt;dfs.replication&lt;/name&gt;
&lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
&lt;value&gt;file:/opt/module/hadoop-3.2.3/tmp/dfs/name&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
&lt;value&gt;file:/opt/module/hadoop-3.2.3/tmp/dfs/data&lt;/value&gt;
&lt;/property&gt;

&lt;/configuration&gt;
</code></pre> 
<ul><li>namenode初始化</li></ul> 
<pre><code>cd /opt/module/hadoop-3.2.3
./bin/hdfs namenode -format
</code></pre> 
<ul><li> <p>出现类似日志信息表示初始化成功<br> <img src="https://images2.imgbox.com/40/88/PnCFgEJR_o.png" alt="namenode"></p> </li><li> <p>启动hdfs</p> </li></ul> 
<pre><code>cd /opt/module/hadoop-3.2.3/
./sbin/start-dfs.sh
</code></pre> 
<ul><li> <p>浏览器访问localhost:9870可访问web页面<br> Utilities -&gt; Browse the file system 查看hdfs文件系统</p> </li><li> <p>如果需要web端进行操作,可以在Hadoop目录下关闭安全模式<br> <code>./bin/hadoop dfsadmin -safemode leave</code></p> </li></ul> 
<h4><a id="wordcount_137"></a>运行wordcount示例代码</h4> 
<ul><li>创建 test.txt文件</li></ul> 
<pre><code>cd /opt/module/hadoop-3.2.3/
mkdir input
vim test.txt
</code></pre> 
<ul><li>测试文件内容</li></ul> 
<pre><code>I learn C language
I like Java
I do not like Python
</code></pre> 
<ul><li> <p>hdfs创建用户目录<br> <code>./bin/hadoop fs -mkdir -p /user/xuxin</code></p> </li><li> <p>由于指定了用户目录,因此在命令中就可以使用相对路径如 <em>input</em>，其对应的绝对路径就是 <em>/user/xuxin/input</em><br> <code>./bin/hadoop fs -put ./input input</code></p> </li><li> <p>在web端可见已经上传成功<br> <img src="https://images2.imgbox.com/91/e4/D5d7ikpx_o.png" alt=""></p> </li><li> <p>运行实例wordcount代码<br> <code>./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar wordcount input output</code></p> </li><li> <p>查看output输出<br> <code>./bin/hadoop fs -cat output/*</code><br> <img src="https://images2.imgbox.com/3b/9f/grntf4By_o.png" alt="wordcount"></p> </li></ul> 
<p><em><strong>OK,至此成功完成hadoop环境准备</strong></em></p> 
<h3><a id="Spark_170"></a>Spark简单示例</h3> 
<h4><a id="_171"></a>资源准备</h4> 
<ul><li> <p>下载spark-3.2.4版本<br> &lt;<a href="https://spark.apache.org/downloads.html" rel="nofollow">Downloads | Apache Spark</a>&gt;</p> </li><li> <p>同样的方式将文件直接复制粘贴进/opt/software目录下</p> </li><li> <p>解压</p> </li></ul> 
<pre><code>cd /opt/software
tar -zxvf ./spark-3.2.4-bin-hadoop3.2.tgz -C ../module
</code></pre> 
<h4><a id="spark_182"></a>配置spark环境</h4> 
<ul><li>修改配置文件</li></ul> 
<pre><code>mv spark-env.sh.template spark-env.sh
vim spark-env.sh

</code></pre> 
<ul><li>添加一行<br> <code>export SPARK_DIST_CLASSPATH=$(/opt/module/hadoop-3.2.3/bin/hadoop classpath)</code></li></ul> 
<h4><a id="_193"></a>运行示例代码</h4> 
<pre><code>cd /opt/module/spark-3.2.4-bin-hadoop3.2
./bin/run-example SparkPi
</code></pre> 
<ul><li> <p>运行结果<br> <img src="https://images2.imgbox.com/4b/cb/TIBBrUc3_o.png" alt=""></p> </li><li> <p>观察到输出结果: <strong>Pi is roughly 3.1451557257786287</strong></p> </li></ul> 
<h3><a id="PySpark_202"></a>配置PySpark环境</h3> 
<h4><a id="Spark_On_Yarn_203"></a>Spark On Yarn</h4> 
<h4><a id="yarn_204"></a>yarn准备</h4> 
<ul><li>修改<strong>mapred-site.xml</strong>配置文件</li></ul> 
<pre><code>cd /opt/module/hadoop-3.2.3/etc/hadoop
vim mapred-site.xml
</code></pre> 
<p>添加以下内容</p> 
<pre><code>&lt;property&gt;
&lt;name&gt;mapreduce.framework.name&lt;/name&gt;
&lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;
</code></pre> 
<ul><li>修改 <strong>yarn-site.xml</strong>配置文件<br> <code>vim yarn-site.xml</code></li></ul> 
<pre><code>&lt;property&gt;
&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
&lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
</code></pre> 
<ul><li> <p>开启yarn<br> <code>./sbin/start-yarn.sh</code><br> 查看当前进程<br> <img src="https://images2.imgbox.com/7c/d1/fHg59K8N_o.png" alt="jps"></p> </li><li> <p>可以简单编写脚本方便hadoop启动与关闭</p> 
  <ul><li>简单参考<strong>my_hadoop.sh</strong></li></ul> </li></ul> 
<pre><code>#!/bin/bash

if [ $# -lt 1 ]
then
        echo "No Args Input..."
        exit ;
fi

case $1 in
"start")
        echo "==================== 启动 hadoop集群 ===================="

        echo "------------- 启动 hdfs -------------"
         "/opt/module/hadoop-3.2.3/sbin/start-dfs.sh"
        echo "------------- 启动 yarn -------------"
         "/opt/module/hadoop-3.2.3/sbin/start-yarn.sh"
        echo "GO SEE http://localhost:9870/explorer.html#/ EXPLORE YOUR HDFS!"
;;
"stop")
        echo "==================== 关闭 hadoop集群 ===================="
        echo "------------- 关闭 yarn -------------"
        "/opt/module/hadoop-3.2.3/sbin/stop-yarn.sh"
        echo "------------- 关闭 hdfs -------------"
         "/opt/module/hadoop-3.2.3/sbin/stop-dfs.sh"
;;
*)
                echo "Input Args Error..."
;;
esac
</code></pre> 
<pre><code>my_hadoop.sh start # 开启hdfs &amp; yarn
my_hadoop.sh stop  # 关闭yarn &amp; hdfs
</code></pre> 
<h4><a id="Miniconda3_269"></a>安装Miniconda3</h4> 
<ul><li>资源准备<br> &lt;<a href="https://docs.conda.io/projects/miniconda/en/latest/" rel="nofollow">Miniconda — miniconda documentation</a>&gt;<br> 选择Linux版本下载</li><li>同样方式复制到/opt/software目录下并运行</li></ul> 
<pre><code>cd /opt/software
bash ./Miniconda3-latest-Linux-x86_64.sh
</code></pre> 
<ul><li> <p>跟着指引安装即可, 安装目录可以选择 /opt/module/miniconda3<br> <img src="https://images2.imgbox.com/33/94/5zu4sfQn_o.png" alt=""></p> </li><li> <p>安装完成<br> <img src="https://images2.imgbox.com/db/ab/lyXsO8lx_o.png" alt="在这里插入图片描述"></p> </li><li> <p>重启shell可以看见前边多了一个(base)<br> <img src="https://images2.imgbox.com/bb/bb/TXHQmYXz_o.png" alt=""></p> </li><li> <p>创建conda虚拟环境<br> <code>conda create -n pyspark python=3.10</code></p> </li></ul> 
<h4><a id="spark_291"></a>配置spark文件</h4> 
<ul><li>在spark-env.sh文件中添加:</li></ul> 
<pre><code>HADOOP_CONF_DIR=/opt/module/hadoop-3.2.3
YARN_CONF_DIR=/opt/module/hadoop-3.2.3
</code></pre> 
<ul><li>在.bashrc中添加配置:</li></ul> 
<pre><code>export PYSPARK_PYTHON=/opt/module/miniconda3/envs/pyspark/bin/python
export PYSPARK_DRIVER_PYTHON=/opt/module/miniconda3/envs/pyspark/bin/python
</code></pre> 
<h3><a id="Vscode_302"></a>Vscode配置远程连接</h3> 
<h4><a id="python_303"></a>插件准备及python解释器选择</h4> 
<ul><li> <p>安装Remote Development插件</p> </li><li> <p>安装Python插件<br> <img src="https://images2.imgbox.com/07/74/gOh4ADXF_o.png" alt="在这里插入图片描述"></p> </li><li> <p>左侧找到远程资源管理器选择Ubuntu并在当前窗口连接<br> <img src="https://images2.imgbox.com/f0/20/ZRemMMVL_o.png" alt="在这里插入图片描述"></p> </li><li> <p>在家目录创建pyspark-project文件夹</p> </li></ul> 
<pre><code>cd ~
mkdir pyspark-project
</code></pre> 
<ul><li> <p>在vscode中选择此文件夹打开<br> <img src="https://images2.imgbox.com/c2/6c/IODLUDjt_o.png" alt="![[Pasted image 20231112194429.png]]"></p> </li><li> <p>安装python库pyspark<br> ctrl + ` 打开终端进行安装</p> </li></ul> 
<pre><code>conda activate pyspark
pip install pyspark==3.2.0  # pyspark版本不能太高,否则有兼容性问题
</code></pre> 
<ul><li>python选择pyspark虚拟环境为解释器<br> <img src="https://images2.imgbox.com/c9/74/yyRg55Gf_o.png" alt="在这里插入图片描述"></li></ul> 
<h4><a id="WordCount_328"></a>测试WordCount程序</h4> 
<ul><li>编写word_count.py程序</li></ul> 
<pre><code># coding:utf8

'''
word_count.py
单词计数
'''

from pyspark import SparkConf, SparkContext


if __name__ == '__main__':
    conf = SparkConf().setAppName("WordCount")
    sc = SparkContext(conf=conf)

    input_path = "input/test.txt"

    file_rdd = sc.textFile(input_path)

    words_rdd = file_rdd.flatMap(lambda line: line.split(" "))

    words_with_one_rdd = words_rdd.map(lambda x: (x, 1))

    result_rdd = words_with_one_rdd.reduceByKey(lambda a, b: a + b)

    result_rdd.coalesce(1).saveAsTextFile("output")
    
</code></pre> 
<ul><li>执行代码</li></ul> 
<pre><code>cd /opt/module/spark-3.2.4-bin-hadoop3.2
 ./bin/spark-submit --master yarn ~/pyspark-project/test/word_count.py
</code></pre> 
<ul><li> <p>运行结果<br> <img src="https://images2.imgbox.com/ab/fd/nd7w93aK_o.png" alt="![[Pasted image 20231112200340.png]]"></p> </li><li> <p><strong>输出文件</strong>:<br> <img src="https://images2.imgbox.com/48/ad/g2ok9TdW_o.png" alt=""><br> <em>至此pyspark环境已经搭建完成<br> 可以在vscode上编写程序并且提交到yarn了</em></p> </li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4090b1efe12663e1167d12fd3c554d87/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">hadoop win11安装hadoop环境 winutils.exe获取，windows安装大数据运行环境 winutils文件获取，winutils文件 hadoop(十四)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/158b60cea74100eff04f41506d5689af/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Windows系统中，如何将Python添加到系统环境（图文教程）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程学习者.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>