<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AI大模型探索之路-训练篇17：大语言模型预训练-微调技术之QLoRA - 编程学习者</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcxuexizhe.github.io/posts/b623730f12c9abeacd8b0bfcee93d390/">
  <meta property="og:site_name" content="编程学习者">
  <meta property="og:title" content="AI大模型探索之路-训练篇17：大语言模型预训练-微调技术之QLoRA">
  <meta property="og:description" content="系列篇章💥 AI大模型探索之路-训练篇1：大语言模型微调基础认知
AI大模型探索之路-训练篇2：大语言模型预训练基础认知
AI大模型探索之路-训练篇3：大语言模型全景解读
AI大模型探索之路-训练篇4：大语言模型训练数据集概览
AI大模型探索之路-训练篇5：大语言模型预训练数据准备-词元化
AI大模型探索之路-训练篇6：大语言模型预训练数据准备-预处理
AI大模型探索之路-训练篇7：大语言模型Transformer库之HuggingFace介绍
AI大模型探索之路-训练篇8：大语言模型Transformer库-预训练流程编码体验
AI大模型探索之路-训练篇9：大语言模型Transformer库-Pipeline组件实践
AI大模型探索之路-训练篇10：大语言模型Transformer库-Tokenizer组件实践
AI大模型探索之路-训练篇11：大语言模型Transformer库-Model组件实践
AI大模型探索之路-训练篇12：语言模型Transformer库-Datasets组件实践
AI大模型探索之路-训练篇13：大语言模型Transformer库-Evaluate组件实践
AI大模型探索之路-训练篇14：大语言模型Transformer库-Trainer组件实践
AI大模型探索之路-训练篇15：大语言模型预训练之全量参数微调
AI大模型探索之路-训练篇16：大语言模型预训练-微调技术之LoRA
目录 系列篇章💥前言一、QLoRA 总体概述二、QLoRA原理解释（4-bit NormalFloat）三、QLoRA代码实践学术资源加速步骤1 导入相关包步骤2 加载数据集步骤3 数据集预处理1）获取分词器2）定义数据处理函数3）对数据进行预处理 步骤4 创建模型1、PEFT 步骤1 配置文件2、PEFT 步骤2 创建模型 步骤5 配置训练参数步骤6 创建训练器步骤7 模型训练步骤8 模型推理 总结 前言 在深度学习的不断进步中，大型语言模型（LLMs）的预训练和微调技术成为了研究的热点。其中，量化技术以其在模型压缩和加速方面的潜力备受关注。本文将深入探讨QLoRA（Quantized Low-Rank Adaptation）技术的原理、实践及应用。
一、QLoRA 总体概述 QLoRA技术是一种创新的量化LoRA(Low-Rank Adaptation)的技术，旨在保持模型性能的同时，显著减少模型的内存占用。该技术的核心包括：
1）4bit NormalFloat（NF4）： 这是针对正态分布权重设计的一种信息理论上最优的数据类型。相较于传统的4-bit整数和4-bit浮点数，NF4为正态分布数据提供了更优异的实证性能。
2）双量化：QLoRA采用一种独特的双重量化机制，对初次量化后的常量进行二次量化，进一步压缩存储空间。
3）分页优化器：使用NVIDIA统一内存特性，该特性可以在在GPU偶尔OOM的情况下，进行CPU和GPU之间自动分页到分页的传输，以实现无错误的 GPU 处理。该功能的工作方式类似于 CPU 内存和磁盘之间的常规内存分页。使用此功能为优化器状态（Optimizer）分配分页内存， 然后在 GPU 内存不足时将其自动卸载到 CPU 内存，并在优化器更新步骤需要时将其加载回 GPU 内存。
二、QLoRA原理解释（4-bit NormalFloat） 前面篇章中我们有介绍，通常为了减少GPU的使用，我们会对模型进行量化处理，减少资源的使用；int8、int4量化是一种有效的模型压缩技术，它通过减少数值的精度来换取计算效率的提升，同时尽量保持模型的准确性。
1）常规int8量化和反量化过程：
2）常规int4量化和反量化过程：
3）QLoRA的NF4量化
是一种特殊的4位浮点数（Normal Float 4-bit）量化方法。它不仅定义了一种新的数据类型，还采用了基于分块的分位数量化策略，这种方法能够更有效地保持数值的相对关系，并且减少了由于量化引入的误差。QLoRA的NF4量化通过双重量化进一步减小了缓存占用，并且结合低秩适配器（LoRA）进行模型微调，可以在有限的计算资源下达到较高的性能水平。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-08T07:50:01+08:00">
    <meta property="article:modified_time" content="2024-05-08T07:50:01+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程学习者" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程学习者</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AI大模型探索之路-训练篇17：大语言模型预训练-微调技术之QLoRA</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_2"></a>系列篇章💥</h2> 
<p><a href="https://xundaomalu.blog.csdn.net/article/details/138107946" rel="nofollow">AI大模型探索之路-训练篇1：大语言模型微调基础认知</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138143923" rel="nofollow">AI大模型探索之路-训练篇2：大语言模型预训练基础认知</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138161057" rel="nofollow">AI大模型探索之路-训练篇3：大语言模型全景解读</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138205204" rel="nofollow">AI大模型探索之路-训练篇4：大语言模型训练数据集概览</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138225299" rel="nofollow">AI大模型探索之路-训练篇5：大语言模型预训练数据准备-词元化</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138267915" rel="nofollow">AI大模型探索之路-训练篇6：大语言模型预训练数据准备-预处理</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138294519" rel="nofollow">AI大模型探索之路-训练篇7：大语言模型Transformer库之HuggingFace介绍</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138348834" rel="nofollow">AI大模型探索之路-训练篇8：大语言模型Transformer库-预训练流程编码体验</a><br> <a href="https://blog.csdn.net/xiaobing259/article/details/138373677">AI大模型探索之路-训练篇9：大语言模型Transformer库-Pipeline组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138391592" rel="nofollow">AI大模型探索之路-训练篇10：大语言模型Transformer库-Tokenizer组件实践</a><br> <a href="https://blog.csdn.net/xiaobing259/article/details/138424867">AI大模型探索之路-训练篇11：大语言模型Transformer库-Model组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138426216" rel="nofollow">AI大模型探索之路-训练篇12：语言模型Transformer库-Datasets组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138448172" rel="nofollow">AI大模型探索之路-训练篇13：大语言模型Transformer库-Evaluate组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138448511" rel="nofollow">AI大模型探索之路-训练篇14：大语言模型Transformer库-Trainer组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138472105" rel="nofollow">AI大模型探索之路-训练篇15：大语言模型预训练之全量参数微调</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138518728" rel="nofollow">AI大模型探索之路-训练篇16：大语言模型预训练-微调技术之LoRA</a></p> 
<hr> 
<p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#_2" rel="nofollow">系列篇章💥</a></li><li><a href="#_24" rel="nofollow">前言</a></li><li><a href="#QLoRA__26" rel="nofollow">一、QLoRA 总体概述</a></li><li><a href="#QLoRA4bit_NormalFloat_33" rel="nofollow">二、QLoRA原理解释（4-bit NormalFloat）</a></li><li><a href="#QLoRA_48" rel="nofollow">三、QLoRA代码实践</a></li><li><ul><li><a href="#_49" rel="nofollow">学术资源加速</a></li><li><a href="#1__64" rel="nofollow">步骤1 导入相关包</a></li><li><a href="#2__72" rel="nofollow">步骤2 加载数据集</a></li><li><a href="#3__100" rel="nofollow">步骤3 数据集预处理</a></li><li><ul><li><a href="#1_102" rel="nofollow">1）获取分词器</a></li><li><a href="#2_115" rel="nofollow">2）定义数据处理函数</a></li><li><a href="#3_146" rel="nofollow">3）对数据进行预处理</a></li></ul> 
   </li><li><a href="#4__163" rel="nofollow">步骤4 创建模型</a></li><li><ul><li><a href="#1PEFT_1__497" rel="nofollow">1、PEFT 步骤1 配置文件</a></li><li><a href="#2PEFT_2__524" rel="nofollow">2、PEFT 步骤2 创建模型</a></li></ul> 
   </li><li><a href="#5__588" rel="nofollow">步骤5 配置训练参数</a></li><li><a href="#6__607" rel="nofollow">步骤6 创建训练器</a></li><li><a href="#7__620" rel="nofollow">步骤7 模型训练</a></li><li><a href="#8__627" rel="nofollow">步骤8 模型推理</a></li></ul> 
  </li><li><a href="#_654" rel="nofollow">总结</a></li></ul> 
</div> 
<p></p> 
<hr> 
<h2><a id="_24"></a>前言</h2> 
<p>在深度学习的不断进步中，大型语言模型（LLMs）的预训练和微调技术成为了研究的热点。其中，量化技术以其在模型压缩和加速方面的潜力备受关注。本文将深入探讨QLoRA（Quantized Low-Rank Adaptation）技术的原理、实践及应用。</p> 
<h2><a id="QLoRA__26"></a>一、QLoRA 总体概述</h2> 
<p>QLoRA技术是一种创新的量化LoRA(Low-Rank Adaptation)的技术，旨在保持模型性能的同时，显著减少模型的内存占用。该技术的核心包括：<br> <strong>1）4bit NormalFloat（NF4）</strong>： 这是针对正态分布权重设计的一种信息理论上最优的数据类型。相较于传统的4-bit整数和4-bit浮点数，NF4为正态分布数据提供了更优异的实证性能。<br> <strong>2）双量化</strong>：QLoRA采用一种独特的双重量化机制，对初次量化后的常量进行二次量化，进一步压缩存储空间。<br> <strong>3）分页优化器</strong>：使用NVIDIA统一内存特性，该特性可以在在GPU偶尔OOM的情况下，进行CPU和GPU之间自动分页到分页的传输，以实现无错误的 GPU 处理。该功能的工作方式类似于 CPU 内存和磁盘之间的常规内存分页。使用此功能为优化器状态（Optimizer）分配分页内存， 然后在 GPU 内存不足时将其自动卸载到 CPU 内存，并在优化器更新步骤需要时将其加载回 GPU 内存。<br> <img src="https://images2.imgbox.com/4b/ba/2QU0zBmy_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="QLoRA4bit_NormalFloat_33"></a>二、QLoRA原理解释（4-bit NormalFloat）</h2> 
<p>前面篇章中我们有介绍，通常为了减少GPU的使用，我们会对模型进行量化处理，减少资源的使用；int8、int4量化是一种有效的模型压缩技术，它通过减少数值的精度来换取计算效率的提升，同时尽量保持模型的准确性。<br> <img src="https://images2.imgbox.com/6b/69/UKYkSY4s_o.png" alt="在这里插入图片描述"></p> 
<p>1）常规int8量化和反量化过程：<br> <img src="https://images2.imgbox.com/f9/f8/6jQp0xjs_o.png" alt="在这里插入图片描述"></p> 
<p>2）常规int4量化和反量化过程：<br> <img src="https://images2.imgbox.com/d3/54/94lZoxJS_o.png" alt="在这里插入图片描述"></p> 
<p>3）QLoRA的NF4量化<br> 是一种特殊的4位浮点数（Normal Float 4-bit）量化方法。它不仅定义了一种新的数据类型，还采用了基于分块的分位数量化策略，这种方法能够更有效地保持数值的相对关系，并且减少了由于量化引入的误差。QLoRA的NF4量化通过双重量化进一步减小了缓存占用，并且结合低秩适配器（LoRA）进行模型微调，可以在有限的计算资源下达到较高的性能水平。<br> <img src="https://images2.imgbox.com/8c/ea/vvxqz9FA_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="QLoRA_48"></a>三、QLoRA代码实践</h2> 
<h3><a id="_49"></a>学术资源加速</h3> 
<p>方便从huggingface下载模型，这云平台<a href="https://www.autodl.com/" rel="nofollow">autodl</a>提供的，仅适用于autodl。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> subprocess
<span class="token keyword">import</span> os

result <span class="token operator">=</span> subprocess<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token string">'bash -c "source /etc/network_turbo &amp;&amp; env | grep proxy"'</span><span class="token punctuation">,</span> shell<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> capture_output<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> result<span class="token punctuation">.</span>stdout
<span class="token keyword">for</span> line <span class="token keyword">in</span> output<span class="token punctuation">.</span>splitlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token string">'='</span> <span class="token keyword">in</span> line<span class="token punctuation">:</span>
        var<span class="token punctuation">,</span> value <span class="token operator">=</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'='</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span>var<span class="token punctuation">]</span> <span class="token operator">=</span> value
</code></pre> 
<h3><a id="1__64"></a>步骤1 导入相关包</h3> 
<p>开始之前，我们需要导入适用于模型训练和推理的必要库，如transformers。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> datasets <span class="token keyword">import</span> Dataset
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForCausalLM<span class="token punctuation">,</span> DataCollatorForSeq2Seq<span class="token punctuation">,</span> TrainingArguments<span class="token punctuation">,</span> Trainer
</code></pre> 
<h3><a id="2__72"></a>步骤2 加载数据集</h3> 
<p>使用适当的数据加载器，例如datasets库，来加载预处理过的指令遵循性任务数据集。</p> 
<pre><code class="prism language-python">ds <span class="token operator">=</span> Dataset<span class="token punctuation">.</span>load_from_disk<span class="token punctuation">(</span><span class="token string">"/root/tuning/lesson01/data/alpaca_data_zh/"</span><span class="token punctuation">)</span>
ds
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">Dataset<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span>
    features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'output'</span><span class="token punctuation">,</span> <span class="token string">'input'</span><span class="token punctuation">,</span> <span class="token string">'instruction'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    num_rows<span class="token punctuation">:</span> <span class="token number">26858</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">ds<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span>
</code></pre> 
<p>输出</p> 
<pre><code class="prism language-python"><span class="token punctuation">{<!-- --></span><span class="token string">'output'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'以下是保持健康的三个提示：\n\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\n\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\n\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 <span class="token string">'input'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">''</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 <span class="token string">'instruction'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'保持健康的三个提示。'</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
</code></pre> 
<h3><a id="3__100"></a>步骤3 数据集预处理</h3> 
<p>利用预训练模型的分词器（Tokenizer）对原始文本进行编码，并生成相应的输入ID、注意力掩码和标签。</p> 
<h4><a id="1_102"></a>1）获取分词器</h4> 
<pre><code class="prism language-python">tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"Langboat/bloom-1b4-zh"</span><span class="token punctuation">)</span>
tokenizer
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">BloomTokenizerFast<span class="token punctuation">(</span>name_or_path<span class="token operator">=</span><span class="token string">'Langboat/bloom-1b4-zh'</span><span class="token punctuation">,</span> vocab_size<span class="token operator">=</span><span class="token number">46145</span><span class="token punctuation">,</span> model_max_length<span class="token operator">=</span><span class="token number">1000000000000000019884624838656</span><span class="token punctuation">,</span> is_fast<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> padding_side<span class="token operator">=</span><span class="token string">'left'</span><span class="token punctuation">,</span> truncation_side<span class="token operator">=</span><span class="token string">'right'</span><span class="token punctuation">,</span> special_tokens<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token string">'bos_token'</span><span class="token punctuation">:</span> <span class="token string">'&lt;s&gt;'</span><span class="token punctuation">,</span> <span class="token string">'eos_token'</span><span class="token punctuation">:</span> <span class="token string">'&lt;/s&gt;'</span><span class="token punctuation">,</span> <span class="token string">'unk_token'</span><span class="token punctuation">:</span> <span class="token string">'&lt;unk&gt;'</span><span class="token punctuation">,</span> <span class="token string">'pad_token'</span><span class="token punctuation">:</span> <span class="token string">'&lt;pad&gt;'</span><span class="token punctuation">}</span><span class="token punctuation">,</span> clean_up_tokenization_spaces<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="2_115"></a>2）定义数据处理函数</h4> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">process_func</span><span class="token punctuation">(</span>example<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 设置最大长度为256</span>
    MAX_LENGTH <span class="token operator">=</span> <span class="token number">256</span>
    <span class="token comment"># 初始化输入ID、注意力掩码和标签列表</span>
    input_ids<span class="token punctuation">,</span> attention_mask<span class="token punctuation">,</span> labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token comment"># 对指令和输入进行编码</span>
    instruction <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"Human: "</span> <span class="token operator">+</span> example<span class="token punctuation">[</span><span class="token string">"instruction"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> example<span class="token punctuation">[</span><span class="token string">"input"</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"\n\nAssistant: "</span><span class="token punctuation">)</span>
    <span class="token comment"># 对输出进行编码，并添加结束符</span>
    response <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>example<span class="token punctuation">[</span><span class="token string">"output"</span><span class="token punctuation">]</span> <span class="token operator">+</span> tokenizer<span class="token punctuation">.</span>eos_token<span class="token punctuation">)</span>
    <span class="token comment"># 将指令和响应的输入ID拼接起来</span>
    input_ids <span class="token operator">=</span> instruction<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span> <span class="token operator">+</span> response<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span>
    <span class="token comment"># 将指令和响应的注意力掩码拼接起来</span>
    attention_mask <span class="token operator">=</span> instruction<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span> <span class="token operator">+</span> response<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span>
    <span class="token comment"># 将指令的标签设置为-100，表示不计算损失；将响应的输入ID作为标签</span>
    labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">100</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>instruction<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> response<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span>
    <span class="token comment"># 如果输入ID的长度超过最大长度，截断输入ID、注意力掩码和标签</span>
    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span> <span class="token operator">&gt;</span> MAX_LENGTH<span class="token punctuation">:</span>
        input_ids <span class="token operator">=</span> input_ids<span class="token punctuation">[</span><span class="token punctuation">:</span>MAX_LENGTH<span class="token punctuation">]</span>
        attention_mask <span class="token operator">=</span> attention_mask<span class="token punctuation">[</span><span class="token punctuation">:</span>MAX_LENGTH<span class="token punctuation">]</span>
        labels <span class="token operator">=</span> labels<span class="token punctuation">[</span><span class="token punctuation">:</span>MAX_LENGTH<span class="token punctuation">]</span>
    <span class="token comment"># 返回处理后的数据</span>
    <span class="token keyword">return</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">"input_ids"</span><span class="token punctuation">:</span> input_ids<span class="token punctuation">,</span>
        <span class="token string">"attention_mask"</span><span class="token punctuation">:</span> attention_mask<span class="token punctuation">,</span>
        <span class="token string">"labels"</span><span class="token punctuation">:</span> labels
    <span class="token punctuation">}</span>
</code></pre> 
<h4><a id="3_146"></a>3）对数据进行预处理</h4> 
<pre><code class="prism language-python">tokenized_ds <span class="token operator">=</span> ds<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>process_func<span class="token punctuation">,</span> remove_columns<span class="token operator">=</span>ds<span class="token punctuation">.</span>column_names<span class="token punctuation">)</span>
tokenized_ds
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">Dataset<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span>
    features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">,</span> <span class="token string">'attention_mask'</span><span class="token punctuation">,</span> <span class="token string">'labels'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    num_rows<span class="token punctuation">:</span> <span class="token number">26858</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>

</code></pre> 
<h3><a id="4__163"></a>步骤4 创建模型</h3> 
<p>然后，我们实例化一个预训练模型，这个模型将作为微调的基础。对于大型模型，我们可能还需要进行一些特定的配置，以适应可用的计算资源。（<code>在实例化时，指定量化参数</code>）</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token comment">##修改</span>
<span class="token comment"># low_cpu_mem_usage=True: 这个参数设定为True意味着在模型加载时会尽可能地减少CPU内存的使用。</span>
<span class="token comment"># torch_dtype=torch.half: 这个参数设置了模型中张量的数据类型为半精度浮点数，这可以减少内存占用和计算时间，但可能会牺牲一些精度。</span>
<span class="token comment"># device_map="auto": 这个参数设置了模型应该在哪个设备上运行。“auto”意味着它将自动选择可用的设备，优先选择GPU，如果没有GPU则选择CPU。</span>
<span class="token comment"># load_in_4bit=True: 这个参数设置为True意味着在模型加载时将使用4位量化，这可以进一步减少内存占用。</span>
<span class="token comment"># bnb_4bit_compute_dtype=torch.half: 这个参数设置了在4位量化时的计算数据类型，这里设置为半精度浮点数。</span>
<span class="token comment"># bnb_4bit_quant_type="nf4": 这个参数设置了4位量化的类型，"nf4"是一种特定的量化策略。</span>
<span class="token comment"># bnb_4bit_use_double_quant=True: 这个参数设置为True意味着在4位量化时使用双重量化。</span>
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"Langboat/bloom-1b4-zh"</span><span class="token punctuation">,</span>
                                              torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>half<span class="token punctuation">,</span>
                                              low_cpu_mem_usage<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> 
                                              device_map<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">,</span> 
                                              load_in_4bit<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                              bnb_4bit_quant_type<span class="token operator">=</span><span class="token string">"nf4"</span><span class="token punctuation">,</span> 
                                              bnb_4bit_use_double_quant<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">model<span class="token punctuation">.</span>dtype
</code></pre> 
<p>torch.float16</p> 
<p>查看参数，查看模型有哪些层，可以用于添加LoRA旁路</p> 
<pre><code class="prism language-python"><span class="token keyword">for</span> name<span class="token punctuation">,</span> parameter <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span>parameter<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
</code></pre> 
<p>输出</p> 
<pre><code class="prism language-python">transformer<span class="token punctuation">.</span>word_embeddings<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>word_embeddings_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>word_embeddings_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>uint8
transformer<span class="token punctuation">.</span>h<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>ln_f<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>float16
transformer<span class="token punctuation">.</span>ln_f<span class="token punctuation">.</span>bias torch<span class="token punctuation">.</span>float16
</code></pre> 
<p><code>下面2个部分是LoRA相关的配置。</code></p> 
<h4><a id="1PEFT_1__497"></a>1、PEFT 步骤1 配置文件</h4> 
<p>在使用PEFT进行微调时，我们首先需要创建一个配置文件，该文件定义了微调过程中的各种设置，如学习率调度、优化器选择等。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> peft <span class="token keyword">import</span> LoraConfig<span class="token punctuation">,</span> TaskType<span class="token punctuation">,</span> get_peft_model
<span class="token comment">## ,target_modules=["query_key_value"],r=8</span>
config <span class="token operator">=</span> LoraConfig<span class="token punctuation">(</span>task_type<span class="token operator">=</span>TaskType<span class="token punctuation">.</span>CAUSAL_LM<span class="token punctuation">,</span>r<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> target_modules<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'query_key_value'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
config
</code></pre> 
<p>输出</p> 
<pre><code class="prism language-python">LoraConfig<span class="token punctuation">(</span>peft_type<span class="token operator">=</span><span class="token operator">&lt;</span>PeftType<span class="token punctuation">.</span>LORA<span class="token punctuation">:</span> <span class="token string">'LORA'</span><span class="token operator">&gt;</span><span class="token punctuation">,</span> auto_mapping<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> base_model_name_or_path<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> revision<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> task_type<span class="token operator">=</span><span class="token operator">&lt;</span>TaskType<span class="token punctuation">.</span>CAUSAL_LM<span class="token punctuation">:</span> <span class="token string">'CAUSAL_LM'</span><span class="token operator">&gt;</span><span class="token punctuation">,</span> inference_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> r<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> target_modules<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'query_key_value'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> lora_alpha<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> lora_dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> fan_in_fan_out<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">,</span> modules_to_save<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> init_lora_weights<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> layers_to_transform<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> layers_pattern<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p>启用梯度计算</p> 
<pre><code class="prism language-python"><span class="token comment"># 在深度神经网络 [deep neural network] 训练时，需要对每个参数或权重 [parameter/weight] 计算其对损失函数 </span>
<span class="token comment"># [loss function] 的梯度 [gradient]，从而进行反向传播 [back propagation] 和优化[optimization]。</span>
<span class="token comment"># 默认情况下不会计算输入数据 [input data] 的梯度，即使它们在计算中起到了关键的作用。但是，在某些应用场景中，</span>
<span class="token comment"># 例如图像生成 [image generation]、注意力机制 [attention mechanism] 等，需要计算输入数据的梯度。此时，</span>
<span class="token comment"># 可以通过启用计算输入梯度的功能，对输入数据进行求导并利用其梯度信息进行优化。</span>
<span class="token comment"># 作用: 启用该功能这对于在保持模型权重固定的同时微调适配器权重非常有用。</span>

model<span class="token punctuation">.</span>enable_input_require_grads<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="2PEFT_2__524"></a>2、PEFT 步骤2 创建模型</h4> 
<p>接下来，我们使用PEFT和预训练模型来创建一个微调模型。这个模型将包含原始的预训练模型以及由PEFT引入的低秩参数。</p> 
<pre><code class="prism language-python">model <span class="token operator">=</span> get_peft_model<span class="token punctuation">(</span>model<span class="token punctuation">,</span> config<span class="token punctuation">)</span>
model
</code></pre> 
<p>输出</p> 
<pre><code class="prism language-python">PeftModelForCausalLM<span class="token punctuation">(</span>
  <span class="token punctuation">(</span>base_model<span class="token punctuation">)</span><span class="token punctuation">:</span> LoraModel<span class="token punctuation">(</span>
    <span class="token punctuation">(</span>model<span class="token punctuation">)</span><span class="token punctuation">:</span> BloomForCausalLM<span class="token punctuation">(</span>
      <span class="token punctuation">(</span>transformer<span class="token punctuation">)</span><span class="token punctuation">:</span> BloomModel<span class="token punctuation">(</span>
        <span class="token punctuation">(</span>word_embeddings<span class="token punctuation">)</span><span class="token punctuation">:</span> Embedding<span class="token punctuation">(</span><span class="token number">46145</span><span class="token punctuation">,</span> <span class="token number">2048</span><span class="token punctuation">)</span>
        <span class="token punctuation">(</span>word_embeddings_layernorm<span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token punctuation">(</span>h<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleList<span class="token punctuation">(</span>
          <span class="token punctuation">(</span><span class="token number">0</span><span class="token operator">-</span><span class="token number">23</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token number">24</span> x BloomBlock<span class="token punctuation">(</span>
            <span class="token punctuation">(</span>input_layernorm<span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
            <span class="token punctuation">(</span>self_attention<span class="token punctuation">)</span><span class="token punctuation">:</span> BloomAttention<span class="token punctuation">(</span>
              <span class="token punctuation">(</span>query_key_value<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear4bit<span class="token punctuation">(</span>
                in_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">6144</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span>
                <span class="token punctuation">(</span>lora_dropout<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleDict<span class="token punctuation">(</span>
                  <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token punctuation">:</span> Identity<span class="token punctuation">(</span><span class="token punctuation">)</span>
                <span class="token punctuation">)</span>
                <span class="token punctuation">(</span>lora_A<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleDict<span class="token punctuation">(</span>
                  <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
                <span class="token punctuation">)</span>
                <span class="token punctuation">(</span>lora_B<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleDict<span class="token punctuation">(</span>
                  <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">6144</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
                <span class="token punctuation">)</span>
                <span class="token punctuation">(</span>lora_embedding_A<span class="token punctuation">)</span><span class="token punctuation">:</span> ParameterDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
                <span class="token punctuation">(</span>lora_embedding_B<span class="token punctuation">)</span><span class="token punctuation">:</span> ParameterDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
              <span class="token punctuation">)</span>
              <span class="token punctuation">(</span>dense<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear4bit<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
              <span class="token punctuation">(</span>attention_dropout<span class="token punctuation">)</span><span class="token punctuation">:</span> Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
            <span class="token punctuation">)</span>
            <span class="token punctuation">(</span>post_attention_layernorm<span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
            <span class="token punctuation">(</span>mlp<span class="token punctuation">)</span><span class="token punctuation">:</span> BloomMLP<span class="token punctuation">(</span>
              <span class="token punctuation">(</span>dense_h_to_4h<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear4bit<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">8192</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
              <span class="token punctuation">(</span>gelu_impl<span class="token punctuation">)</span><span class="token punctuation">:</span> BloomGelu<span class="token punctuation">(</span><span class="token punctuation">)</span>
              <span class="token punctuation">(</span>dense_4h_to_h<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear4bit<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">8192</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
            <span class="token punctuation">)</span>
          <span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        <span class="token punctuation">(</span>ln_f<span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
      <span class="token punctuation">)</span>
      <span class="token punctuation">(</span>lm_head<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">46145</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span>
  <span class="token punctuation">)</span>
<span class="token punctuation">)</span>
</code></pre> 
<p>查看配置</p> 
<pre><code class="prism language-python">config
</code></pre> 
<p>输出</p> 
<pre><code class="prism language-python">LoraConfig<span class="token punctuation">(</span>peft_type<span class="token operator">=</span><span class="token operator">&lt;</span>PeftType<span class="token punctuation">.</span>LORA<span class="token punctuation">:</span> <span class="token string">'LORA'</span><span class="token operator">&gt;</span><span class="token punctuation">,</span> auto_mapping<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> base_model_name_or_path<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> revision<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> task_type<span class="token operator">=</span><span class="token operator">&lt;</span>TaskType<span class="token punctuation">.</span>CAUSAL_LM<span class="token punctuation">:</span> <span class="token string">'CAUSAL_LM'</span><span class="token operator">&gt;</span><span class="token punctuation">,</span> inference_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> r<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> target_modules<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token string">'query_key_value'</span><span class="token punctuation">,</span> <span class="token string">'dense_4h_to_h'</span><span class="token punctuation">}</span><span class="token punctuation">,</span> lora_alpha<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> lora_dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> fan_in_fan_out<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">,</span> modules_to_save<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> init_lora_weights<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> layers_to_transform<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> layers_pattern<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> rank_pattern<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">,</span> alpha_pattern<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">,</span> megatron_config<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> megatron_core<span class="token operator">=</span><span class="token string">'megatron.core'</span><span class="token punctuation">,</span> loftq_config<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="5__588"></a>步骤5 配置训练参数</h3> 
<p>在这一步，我们定义训练参数，这些参数包括输出目录、学习率、权重衰减、梯度累积步数、训练周期数等。这些参数将被用来配置训练过程。</p> 
<blockquote> 
 <p><strong>指定分页优化器为"paged_adamw_32bit"，这是一种针对低秩模型的优化算法</strong></p> 
</blockquote> 
<pre><code class="prism language-python">args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
    output_dir<span class="token operator">=</span><span class="token string">"/root/autodl-tmp/tuningdata/qlora"</span><span class="token punctuation">,</span>  <span class="token comment"># 指定模型训练结果的输出目录</span>
    per_device_train_batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>  <span class="token comment"># 设置每个设备（如GPU）在训练过程中的批次大小为4</span>
    gradient_accumulation_steps<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>  <span class="token comment"># 指定梯度累积步数为8，即将多个批次的梯度累加后再进行一次参数更新</span>
    logging_steps<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span>  <span class="token comment"># 每20个步骤记录一次日志信息</span>
    num_train_epochs<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>  <span class="token comment"># 指定训练的总轮数为1</span>
    gradient_checkpointing<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>  <span class="token comment"># 启用梯度检查点技术，可以减少内存占用并加速训练过程</span>
    optim<span class="token operator">=</span><span class="token string">"paged_adamw_32bit"</span>  <span class="token comment"># 指定分页优化器为"paged_adamw_32bit"，这是一种针对低秩模型的优化算法</span>
<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="6__607"></a>步骤6 创建训练器</h3> 
<p>最后，我们创建一个训练器实例，它封装了训练循环。训练器将负责运行训练过程，并根据我们之前定义的参数进行优化。</p> 
<pre><code class="prism language-python">trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>
    model<span class="token operator">=</span>model<span class="token punctuation">,</span>
    args<span class="token operator">=</span>args<span class="token punctuation">,</span>
    train_dataset<span class="token operator">=</span>tokenized_ds<span class="token punctuation">,</span>
    data_collator<span class="token operator">=</span>DataCollatorForSeq2Seq<span class="token punctuation">(</span>tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="7__620"></a>步骤7 模型训练</h3> 
<p>通过调用训练器的<code>train()</code>方法，我们启动模型的训练过程。这将根据之前定义的参数执行模型的训练。</p> 
<pre><code class="prism language-python">trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="8__627"></a>步骤8 模型推理</h3> 
<p>训练完成后，我们可以使用训练好的模型进行推理。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> peft <span class="token keyword">import</span> PeftModel
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

<span class="token comment">#加载基础模型</span>
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"Langboat/bloom-1b4-zh"</span><span class="token punctuation">,</span> low_cpu_mem_usage<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"Langboat/bloom-1b4-zh"</span><span class="token punctuation">)</span>

<span class="token comment">#加载lora模型</span>
p_model <span class="token operator">=</span> PeftModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model<span class="token operator">=</span>model<span class="token punctuation">,</span> model_id<span class="token operator">=</span><span class="token string">"/root/autodl-tmp/tuningdata/qlora/checkpoint-500"</span><span class="token punctuation">)</span>

<span class="token comment">#模型推理</span>
pipe <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"text-generation"</span><span class="token punctuation">,</span> model<span class="token operator">=</span>p_model<span class="token punctuation">,</span> tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
ipt <span class="token operator">=</span> <span class="token string">"Human: {}\n{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token string">"如何写好一个简历？"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"\n\nAssistant: "</span>
pipe<span class="token punctuation">(</span>ipt<span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span> do_sample<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token punctuation">)</span>
</code></pre> 
<p>输出</p> 
<pre><code class="prism language-python"><span class="token punctuation">[</span><span class="token punctuation">{<!-- --></span><span class="token string">'generated_text'</span><span class="token punctuation">:</span> <span class="token string">'Human: 如何写好一个简历？\n\nAssistant: 好的，那么你应该考虑以下几点：\n\n1. 职位相关性\n\n有些职位可能会要求你具有相应的学历或工作经验，所以你需要在简历中附上这些信息，以确保你不会被误解为没有相关经验或学历。\n\n2. 个人信息部分\n\n在你的个人信息部分上，一定要附上你自述的职位，并提供你详细的职位描述。\n\n3. 背景与工作经历\n\n在这里你可以列出你过去的工作经历，包括工作项目、取得的奖励、你的发展方向、参加过的课程等。\n\n4. 优势项目\n\n除了上面提到的经历外，你还可以补充一些你擅长的项目，这样你就可以更容易让招聘人员了解到你的特质并做出判断。\n\n5. 能力证明部分\n\n这里你需要附上你过去工作中涉及到的关键工具和流程，以及通过这些工具和流程实现的实际结果。\n\n6. 本职工作领域\n\n你还应该附上你的主要工作领域，这样招聘人员就可以了解你的技能、经验和知识在哪些领域发挥着作用。\n\n7. 专长描述部分\n\n在这些方面，你可以描述一下你在这个专业领域拥有过哪些独特的技能，哪些领域你比其他人更有优势，以及有哪些是你自己擅长的。\n\n8. 职位描述部分\n\n在这里你可以附上你在当前工作领域取得的成就，描述下你在这项工作中能够为公司带来的价值，并证明你能够胜任这份工作。\n\n9. 未来的发展规划\n\n除了这个部分，你也可以补充一些未来的发展规划，这样招聘人员就可以了解你的目标和野心。\n\n10. 联系方式\n\n这里你可以附上你的联络方式，以便招聘人员能够及时与你联系，讨论相关事宜。\n\n11. 备注部分\n\n在简历的最后，你可以附上一个个人备注部分，你可以在这里说明如何能够更好的帮助面试者了解你，并阐述下你想找工作的原因。'</span><span class="token punctuation">}</span><span class="token punctuation">]</span>
</code></pre> 
<h2><a id="_654"></a>总结</h2> 
<p>QLoRA技术为大型语言模型的预训练与微调提供了一种高效、节省资源的方案。通过精心设计的量化策略和低秩适配器，QLoRA在保证模型性能的同时，显著降低了内存占用，为AI领域的研究者和工程师提供了宝贵的实践经验。</p> 
<p><img src="https://images2.imgbox.com/93/c5/I4b3wX1R_o.png" alt="在这里插入图片描述"></p> 
<p>🎯🔖更多专栏系列文章：<a href="https://blog.csdn.net/xiaobing259/category_12628007.html?spm=1001.2014.3001.5482"><strong>AIGC-AI大模型探索之路</strong></a></p> 
<blockquote> 
 <p>如果文章内容对您有所触动，别忘了<font color="red"><strong>点赞、⭐关注，收藏</strong></font>！加入我，让我们携手同行AI的探索之旅，一起开启智能时代的大门！</p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/216c1addf82cca0c1368dbaee5017739/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">2024 Flutter iOS 隐私清单上线，5 月 1 号最后期限，你收到 「ITMS-91053」 了吗？</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bf3cf3a625832d68545e8c02faa87cfd/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">基于大数据&#43;Hadoop的豆瓣电子图书推荐系统设计和实现</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程学习者.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>