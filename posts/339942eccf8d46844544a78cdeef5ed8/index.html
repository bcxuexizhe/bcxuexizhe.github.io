<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>CVPR 2024 | 图像检测类（目标、deepfake、异常）！AIGC扩散模型diffusion解决detection任务... - 编程学习者</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcxuexizhe.github.io/posts/339942eccf8d46844544a78cdeef5ed8/">
  <meta property="og:site_name" content="编程学习者">
  <meta property="og:title" content="CVPR 2024 | 图像检测类（目标、deepfake、异常）！AIGC扩散模型diffusion解决detection任务...">
  <meta property="og:description" content="目标跟踪 1、Delving into the Trajectory Long-tail Distribution for Muti-object Tracking 多目标跟踪（Multiple Object Tracking，MOT）是计算机视觉领域中一个关键领域，有广泛应用。当前研究主要集中在跟踪算法的开发和后处理技术的改进上。然而，对跟踪数据本身的特性缺乏深入的研究。
本研究首次对跟踪数据的分布模式进行探索，并发现现有 MOT 数据集中存在明显的长尾分布问题。发现不同行人分布存在显著不平衡现象，将其称为“行人轨迹长尾分布”。针对这一挑战，提出一种专门设计用于减轻这种分布影响的策略。具体而言，提出两种数据增强策略，包括静态摄像机视图数据增强（SVA）和动态摄像机视图数据增强（DVA），针对视点状态，以及面向 Re-ID 的 Group Softmax（GS）模块。SVA 是为了回溯并预测尾部类别的行人轨迹，而 DVA 则使用扩散模型改变场景的背景。GS 将行人划分为不相关的组，并对每个组进行 softmax 操作。
策略可以集成到许多现有的跟踪系统中，实验证实方法在降低长尾分布对多目标跟踪性能的影响方面的有效性。https://github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT
目标检测 2、SAFDNet: A Simple and Effective Network for Fully Sparse 3D Object Detection 基于 LiDAR 的三维物体检测，在自动驾驶中起关键作用。目前已有的高性能三维物体检测器通常在骨干网络和预测头中构建密集特征图。然而，随着感知范围增加，密集特征图带来的计算成本呈二次增长，使得这些模型很难扩展到长距离检测。最近一些研究尝试构建完全稀疏的检测器来解决这个问题，然而所得模型要么依赖于复杂的多阶段流水线，要么表现不佳。
本文提出 SAFDNet，简单高效，专为完全稀疏的三维物体检测而设计。在 SAFDNet 中，设计了一种自适应特征扩散策略来解决中心特征丢失的问题。在 Waymo Open、nuScenes 和 Argoverse2 数据集上进行大量实验证明，SAFDNet 在前两个数据集上的性能略优于先前的 SOTA，但在具有长距离检测特点的最后一个数据集上表现更好，验证 SAFDNet 在需要长距离检测的场景中的有效性。
在 Argoverse2 上，SAFDNet 在速度上比先前最好的混合检测器 HEDNet 快 2.1 倍，并且相对于先前最好的稀疏检测器 FSDv2 提高了 2.1% 的 mAP，速度提高了 1.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-08T11:18:34+08:00">
    <meta property="article:modified_time" content="2024-04-08T11:18:34+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程学习者" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程学习者</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">CVPR 2024 | 图像检测类（目标、deepfake、异常）！AIGC扩散模型diffusion解决detection任务...</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <h2>目标跟踪</h2> 
 <h2>1、Delving into the Trajectory Long-tail Distribution for Muti-object Tracking</h2> 
 <img src="https://images2.imgbox.com/d3/51/Zt0Vm27n_o.png" alt="6eeaa2b1f2c01ff8c7dc64726e66cba7.png"> 
 <p>多目标跟踪（Multiple Object Tracking，MOT）是计算机视觉领域中一个关键领域，有广泛应用。当前研究主要集中在跟踪算法的开发和后处理技术的改进上。然而，对跟踪数据本身的特性缺乏深入的研究。</p> 
 <p>本研究首次对跟踪数据的分布模式进行探索，并发现现有 MOT 数据集中存在明显的长尾分布问题。发现不同行人分布存在显著不平衡现象，将其称为“行人轨迹长尾分布”。针对这一挑战，提出一种专门设计用于减轻这种分布影响的策略。具体而言，提出两种数据增强策略，包括静态摄像机视图数据增强（SVA）和动态摄像机视图数据增强（DVA），针对视点状态，以及面向 Re-ID 的 Group Softmax（GS）模块。SVA 是为了回溯并预测尾部类别的行人轨迹，而 DVA 则使用扩散模型改变场景的背景。GS 将行人划分为不相关的组，并对每个组进行 softmax 操作。</p> 
 <p>策略可以集成到许多现有的跟踪系统中，实验证实方法在降低长尾分布对多目标跟踪性能的影响方面的有效性。https://github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT</p> 
 <h2>目标检测</h2> 
 <h2>2、SAFDNet: A Simple and Effective Network for Fully Sparse 3D Object Detection</h2> 
 <img src="https://images2.imgbox.com/50/f4/qQBTaaDt_o.png" alt="f3539a61d427beb0fe7b104bdb4a485a.png"> 
 <p>基于 LiDAR 的三维物体检测，在自动驾驶中起关键作用。目前已有的高性能三维物体检测器通常在骨干网络和预测头中构建密集特征图。然而，随着感知范围增加，密集特征图带来的计算成本呈二次增长，使得这些模型很难扩展到长距离检测。最近一些研究尝试构建完全稀疏的检测器来解决这个问题，然而所得模型要么依赖于复杂的多阶段流水线，要么表现不佳。</p> 
 <p>本文提出 SAFDNet，简单高效，专为完全稀疏的三维物体检测而设计。在 SAFDNet 中，设计了一种自适应特征扩散策略来解决中心特征丢失的问题。在 Waymo Open、nuScenes 和 Argoverse2 数据集上进行大量实验证明，SAFDNet 在前两个数据集上的性能略优于先前的 SOTA，但在具有长距离检测特点的最后一个数据集上表现更好，验证 SAFDNet 在需要长距离检测的场景中的有效性。</p> 
 <p>在 Argoverse2 上，SAFDNet 在速度上比先前最好的混合检测器 HEDNet 快 2.1 倍，并且相对于先前最好的稀疏检测器 FSDv2 提高了 2.1% 的 mAP，速度提高了 1.3 倍。https://github.com/zhanggang001/HEDNet</p> 
 <h2>3、DetDiffusion: Synergizing Generative and Perceptive Models for Enhanced Data Generation and Perception</h2> 
 <img src="https://images2.imgbox.com/3e/37/JMQH7xoT_o.png" alt="cd7786f475cea8c752819d34a713ee66.png"> 
 <p>当前的感知模型严重依赖于资源密集型数据集，因此需要创新性的解决方案。利用最近在扩散模型和合成数据方面的进展，通过构造各种标签图像输入，合成数据有助于下游任务。尽管之前的方法已经分别解决了生成和感知模型的问题，但是 DetDiffusion 是第一个在生成有效数据的感知模型方面进行了整合的方法。</p> 
 <p>为增强感知模型的图像生成能力，引入感知损失（P.A. loss）通过分割来改善质量和可控性。为提高特定感知模型的性能，方法通过提取和利用感知感知属性（P.A. Attr）来定制数据增强。来自目标检测任务的实验结果凸显了 DetDiffusion 在布局导向生成方面的出色性能，显著提高了下游检测性能。</p> 
 <h2>4、SDDGR: Stable Diffusion-based Deep Generative Replay for Class Incremental Object Detection</h2> 
 <img src="https://images2.imgbox.com/5e/1f/1d3KVXmk_o.png" alt="a2e9e8415f52ff4f93ada302f056efa0.png"> 
 <p>在类别增量学习（CIL）领域，generative replay已成为缓解灾难性遗忘的方法，随着生成模型的不断改进，越来越受到关注。然而，在类别增量物体检测（CIOD）中的应用受到很大限制，主要是由于涉及多个标签的场景的复杂性。</p> 
 <p>本文提出一种名为stable diffusion deep generative replay（SDDGR）的用于 CIOD 的新方法。方法利用基于扩散的生成模型与预训练的文本到扩散网络相结合，生成真实多样的合成图像。SDDGR采用迭代优化策略，生成高质量的旧类别样本。此外，采用L2知识蒸馏技术，以提高合成图像中先前知识的保留。此外，方法还包括对新任务图像中的旧对象进行伪标签，以防止将其错误分类为背景元素。</p> 
 <p>对COCO 2017数据集的大量实验表明，SDDGR在各种CIOD场景下明显优于现有算法，达到了新的技术水平。</p> 
 <h2>关键点检测</h2> 
 <h2>5、Pose-Guided Self-Training with Two-Stage Clustering for Unsupervised Landmark Discovery</h2> 
 <img src="https://images2.imgbox.com/f7/bc/S4Ax0WTx_o.png" alt="ee1ba85acf0c099ff3a75cec9a49ce8d.png"> 
 <p>无监督的Unsupervised landmarks discovery（ULD）是具有挑战性的计算机视觉问题。为利用扩散模型在ULD任务中的潜力，首先，提出一种基于随机像素位置的简单聚类的零样本ULD基线，通过最近邻匹配提供了比现有ULD方法更好的结果。其次，在零样本性能的基础上，通过自训练和聚类开发了一种基于扩散特征的ULD算法，以显著超越以前的方法。第三，引入一个基于生成潜在姿势代码的新代理任务，并提出了一个两阶段的聚类机制，以促进有效的伪标签生成，从而显著提高性能。</p> 
 <p>总的来说，方法在四个具有挑战性的基准测试（AFLW、MAFL、CatHeads 和 LS3D）上一贯优于现有的最先进方法。</p> 
 <h2>deepfake检测</h2> 
 <h2>6、Latent Reconstruction Error Based Method for Diffusion-Generated Image Detection</h2> 
 <img src="https://images2.imgbox.com/dc/80/w04oBcY1_o.png" alt="a1f8c2d75528364504a5b03da49ba6a4.png"> 
 <p>扩散模型极大提高了图像生成质量，使得真实图像和生成图像之间越来越难以区分。然而，这一发展也引发了重大的隐私和安全问题。针对这一问题，提出一种新的潜变量重构误差引导特征优化方法（Latent REconstruction error guided feature REfinement， LaRE2），用于检测生成图像。</p> 
 <p>提出潜变量重构误差（Latent Reconstruction Error，LaRE），一种基于重构误差的潜在空间特征，用于生成图像检测。LaRE 在特征提取效率方面超过了现有方法，同时保留了区分真实与伪造图像所需的关键线索。为了利用 LaRE，提出一个带有误差引导特征优化模块（EGRE）的方法，通过 LaRE 引导图像特征的优化，以增强特征的辨别力。</p> 
 <p>EGRE 采用对齐然后细化机制，可以从空间和通道角度有效地细化图像特征，以进行生成图像检测。在大规模 GenImage 基准测试上的大量实验证明LaRE2 的优越性，在 8 个不同的图像生成器中超过了最好的 SoTA 方法，平均 ACC/AP 高达 11.9%/12.1%。LaRE 在特征提取成本方面也超越了现有方法，速度提升8倍。</p> 
 <h2>异常检测</h2> 
 <h2>7、RealNet: A Feature Selection Network with Realistic Synthetic Anomaly for Anomaly Detection</h2> 
 <img src="https://images2.imgbox.com/d5/21/yQUuVKUp_o.png" alt="75a46157c86b197ebcc32faa384d0205.png"> 
 <p>自监督特征重建方法在工业图像异常检测和定位方面显示出有希望进展。这些方法在合成真实且多样化的异常样本以及解决预训练特征的特征冗余和预训练偏差方面仍然面临挑战。</p> 
 <p>这项工作提出 RealNet，一种具有现实合成异常和自适应特征选择的特征重建网络。它包含三个关键创新：首先，提出强度可控扩散异常合成（SDAS），一种基于扩散过程的合成策略，能够生成具有不同异常强度的样本，模仿真实异常样本的分布。其次，开发了异常感知特征选择（AFS），一种选择具有代表性和判别性的预训练特征子集的方法，以提高异常检测性能，同时控制计算成本。第三，引入了重建残差选择（RRS），一种自适应选择判别残差以跨多个粒度级别全面识别异常区域的策略。</p> 
 <p>在四个基准数据集上评估 RealNet，结果表明与当前最先进的方法相比，图像 AUROC 和像素 AUROC 都有改进。https://github.com/cnulab/RealNet</p> 
 <p><strong>更多：</strong></p> 
 <p><a href="" rel="nofollow"><strong><em>CVPR 2024 | 图像超分、图像恢复汇总！用AIGC扩散模型diffusion来解决图像low-level任务的思路</em></strong></a><br></p> 
 <p><a href="" rel="nofollow"><strong><em>CVPR 2024 | 风格迁移和人像生成汇总！扩散模型diffusion用于经典AIGC方向</em></strong></a><br></p> 
 <p><a href="" rel="nofollow"><strong><em>CVPR 2024 | 从6篇论文看扩散模型diffusion的改进方向</em></strong></a><br></p> 
 <p><a href="" rel="nofollow"><strong><em>CVPR 2024 | 前沿而相对小众！几个AIGC扩散模型diffusion应用一览</em></strong></a><br></p> 
 <p style="text-align:center;"><strong>关注公众号【机器学习与AI生成创作】，更多精彩等你来读</strong></p> 
 <p style="text-align:center;"><strong><strong><a href="" rel="nofollow">不是一杯奶茶喝不起，而是我T M直接用来跟进 AIGC+CV视觉 前沿技术，它不香？！</a></strong></strong></p> 
 <p style="text-align:center;"><strong><strong><a href="" rel="nofollow">ICCV 2023 | 最全AIGC梳理，5w字30个diffusion扩散模型方向，近百篇论文！</a></strong></strong><strong><strong><br></strong></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">卧剿，6万字！30个方向130篇！CVPR 2023 最全 AIGC 论文！一口气读完</a></strong><strong><br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">深入浅出stable diffusion：AI作画技术背后的潜在扩散模型论文解读</a></strong><strong><br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">深入浅出ControlNet，一种可控生成的AIGC绘画生成算法！</a> <br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">经典GAN不得不读：StyleGAN</a><br></strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/9b/bf/Q8bxaUr5_o.png" alt="cc29d39a6d46bf1fe48ee3142921ef94.png"> <strong><a href="" rel="nofollow">戳我，查看GAN的系列专辑~！</a></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">最新最全100篇汇总！生成扩散模型Diffusion Models</a></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">ECCV2022 | 生成对抗网络GAN部分论文汇总</a><br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">CVPR 2022 | 25+方向、最新50篇GAN论文</a><br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow"> ICCV 2021 | 35个主题GAN论文汇总</a><br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">超110篇！CVPR 2021最全GAN论文梳理</a></strong><strong><br></strong></p> 
 <p style="text-align:center;"><a href="" rel="nofollow"><strong>超100篇！CVPR 2020最全GAN论文梳理</strong></a></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">拆解组新的GAN：解耦表征MixNMatch</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">StarGAN第2版：多域多样性图像生成</a></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">附下载 | 《可解释的机器学习》中文版</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">附下载 |《TensorFlow 2.0 深度学习算法实战》</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">附下载 |《计算机视觉中的数学方法》分享</a></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">《基于深度学习的表面缺陷检测方法综述》</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">《零样本图像分类综述: 十年进展》</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">《基于深度神经网络的少样本学习综述》</a></p> 
 <blockquote> 
  <p>《礼记·学记》有云：独学而无友，则孤陋而寡闻</p> 
 </blockquote> 
 <p><strong>点击</strong><em><strong><a href="" rel="nofollow"><strong></strong></a><strong><a href="" rel="nofollow">跟进 AIGC+CV视觉 前沿技术，真香！</a></strong></strong></em><strong>，加入 </strong><strong>AI生成创作与计算机视觉</strong><strong> 知识星球！</strong></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f71c66dca14d18687a14a2d5dfcfb102/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">前端给后端传数据的几种方式</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1160ca1fe949ee97d43cef8cd3529c92/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python和pycharm从安装到激活全过程（保姆级教程）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程学习者.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>