<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>三分钟认知Softmax和Sigmoid的详细区别 - 编程学习者</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcxuexizhe.github.io/posts/f032f1ef380108bcd6fd8dc32682be29/">
  <meta property="og:site_name" content="编程学习者">
  <meta property="og:title" content="三分钟认知Softmax和Sigmoid的详细区别">
  <meta property="og:description" content="目录 前言1. Softmax2. Sigmoid3. 总结 前言 Softmax以及Sigmoid这两者都是神经网络中的激活函数，对应还有其他的激活函数
引入激活函数是为了将其输入非线性化，使得神经网络可以逼近任何非线性函数
（原本没有引入激活函数，就是多个矩阵进行相乘，无论神经网络多少层都是线性组合，这个概念是感知机）
Softmax以及Sigmoid两者都是作为神经网络的最后一层，通过激活函数之后转换为概率值
1. Softmax 作为二分类问题探讨，是二分类的拓展版，将其拓展为N分类，对应以概率的形式展示（概率最大的类别为此类别）
全连接层的输出使用Softmax，将其输出的结果表示为概率类别（所有概率加起来为1）。
Softmax将其泛化为多分类（SVM得出的是每个类别的分数），Softmax得出的是归一化类别概率（将其所有的输出结果都归一到0和1范围内）。
对应Softmax输入N个值，输出的结果为这N个值的概率（符合概率分布），预测出的所有值加起来为1，对应哪个值比较大，则判定为该类别
其公式具体如下：（使用ex，是为了将其预测结果转换为正数，保证概率不为负数）
通过其公式可看出其特性为：
零点不可微负输入梯度为0 例子如下：
A = 1，B = 2，C = 3
对应的概率值分别为：
P（A）= e1 / (e1 &#43; e2 &#43; e3)
P（B）= e2 / (e1 &#43; e2 &#43; e3)
P（C）= e3 / (e1 &#43; e2 &#43; e3)
对应代码模块如下：
import numpy as np scores = np.array([1, 2, 3]) softmax = np.exp(scores) / np.sum(np.exp(scores)) print(softmax) 截图如下：">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2022-10-13T20:26:49+08:00">
    <meta property="article:modified_time" content="2022-10-13T20:26:49+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程学习者" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程学习者</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">三分钟认知Softmax和Sigmoid的详细区别</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#_3" rel="nofollow">前言</a></li><li><a href="#1_Softmax_10" rel="nofollow">1. Softmax</a></li><li><a href="#2_Sigmoid_87" rel="nofollow">2. Sigmoid</a></li><li><a href="#3__111" rel="nofollow">3. 总结</a></li></ul> 
</div> 
<p></p> 
<h2><a id="_3"></a>前言</h2> 
<p>Softmax以及Sigmoid这两者都是神经网络中的激活函数，对应还有其他的激活函数</p> 
<p>引入激活函数是为了将其输入非线性化，使得神经网络可以逼近任何非线性函数<br> （原本没有引入激活函数，就是多个矩阵进行相乘，无论神经网络多少层都是线性组合，这个概念是感知机）</p> 
<p>Softmax以及Sigmoid两者都是作为神经网络的最后一层，通过激活函数之后转换为概率值</p> 
<h2><a id="1_Softmax_10"></a>1. Softmax</h2> 
<p>作为二分类问题探讨，是二分类的拓展版，将其拓展为N分类，对应以概率的形式展示（概率最大的类别为此类别）</p> 
<p>全连接层的输出使用Softmax，将其输出的结果表示为概率类别（所有概率加起来为1）。</p> 
<p>Softmax将其泛化为多分类（SVM得出的是每个类别的分数），Softmax得出的是归一化类别概率（将其所有的输出结果都归一到0和1范围内）。<br> 对应Softmax输入N个值，输出的结果为这N个值的概率（符合概率分布），预测出的所有值加起来为1，对应哪个值比较大，则判定为该类别</p> 
<p>其公式具体如下：（使用e<sup>x</sup>，是为了将其预测结果转换为正数，保证概率不为负数）<br> <img src="https://images2.imgbox.com/d6/44/4hTPM2I9_o.png" alt="在这里插入图片描述"></p> 
<p>通过其公式可看出其特性为：</p> 
<ul><li>零点不可微</li><li>负输入梯度为0</li></ul> 
<blockquote> 
 <p>例子如下：</p> 
</blockquote> 
<p>A = 1，B = 2，C = 3<br> 对应的概率值分别为：<br> P（A）= e<sup>1</sup> / (e<sup>1</sup> + e<sup>2</sup> + e<sup>3</sup>)<br> P（B）= e<sup>2</sup> / (e<sup>1</sup> + e<sup>2</sup> + e<sup>3</sup>)<br> P（C）= e<sup>3</sup> / (e<sup>1</sup> + e<sup>2</sup> + e<sup>3</sup>)</p> 
<p>对应代码模块如下：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
scores <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
softmax <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>scores<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>scores<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>softmax<span class="token punctuation">)</span> 
</code></pre> 
<p>截图如下：<br> <img src="https://images2.imgbox.com/05/ca/7dfYgaQB_o.png" alt="在这里插入图片描述"></p> 
<p>三者的概率值加起来为1，而且P（C）的概率值要远远大于P（A）以及 P（B）</p> 
<hr> 
<p>对此Softmax的特性：</p> 
<ul><li>归一化并且对应的所有概率值加起来为1</li><li>对应的真实类别概率值特别大，有放大（但是数值过大可能会有溢出的风险）</li><li>算出的概率值为非负数</li></ul> 
<p>一般在使用Softmax函数作为激活函数的时候，避免溢出，通常会做特殊的处理，将其e<sup>x</sup>都替换成e<sup>-x</sup>，防止数值过大产生溢出</p> 
<p>在TensorFlow中一般使用统一的接口：</p> 
<pre><code class="prism language-Python">tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits = False)
</code></pre> 
<p>通过<code>from_logits</code>参数设置，该参数为布尔变量</p> 
<ul><li>False，网络预测值y_pred经过Softmax输出值</li><li>True，网络预测值y_pred未经过Softmax输出值</li></ul> 
<p>测试代码如下：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf

<span class="token comment"># 3个样本，10个类别</span>
x <span class="token operator">=</span> tf<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># 3个样本标签值</span>
y <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">99</span><span class="token punctuation">,</span><span class="token number">999</span><span class="token punctuation">]</span><span class="token punctuation">)</span> 
<span class="token comment"># 采用one_hot编码模式</span>
y_true <span class="token operator">=</span> tf<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>y<span class="token punctuation">,</span> depth <span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">)</span> 

loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>losses<span class="token punctuation">.</span>categorical_crossentropy<span class="token punctuation">(</span>y_true<span class="token punctuation">,</span> x<span class="token punctuation">,</span> from_logits <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span> 
</code></pre> 
<p>截图如下：（如下使用的是False，表示经过激活函数。如果为True，输出的值也是一样的，只不过异常值的时候，False参数，Softmax会有所优化）<br> <img src="https://images2.imgbox.com/23/f7/4aehVpVT_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="2_Sigmoid_87"></a>2. Sigmoid</h2> 
<p>逻辑回归二分类将其输入映射到【0,1】的概率分布中，Sigmoid也有这样的功能</p> 
<p>数学公式如下：<br> <img src="https://images2.imgbox.com/6d/1b/dZ149PCL_o.png" alt="在这里插入图片描述"></p> 
<p>单调递增且其反函数也有递增的性质，此函数也经常被用作神经网络的阈值函数中</p> 
<p>用此函数预测类别，对应其值加起来并不为1，而Softmax函数加起来为1</p> 
<p>其图像如下：<br> <img src="https://images2.imgbox.com/4e/6a/g9aC8VVG_o.png" alt="在这里插入图片描述"></p> 
<p>用在神经网络中，其特点如下：</p> 
<ul><li>梯度平滑，避免梯度跳跃</li><li>连续函数，可导可微</li></ul> 
<p>但是缺点如下：</p> 
<ul><li>横向坐标轴正负无穷的时候，两侧导数为0，造成梯度消失</li><li>输出非0时，均值收敛速度慢（容易对梯度造成影响）</li><li>e的幂次运算比较复杂，训练时间比较长</li></ul> 
<h2><a id="3__111"></a>3. 总结</h2> 
<p>这两种激活函数如何选择，以及如何应用在不同场景，本身就是伯努利分布和二项分布的差别</p> 
<p>Softmax是为了判定该类别是什么（激活函数Softmax可使用的情况下，Sigmoid也可用）</p> 
<ul><li>N分类互斥，且只能选择其一，选择Softmax</li><li>N分类互斥，可选多个类别，选择Sigmoid</li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6f78deaed917c7437e82138cc4b80c99/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">图的遍历 —— 广度优先遍历</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a5b4989c2dc3cdd4ea7c468378605767/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">echarts入门教程（超级详细带案例）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程学习者.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>