<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>字节携港大南大升级 LLaVA-NeXT：借 LLaMA-3 和 Qwen-1.5 脱胎换骨，轻松追平 GPT-4V - 编程学习者</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcxuexizhe.github.io/posts/0fa6ff9fc5ac9b156499520a553b4375/">
  <meta property="og:site_name" content="编程学习者">
  <meta property="og:title" content="字节携港大南大升级 LLaVA-NeXT：借 LLaMA-3 和 Qwen-1.5 脱胎换骨，轻松追平 GPT-4V">
  <meta property="og:description" content="文 | 王启隆
出品 | 《新程序员》编辑部
2023 年，威斯康星大学麦迪逊分校、微软研究院和哥伦比亚大学的研究人员共同开发的 LLaVA 首次亮相，彼时它被视为一个端到端训练的大型多模态模型，展现了在视觉与语言融合领域的潜力。今年 1 月 30 日，LLaVA 的后续版本 LLaVA-NeXT 推出，它利用了当时最强的 LLM——Yi-34B，进一步增强了多模态理解、OCR（光学字符识别）和世界知识等方面的能力，甚至在一些基准测试上与 Gemini-Pro 和 GPT-V 相媲美。
在全世界默默等待 GPT-5 消息的这几个月里，开源社区出现了 LLaMA-3 和 Qwen-1.5 等语言能力更为强大的模型，阿里的 Qwen 更是在昨天发布了 2.5 版本，剑指 GPT-4。因此，LLaVA-NeXT 的研究团队开始思考一个问题：随着新型强力语言模型的诞生，开源 LLM 和私有 LLM 之间的性能差距正在缩小。当这些更强大的 LLM 被用于增强多模态模型时，是否也会促成开源多模态模型与私有多模态模型之间差距的缩小？
思来想去不如直接动手，LLaVA-NeXT 今日正式升级，研究团队直接用上了 LLaMA-3（8B）和 Qwen-1.5（72B &amp; 110B）为 LLaVA-NeXT 提升多模态能力，最大可达模型规模的 3 倍。这使得多模态模型能够展示从 LLM 继承的更好的视觉世界知识和逻辑推理能力。
代码链接：https://github.com/LLaVA-VL/LLaVA-NeXT
此外，新版本的 LLaVA-NeXT 针对更丰富的现实场景优化视觉对话功能，满足多样应用需求。为了检验在复杂环境下的多模态能力进步，作者们搜集并开发了新评估数据集 LLaVA-Bench（Wilder），它承袭了 LLaVA-Bench (in-the-wild) 的精神，深入探究日常生活中的视觉对话，并大幅增加了数据量以进行全面评估。
开源数据集链接：https://huggingface.co/datasets/liuhaotian/llava-bench-in-the-wild
为清晰体现“换了个 LLM”对多模态性能提升的贡献，本次升级沿用了 LLaVA-NeXT 的原训练方案，保持了该系列模型的简约设计和数据利用效率。最大的 1100 亿参数版本仅需在 128 台 H800 服务器上运行 18 小时即可完成训练。目前最新版 LLaVA-NeXT 的代码、数据和模型都将向公众开放。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-21T13:29:45+08:00">
    <meta property="article:modified_time" content="2024-05-21T13:29:45+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程学习者" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程学习者</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">字节携港大南大升级 LLaVA-NeXT：借 LLaMA-3 和 Qwen-1.5 脱胎换骨，轻松追平 GPT-4V</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align:center;"><img alt="ef03c19ac3764cf218e47f1d76ccdea3.gif" height="280" src="https://images2.imgbox.com/87/66/xaiXc5tm_o.jpg" width="1200"></p> 
 <p style="text-align:left;">文 | 王启隆</p> 
 <p style="text-align:left;">出品 | 《<a class="link-info" href="https://mall.csdn.net/item/98796" title="新程序员">新程序员</a>》编辑部</p> 
 <p>2023 年，威斯康星大学麦迪逊分校、微软研究院和哥伦比亚大学的研究人员共同开发的 <strong>LLaVA</strong> 首次亮相，彼时它被视为一个端到端训练的大型多模态模型，展现了在视觉与语言融合领域的潜力。今年 1 月 30 日，LLaVA 的后续版本 <strong>LLaVA-NeXT</strong> 推出，它利用了当时最强的 LLM——Yi-34B，进一步增强了多模态理解、OCR（光学字符识别）和世界知识等方面的能力，甚至在一些基准测试上与 Gemini-Pro 和 GPT-V 相媲美。</p> 
 <p style="text-align:center;"><img alt="7728628d0af63b6ecadb96f19c142c36.png" src="https://images2.imgbox.com/a1/6e/qZUYJur3_o.png"></p> 
 <p>在全世界默默等待 GPT-5 消息的这几个月里，开源社区出现了 LLaMA-3 和 Qwen-1.5 等语言能力更为强大的模型，阿里的 Qwen 更是在昨天发布了 2.5 版本，剑指 GPT-4。因此，LLaVA-NeXT 的研究团队开始思考一个问题：<strong>随着新型强力语言模型的诞生，开源 LLM 和私有 LLM 之间的性能差距正在缩小。当这些更强大的 LLM 被用于增强多模态模型时，是否也会促成开源多模态模型与私有多模态模型之间差距的缩小？</strong></p> 
 <p>思来想去不如直接动手，LLaVA-NeXT 今日正式升级，研究团队直接用上了 <strong>LLaMA-3（8B）</strong>和 <strong>Qwen-1.5（72B &amp; 110B）</strong>为 LLaVA-NeXT 提升多模态能力，最大可达模型规模的 <strong>3 倍</strong>。这使得多模态模型能够展示从 LLM 继承的更好的视觉世界知识和逻辑推理能力。</p> 
 <p style="text-align:center;"><img alt="7e4dae1085172316251d813347cb9001.png" src="https://images2.imgbox.com/c5/3e/rbseoBKQ_o.png"></p> 
 <p style="text-align:left;"><strong>代码链接：</strong>https://github.com/LLaVA-VL/LLaVA-NeXT</p> 
 <p>此外，新版本的 LLaVA-NeXT 针对更丰富的现实场景优化视觉对话功能，满足多样应用需求。为了检验在复杂环境下的多模态能力进步，作者们搜集并开发了新评估数据集 <strong>LLaVA-Bench（Wilder）</strong>，它承袭了 <strong>LLaVA-Bench (in-the-wild) </strong>的精神，深入探究日常生活中的视觉对话，并大幅增加了数据量以进行全面评估。</p> 
 <p style="text-align:center;"><img alt="bb42f428eba986faad782baa7313af19.png" src="https://images2.imgbox.com/22/6c/0DJ0k264_o.png"></p> 
 <p style="text-align:left;"><strong>开源数据集链接：</strong>https://huggingface.co/datasets/liuhaotian/llava-bench-in-the-wild</p> 
 <p>为清晰体现“换了个 LLM”对多模态性能提升的贡献，本次升级沿用了 LLaVA-NeXT 的原训练方案，保持了该系列模型的简约设计和数据利用效率。最大的 1100 亿参数版本仅需在 128 台 H800 服务器上运行 18 小时即可完成训练。目前最新版 LLaVA-NeXT 的代码、数据和模型都将向公众开放。</p> 
 <p style="text-align:center;"><img alt="a615d24d0d6df06c3a0c0f464697c07d.png" src="https://images2.imgbox.com/57/f4/OLbRzVg8_o.png"></p> 
 <p style="text-align:left;"><strong>Demo 链接：</strong>https://llava-next.lmms-lab.com/</p> 
 <p>团队成员如下，由来自南洋理工大学、香港科技大学以及字节跳动/抖音的研究人员组成。</p> 
 <p style="text-align:center;"><img alt="57c517b894a4bf029e305d83e9f2970c.png" src="https://images2.imgbox.com/c7/fe/1BMUjN9K_o.png"></p> 
 <p style="text-align:center;"><img alt="9d107a4ef3ac0bef2b0ba34ec1bce449.png" src="https://images2.imgbox.com/a0/b3/rTLVLsFW_o.png"></p> 
 <p style="text-align:center;"><strong>只换语言模型，竟然就能提升多模态能力？</strong></p> 
 <p><strong>基准测试结果</strong></p> 
 <p style="text-align:center;"><img alt="ad20a7b49cf2958807d386d1cd40d109.png" src="https://images2.imgbox.com/61/81/VZ9D8fos_o.png"></p> 
 <ul><li> <p><strong>SoTA 级别性能</strong>：通过简单增强 LLM 能力，LLaVA-NeXT 在各项基准测试中持续优于先前的开源多模态模型，赶上了 GPT4-V 的某些选定基准。</p> </li><li> <p><strong>低训练成本</strong>：新版本保持了与之前 LLaVA 模型一样高效的训练策略，在与之前 LLaVA-NeXT 7B/13B/34B 模型相同的训练数据上进行了监督微调。当前最大的模型 LLaVA-NeXT-110B 在 128 台 H800-80G 上训练了 18 小时。</p> </li></ul> 
 <p><strong>直接拿多模态模型去 PK SoTA 语言模型的结果</strong></p> 
 <p style="text-align:center;"><img alt="deb14dcd1931239002990c2cb2d59589.png" src="https://images2.imgbox.com/60/86/uNzq7LVY_o.png"></p> 
 <p><strong>探索大语言模型的能力极限</strong></p> 
 <p>通过 LLaVA-NeXT 的实践，研究团队见证了从 130 亿到 340 亿参数的LLM规模跃升带来的显著性能飞跃。随着更强大的开源 LLM 不断涌现，出现了新的问题：这些语言模型的能力要如何有效迁移到多模态场景？</p> 
 <p>为量化 LLM 的语言智能，研究团队参考了大规模多任务语言理解（MMLU）的评估分数。而为了检验应用相同 LLaVA-NeXT 训练方案后的多模态能力，他们参考了四组关键基准：MMMU（跨学科理解）、Mathvista（视觉数学推理）、AI2D（科学图表理解）及 LLaVA-W（日常视觉对话场景）。这些基准全面涵盖了 LMM 在实际应用中的多样化挑战。</p> 
 <p style="text-align:center;"><img alt="de650e1b1914a626d18766fa0c89c6aa.png" src="https://images2.imgbox.com/7b/34/IN231piK_o.png"></p> 
 <p>多模态与语言能力的相互促进关系在上图中以回归线形式直观展现，揭示了各基准测试的内在趋势。圆圈大小代表模型大小。</p> 
 <p style="text-align:center;"><img alt="9478c806feb3528933373e91d1f6fc24.png" src="https://images2.imgbox.com/34/c1/XUepYTjC_o.png"></p> 
 <p>如果换成表格，那就是上图这样。</p> 
 <p style="text-align:center;"><img alt="fa8998ac10cd2f12892dc0934063bc41.png" src="https://images2.imgbox.com/d8/20/iITIOK8H_o.png"></p> 
 <p style="text-align:center;"><strong>看图说话，吟诗作对</strong></p> 
 <p><strong>LLaVA-Bench（Wilder）：日常生活视觉对话测试集</strong></p> 
 <p>开发 LLMs 的最终目标之一是构建全能的通用助手，以帮助人类在日常生活中的各种多模态任务。因此，拥有强大的基准来精确测量相关进展至关重要。LLaVA-Bench（Wilder），也称为 LLaVA-W，正是这样一套用于评测多模态模型日常视觉对话能力的基准。</p> 
 <p>鉴于原版只有 60 个案例，研究团队意识到需要一个更为丰富的数据集。于是他们推出了 LLaVA-Bench（Wilder），分为「轻量级」120 例快速评估版和「进阶」1020 例综合测试版。这些数据集囊括了数学解题、图像解读、代码自动生成、视觉 AI 助手及基于图像的逻辑推理等多场景。数据采集自线上服务中的真实用户需求，经过严格筛选以保护隐私和减少潜在风险。<strong>所有问题的参考答案皆由 GPT4-V 生成</strong>。</p> 
 <p><img alt="3aaa5bcb0b1ca6544e3a3d3269425fa8.jpeg" src="https://images2.imgbox.com/b9/68/ll9KP4xu_o.jpg"></p> 
 <ul><li> <p><strong>与其它基准的对比</strong>：上图展示了 LLaVA-Bench（Wider） 与其他现有的多模态模型评估基准的可视化比较。多数现有基准倾向于使用固定格式的问答（QA），便于评估和模型对比。如 MMMU、Mathvista 和 AI2D 等，专为评估多模态模型在特定知识密集型领域的表现而设。</p> <p>RealWorldQA 虽聚焦日常，却限于简答形式。然而，作为助手模型，具备自由对话能力对于激发用户兴趣、突破简短问答的局限至关重要。因此，将自由形式对话融入日常生活视觉场景成为关键。LLaVA-W 开创性地提出了这一概念，而 LLaVA-Bench-Wilder 进一步扩展，引入更多生活场景和应用实例。</p> </li></ul> 
 <p style="text-align:center;"><img alt="bea87c1745932dd364829b53eb9e6eca.png" src="https://images2.imgbox.com/cd/5a/vJ2bwSul_o.png"></p> 
 <ul><li> <p><strong>数据集构建与评价标准</strong>：对于来自在线服务的海量查询，研究团队使用 ONE-PEACE 嵌入模型生成嵌入并应用了加权 K-Means 聚类，确保高像素值图像优先被纳入测试。去重后，就形成了前文提到的 120 道「轻量级」问题和 1020 道「进阶」问题。研究团队进行了严格的去噪审核，确保数据纯净，两版重合图像比例均低于 2%，而原始 LLaVA-W 为 5%。评估数据独立于 LLaVA-NeXT 训练数据，并完成了去重处理。</p> </li><li> <p><strong>参考答案的构建</strong>：对于每个筛选出的问题，他们首先使用 GPT-4V 生成参考响应，并邀请人工注释者手动验证问题和参考答案的准确性。面对含糊不清、涉及图片分辨率或无关图片内容的询问，GPT4-V 可能拒绝回应或给出错误答案。所以为了维护数据质量只能人工复核并修订了这些问题，确保信息的准确与可信。</p> </li><li> <p><strong>评分机制</strong>：采用了与 LLaVA-W 相同的评估流程，但用 GPT4-V 替换了 GPT-4。研究团队没有采用多分类评分，而是直接比较 GPT4-V 参考答案与模型回答的匹配度。实践中，他们发现这种评分方式未能充分暴露模型间差异，有时不公正地降低了参考答案得分，导致模型缺陷未能在总分中充分体现。为此，他们设定 GPT4-V <strong>对正确答案一律打满分</strong>，确保其他模型因错误而承受更高扣分，从而更精确地评估模型在实际情境中的表现力。</p> </li></ul> 
 <p><strong>再来 PK 一次！</strong></p> 
 <p style="text-align:center;"><img alt="01bcabc91fa1248dff87d42c513c8c7b.png" src="https://images2.imgbox.com/ab/f3/Y5P1GnV2_o.png"></p> 
 <ul><li> <p><strong>量化结果</strong>：与其他基准测试相比，LLaVA-Bench（Wilder）提供的独特测量结果非常明显，因为最先进的（SoTA）LMM 之间存在巨大的性能差距。正如 LLaVA-Bench (Wilder) 所评估的那样，某些在知识密集型任务中表现出色的 LMM 在日常生活的可视聊天场景中可能并不出色。最新版本中的 LLaVA-NeXT 模型在各个领域的性能都有所提高。</p> </li></ul> 
 <p style="text-align:center;"><img alt="f59ff588e84e544ca2ae16feeae8ad2b.png" src="https://images2.imgbox.com/81/e5/10LIu2EA_o.png"></p> 
 <p><strong>参数信息</strong></p> 
 <p>模型型号：<strong>LLaMA-3-LLaVA-NeXT-8B</strong>、<strong>LLaVA-NeXT-72B</strong> 和 <strong>LLaVA-NeXT-110B</strong>。</p> 
 <p style="text-align:center;"><img alt="d91f2a45e10d440ad705843ba1728398.png" src="https://images2.imgbox.com/df/df/poSYDxW3_o.png"></p> 
 <p><strong>模型架构</strong>：</p> 
 <ul><li> <p>视觉编码器部分的参数量为 303.5M。</p> </li><li> <p>连接器部分的参数量分别为 20.0M（LLaMA-3-LLaVA-NeXT-8B）、72.0M（LLaVA-NeXT-72B）和 72.0M（LLaVA-NeXT-110B）。</p> </li><li> <p>大规模语言模型（LLLM）部分的参数量分别为 8.03B（LLaMA-3-LLaVA-NeXT-8B）、738.3B（LLaVA-NeXT-72B）和 111.0B（LLaVA-NeXT-110B）。</p> </li></ul> 
 <p>分辨率列显示了图像输入的尺寸：336 x [(2), (1,2), (2,1), (1,3), (3,1), (1,4), (4,1)]。</p> 
 <p>训练数据部分展示了<strong>两个阶段的数据集大小</strong>：</p> 
 <ul><li> <p>第一阶段的训练数据为 558K 样本。</p> </li><li> <p>第二阶段的训练数据约为 790K 样本。</p> </li></ul> 
 <p>训练模块部分表明第一阶段只训练连接器，而第二阶段则训练整个模型。</p> 
 <p><strong>计算资源部分</strong>说明了每个模型所需的 GPU 数量和训练时间：</p> 
 <ul><li> <p>LLaMA-3-LLaVA-NeXT-8B 使用 <strong>8 个 A100-80G GPU</strong>，训练时间为 20 小时。</p> </li><li> <p>LLaVA-NeXT-72B 使用 <strong>64 个 A100-80G GPU</strong>，训练时间为 18 小时。</p> </li><li> <p>LLaVA-NeXT-110B 使用 <strong>128 个 H800-80G GPU</strong>，训练时间为 18 小时。</p> </li></ul> 
 <p>最后，总训练数据量为 1348K 样本。</p> 
 <p>参考资料：</p> 
 <p>https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/</p> 
 <p style="text-align:center;"><img alt="0ca5e897bfe65342eec11a50d1e97812.gif" src="https://images2.imgbox.com/10/f0/vAFphzGO_o.gif"></p> 
 <p style="text-align:center;"><img alt="23357f89071fbb153079b256de151470.jpeg" src="https://images2.imgbox.com/6b/a6/Bc8QHBQH_o.jpg"></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f3e01fd8c7aa837c20ef4ecef6a9f1b3/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【Rust】——使用线程同时运行代码</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d3a0cdb3f4e00a1ba1508cd35fd0263e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">《MySQL怎样运行的》-从一条记录说起-InnoDB记录存储结构</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程学习者.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>