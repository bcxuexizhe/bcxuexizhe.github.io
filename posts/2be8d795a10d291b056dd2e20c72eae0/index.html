<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AI大模型探索之路-训练篇22： ChatGLM3微调实战-从原理到应用的LoRA技术全解 - 编程学习者</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcxuexizhe.github.io/posts/2be8d795a10d291b056dd2e20c72eae0/">
  <meta property="og:site_name" content="编程学习者">
  <meta property="og:title" content="AI大模型探索之路-训练篇22： ChatGLM3微调实战-从原理到应用的LoRA技术全解">
  <meta property="og:description" content="系列篇章💥 AI大模型探索之路-训练篇1：大语言模型微调基础认知
AI大模型探索之路-训练篇2：大语言模型预训练基础认知
AI大模型探索之路-训练篇3：大语言模型全景解读
AI大模型探索之路-训练篇4：大语言模型训练数据集概览
AI大模型探索之路-训练篇5：大语言模型预训练数据准备-词元化
AI大模型探索之路-训练篇6：大语言模型预训练数据准备-预处理
AI大模型探索之路-训练篇7：大语言模型Transformer库之HuggingFace介绍
AI大模型探索之路-训练篇8：大语言模型Transformer库-预训练流程编码体验
AI大模型探索之路-训练篇9：大语言模型Transformer库-Pipeline组件实践
AI大模型探索之路-训练篇10：大语言模型Transformer库-Tokenizer组件实践
AI大模型探索之路-训练篇11：大语言模型Transformer库-Model组件实践
AI大模型探索之路-训练篇12：语言模型Transformer库-Datasets组件实践
AI大模型探索之路-训练篇13：大语言模型Transformer库-Evaluate组件实践
AI大模型探索之路-训练篇14：大语言模型Transformer库-Trainer组件实践
AI大模型探索之路-训练篇15：大语言模型预训练之全量参数微调
AI大模型探索之路-训练篇16：大语言模型预训练-微调技术之LoRA
AI大模型探索之路-训练篇17：大语言模型预训练-微调技术之QLoRA
AI大模型探索之路-训练篇18：大语言模型预训练-微调技术之Prompt Tuning
AI大模型探索之路-训练篇19：大语言模型预训练-微调技术之Prefix Tuning
AI大模型探索之路-训练篇20：大语言模型预训练-常见微调技术对比
AI大模型探索之路-训练篇21：Llama2微调实战-LoRA技术微调步骤详解
目录 系列篇章💥前言一、经典的Transformer架构二、ChatGLM3架构设计1、GLM 动机2、GLM的核心机制3、预训练任务类型1）掩码语言模型，自编码模型2）因果模型，自回归模型3）序列到序列模型 4、prompt格式 三、ChatGLM3微调准备1、数据准备2、模型选择 四、基于LoRA微调ChatGLM3步骤1 导入相关包步骤2 加载数据集步骤3 数据集预处理1）获取分词器2）定义数据处理函数3）对数据进行预处理4）解码检查input_ids的格式5）检查labels数据格式 步骤4 创建模型1、创建模型实例1）创建模型2）精度查看确认3）查看模型参数 2、PEFT 步骤1 配置文件3、PEFT 步骤2 创建模型1）创建微调模型2）查看LoRA层添加情况3）查看模型中可训练参数的数量 步骤5 配置训练参数步骤6 创建训练器步骤7 模型训练步骤8 模型推理 总结 前言 在自然语言处理的浪潮中，Transformer架构以其独特的设计和卓越性能，成为了大语言模型的基石。ChatGLM3，作为其中的一员，通过微调在特定任务上展现了其强大的适应性和灵活性。本文将深入探讨ChatGLM3的架构设计，微调策略，并提供实战案例，以期为开发者提供宝贵的参考。
一、经典的Transformer架构 Transformer架构自问世以来，已成为NLP领域的一个里程碑。其核心思想是利用自注意力机制（Self-Attention）来捕捉文本中的长距离依赖关系，无需像循环神经网络（RNN）那样逐步处理序列。各大语言模型虽基于Transformer演变；但在结构、编码方式、激活函数、layer Norm方法上各有不同；另外掩码的设计不同，训练数据和目标的多样性等，都赋予了模型不同的特性和应用场景。
二、ChatGLM3架构设计 1、GLM 动机 大型预训练语言模型的发展可归纳为三个主要方向：
1）自编码模型 (Auto Encoding)： BERT，ROBERTa，DeBERTa，ALBERT等；采用双向注意力机制，擅长处理自然语言理解任务，如情感分类、抽取式问答和自然语言推理等。
2）自回归模型 (Auto Regressive) ：GPT系列，Llama，百川，等；通过单向注意力机制，专注于生成任务，包括语言建模和文本生成。
3）编码器-解码器模型 (Encoder-Decoder) : T5，BART，MASS，PALM等；结合了双向和单向注意力（encoder的attention是双向的，decoder的attention是单向的）。适用于条件生成(seq2seq)任务，比如：文本摘要，机器翻译等。
然而，现有模型在多任务性能上存在局限。GLM（General Language Model）旨在融合三者优势，实现在自然语言理解、生成和条件生成任务上的全面优化">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-13T11:17:13+08:00">
    <meta property="article:modified_time" content="2024-05-13T11:17:13+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程学习者" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程学习者</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AI大模型探索之路-训练篇22： ChatGLM3微调实战-从原理到应用的LoRA技术全解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_2"></a>系列篇章💥</h2> 
<p><a href="https://xundaomalu.blog.csdn.net/article/details/138107946" rel="nofollow">AI大模型探索之路-训练篇1：大语言模型微调基础认知</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138143923" rel="nofollow">AI大模型探索之路-训练篇2：大语言模型预训练基础认知</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138161057" rel="nofollow">AI大模型探索之路-训练篇3：大语言模型全景解读</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138205204" rel="nofollow">AI大模型探索之路-训练篇4：大语言模型训练数据集概览</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138225299" rel="nofollow">AI大模型探索之路-训练篇5：大语言模型预训练数据准备-词元化</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138267915" rel="nofollow">AI大模型探索之路-训练篇6：大语言模型预训练数据准备-预处理</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138294519" rel="nofollow">AI大模型探索之路-训练篇7：大语言模型Transformer库之HuggingFace介绍</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138348834" rel="nofollow">AI大模型探索之路-训练篇8：大语言模型Transformer库-预训练流程编码体验</a><br> <a href="https://blog.csdn.net/xiaobing259/article/details/138373677">AI大模型探索之路-训练篇9：大语言模型Transformer库-Pipeline组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138391592" rel="nofollow">AI大模型探索之路-训练篇10：大语言模型Transformer库-Tokenizer组件实践</a><br> <a href="https://blog.csdn.net/xiaobing259/article/details/138424867">AI大模型探索之路-训练篇11：大语言模型Transformer库-Model组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138426216" rel="nofollow">AI大模型探索之路-训练篇12：语言模型Transformer库-Datasets组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138448172" rel="nofollow">AI大模型探索之路-训练篇13：大语言模型Transformer库-Evaluate组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138448511" rel="nofollow">AI大模型探索之路-训练篇14：大语言模型Transformer库-Trainer组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138472105" rel="nofollow">AI大模型探索之路-训练篇15：大语言模型预训练之全量参数微调</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138518728" rel="nofollow">AI大模型探索之路-训练篇16：大语言模型预训练-微调技术之LoRA</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138555530" rel="nofollow">AI大模型探索之路-训练篇17：大语言模型预训练-微调技术之QLoRA</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138595171" rel="nofollow">AI大模型探索之路-训练篇18：大语言模型预训练-微调技术之Prompt Tuning</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138631718" rel="nofollow">AI大模型探索之路-训练篇19：大语言模型预训练-微调技术之Prefix Tuning</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138604711" rel="nofollow">AI大模型探索之路-训练篇20：大语言模型预训练-常见微调技术对比</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138763708" rel="nofollow">AI大模型探索之路-训练篇21：Llama2微调实战-LoRA技术微调步骤详解</a></p> 
<hr> 
<p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#_2" rel="nofollow">系列篇章💥</a></li><li><a href="#_30" rel="nofollow">前言</a></li><li><a href="#Transformer_32" rel="nofollow">一、经典的Transformer架构</a></li><li><a href="#ChatGLM3_38" rel="nofollow">二、ChatGLM3架构设计</a></li><li><ul><li><a href="#1GLM__39" rel="nofollow">1、GLM 动机</a></li><li><a href="#2GLM_47" rel="nofollow">2、GLM的核心机制</a></li><li><a href="#3_54" rel="nofollow">3、预训练任务类型</a></li><li><ul><li><a href="#1_55" rel="nofollow">1）掩码语言模型，自编码模型</a></li><li><a href="#2_59" rel="nofollow">2）因果模型，自回归模型</a></li><li><a href="#3_64" rel="nofollow">3）序列到序列模型</a></li></ul> 
   </li><li><a href="#4prompt_72" rel="nofollow">4、prompt格式</a></li></ul> 
  </li><li><a href="#ChatGLM3_82" rel="nofollow">三、ChatGLM3微调准备</a></li><li><ul><li><a href="#1_83" rel="nofollow">1、数据准备</a></li><li><a href="#2_86" rel="nofollow">2、模型选择</a></li></ul> 
  </li><li><a href="#LoRAChatGLM3_92" rel="nofollow">四、基于LoRA微调ChatGLM3</a></li><li><ul><li><a href="#1__93" rel="nofollow">步骤1 导入相关包</a></li><li><a href="#2__101" rel="nofollow">步骤2 加载数据集</a></li><li><a href="#3__129" rel="nofollow">步骤3 数据集预处理</a></li><li><ul><li><a href="#1_145" rel="nofollow">1）获取分词器</a></li><li><a href="#2_159" rel="nofollow">2）定义数据处理函数</a></li><li><a href="#3_183" rel="nofollow">3）对数据进行预处理</a></li><li><a href="#4input_ids_200" rel="nofollow">4）解码检查input_ids的格式</a></li><li><a href="#5labels_212" rel="nofollow">5）检查labels数据格式</a></li></ul> 
   </li><li><a href="#4__224" rel="nofollow">步骤4 创建模型</a></li><li><ul><li><a href="#1_226" rel="nofollow">1、创建模型实例</a></li><li><ul><li><a href="#1_227" rel="nofollow">1）创建模型</a></li><li><a href="#2_236" rel="nofollow">2）精度查看确认</a></li><li><a href="#3_248" rel="nofollow">3）查看模型参数</a></li></ul> 
    </li><li><a href="#2PEFT_1__462" rel="nofollow">2、PEFT 步骤1 配置文件</a></li><li><a href="#3PEFT_2__479" rel="nofollow">3、PEFT 步骤2 创建模型</a></li><li><ul><li><a href="#1_481" rel="nofollow">1）创建微调模型</a></li><li><a href="#2LoRA_494" rel="nofollow">2）查看LoRA层添加情况</a></li><li><a href="#3_503" rel="nofollow">3）查看模型中可训练参数的数量</a></li></ul> 
   </li></ul> 
   </li><li><a href="#5__513" rel="nofollow">步骤5 配置训练参数</a></li><li><a href="#6__530" rel="nofollow">步骤6 创建训练器</a></li><li><a href="#7__543" rel="nofollow">步骤7 模型训练</a></li><li><a href="#8__550" rel="nofollow">步骤8 模型推理</a></li></ul> 
  </li><li><a href="#_577" rel="nofollow">总结</a></li></ul> 
</div> 
<p></p> 
<hr> 
<h2><a id="_30"></a>前言</h2> 
<p>在自然语言处理的浪潮中，Transformer架构以其独特的设计和卓越性能，成为了大语言模型的基石。ChatGLM3，作为其中的一员，通过微调在特定任务上展现了其强大的适应性和灵活性。本文将深入探讨ChatGLM3的架构设计，微调策略，并提供实战案例，以期为开发者提供宝贵的参考。</p> 
<h2><a id="Transformer_32"></a>一、经典的Transformer架构</h2> 
<p><img src="https://images2.imgbox.com/98/f5/U4sGOHJX_o.png" alt="在这里插入图片描述"></p> 
<p>Transformer架构自问世以来，已成为NLP领域的一个里程碑。其核心思想是利用自注意力机制（Self-Attention）来捕捉文本中的长距离依赖关系，无需像循环神经网络（RNN）那样逐步处理序列。各大语言模型虽基于Transformer演变；<strong>但在结构、编码方式、激活函数、layer Norm方法上各有不同；另外掩码的设计不同，训练数据和目标的多样性等</strong>，都赋予了模型不同的特性和应用场景。<br> <img src="https://images2.imgbox.com/7c/65/GYJBXrYO_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="ChatGLM3_38"></a>二、ChatGLM3架构设计</h2> 
<h3><a id="1GLM__39"></a>1、GLM 动机</h3> 
<p>大型预训练语言模型的发展可归纳为三个主要方向：<br> 1）自编码模型 (Auto Encoding)： BERT，ROBERTa，DeBERTa，ALBERT等；<strong>采用双向注意力机制</strong>，擅长处理自然语言理解任务，如情感分类、抽取式问答和自然语言推理等。<br> 2）自回归模型 (Auto Regressive) ：GPT系列，Llama，百川，等；<strong>通过单向注意力机制</strong>，专注于生成任务，包括语言建模和文本生成。<br> 3）编码器-解码器模型 (Encoder-Decoder) : T5，BART，MASS，PALM等；<strong>结合了双向和单向注意力（encoder的attention是双向的，decoder的attention是单向的）</strong>。适用于条件生成(seq2seq)任务，比如：文本摘要，机器翻译等。</p> 
<blockquote> 
 <p>然而，现有模型在多任务性能上存在局限。GLM（General Language Model）旨在融合三者优势，实现在自然语言理解、生成和条件生成任务上的全面优化</p> 
</blockquote> 
<h3><a id="2GLM_47"></a>2、GLM的核心机制</h3> 
<p><img src="https://images2.imgbox.com/0c/fa/lgVTrEka_o.png" alt="在这里插入图片描述"></p> 
<p>GLM的设计核心在于几个创新机制的引入：</p> 
<ul><li><strong>自回归填空</strong> ：通过预测遮蔽（Masked）的词来训练模型，类似于BERT的Masked Language Model任务，增强模型对语言的理解和生成能力。</li><li><strong>2D位置编码</strong>：采用二维位置编码，更精细地捕捉词与词之间的相对位置关系，提升模型对序列顺序的敏感度。</li><li><strong>填空序列乱序</strong>：在训练过程中对填空任务的序列进行乱序处理，迫使模型学习更深层次的语言结构和转换规则。</li></ul> 
<h3><a id="3_54"></a>3、预训练任务类型</h3> 
<h4><a id="1_55"></a>1）掩码语言模型，自编码模型</h4> 
<p>将一些位置的token替换成特殊[MASK]字符，预测被替换的字符<br> <img src="https://images2.imgbox.com/31/f6/RcIKwhRJ_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="2_59"></a>2）因果模型，自回归模型</h4> 
<p>将完整序列输入，基于上文的token预测下文的token<br> <img src="https://images2.imgbox.com/6c/5d/I6lbB1yC_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="3_64"></a>3）序列到序列模型</h4> 
<p>采用编码器解码器的方式，预测放在解码器部分<br> <img src="https://images2.imgbox.com/95/d5/oywV3KCO_o.png" alt="在这里插入图片描述"></p> 
<p>在hugginface的PEFT库中，GLM归类为因果模型<br> <img src="https://images2.imgbox.com/33/2a/MM9gJXB3_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="4prompt_72"></a>4、prompt格式</h3> 
<p>ChatGLM3的prompt格式如下：<br> [gMASK]sop&lt;|user|&gt; \n Prompt&lt;|assistant|&gt;\n response eos_token<br> 使用参考：<br> tokenizer.build_chat_input(“prompt”, history=[], role=“user”)<br> tokenizer.decode([xxx])<br> <img src="https://images2.imgbox.com/ab/1d/dW7qTSFr_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/f4/1e/FOLiS6xI_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="ChatGLM3_82"></a>三、ChatGLM3微调准备</h2> 
<h3><a id="1_83"></a>1、数据准备</h3> 
<p>数据集：https://huggingface.co/datasets/c-s-ale/alpaca-gpt4-data-zh<br> <img src="https://images2.imgbox.com/cd/d2/0lGblC9a_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="2_86"></a>2、模型选择</h3> 
<p>本次任务选择GLM3的基础模型。<br> 模型地址：https://www.modelscope.cn/models/ZhipuAI/chatglm3-6b-base/files</p> 
<p><img src="https://images2.imgbox.com/cc/82/sc8XWbsC_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="LoRAChatGLM3_92"></a>四、基于LoRA微调ChatGLM3</h2> 
<h3><a id="1__93"></a>步骤1 导入相关包</h3> 
<p>开始之前，我们需要导入适用于模型训练和推理的必要库，如transformers。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> datasets <span class="token keyword">import</span> Dataset
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForCausalLM<span class="token punctuation">,</span> DataCollatorForSeq2Seq<span class="token punctuation">,</span> TrainingArguments<span class="token punctuation">,</span> Trainer
</code></pre> 
<h3><a id="2__101"></a>步骤2 加载数据集</h3> 
<p>使用适当的数据加载器，例如datasets库，来加载预处理过的指令遵循性任务数据集。</p> 
<pre><code class="prism language-python">ds <span class="token operator">=</span> Dataset<span class="token punctuation">.</span>load_from_disk<span class="token punctuation">(</span><span class="token string">"/root/PEFT代码/tuning/lesson01/data/alpaca_data_zh"</span><span class="token punctuation">)</span>
ds
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">Dataset<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span>
    features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'output'</span><span class="token punctuation">,</span> <span class="token string">'input'</span><span class="token punctuation">,</span> <span class="token string">'instruction'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    num_rows<span class="token punctuation">:</span> <span class="token number">26858</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> 
<p>查看数据</p> 
<pre><code class="prism language-python">ds<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python"><span class="token punctuation">{<!-- --></span><span class="token string">'output'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'以下是保持健康的三个提示：\n\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\n\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\n\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 <span class="token string">'input'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">''</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 <span class="token string">'instruction'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'保持健康的三个提示。'</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
</code></pre> 
<h3><a id="3__129"></a>步骤3 数据集预处理</h3> 
<p>利用预训练模型的分词器（Tokenizer）对原始文本进行编码，并生成相应的输入ID、注意力掩码和标签。</p> 
<p><strong>自回归编码指令微调数据处理过程回顾：</strong><br> ①　input输入构建：</p> 
<ul><li>在此步骤中，我们将数据集中的三个主要组成部分（指令、用户输入和预期输出）连接在一起，形成一个单一的字符串。</li><li>这个字符串按照以下格式组织：首先是指令（instruction），然后是用户输入（input），最后是预期输出（output）。</li><li>这种组织方式有助于模型理解输入和输出之间的关系，并学习如何根据指令和用户输入生成正确的输出。</li></ul> 
<p>②　label标签创建：</p> 
<ul><li>此步骤涉及构建用于训练模型的标签（labels）。</li><li>用户输入部分在标签中保持不变，这意味着模型将尝试学习预测与用户输入相对应的输出。</li><li>对于输出部分，我们将其转化为目标标签，以便模型可以学习生成与预期输出相匹配的文本。</li><li>在自回归模型中，除了输出部分外，其他部分（包括指令和输入）的标签被替换为特殊的分隔符（例如：[SEP]）加上-100。这样做的目的是告诉模型不需要预测这些部分，而是将注意力集中在输出部分上。</li><li>通过这种方式，模型将学会根据给定的指令和用户输入来生成正确的输出，同时忽略其他不相关的信息。</li></ul> 
<h4><a id="1_145"></a>1）获取分词器</h4> 
<pre><code class="prism language-python"><span class="token comment">#加载本地模型，提前下载到本地</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"/root/autodl-tmp/chatglm3-6b-base"</span><span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
tokenizer
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">ChatGLMTokenizer<span class="token punctuation">(</span>name_or_path<span class="token operator">=</span><span class="token string">'/root/autodl-tmp/chatglm3-6b-base'</span><span class="token punctuation">,</span> vocab_size<span class="token operator">=</span><span class="token number">64798</span><span class="token punctuation">,</span> model_max_length<span class="token operator">=</span><span class="token number">1000000000000000019884624838656</span><span class="token punctuation">,</span> is_fast<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> padding_side<span class="token operator">=</span><span class="token string">'left'</span><span class="token punctuation">,</span> truncation_side<span class="token operator">=</span><span class="token string">'right'</span><span class="token punctuation">,</span> special_tokens<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">,</span> clean_up_tokenization_spaces<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="2_159"></a>2）定义数据处理函数</h4> 
<p>格式处理：[gMASK]sop&lt;|user|&gt; \n Prompt&lt;|assistant|&gt;\n response eos_token</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">process_func</span><span class="token punctuation">(</span>example<span class="token punctuation">)</span><span class="token punctuation">:</span>
    MAX_LENGTH <span class="token operator">=</span> <span class="token number">256</span> <span class="token comment"># 设置最大长度为256</span>
    input_ids<span class="token punctuation">,</span> attention_mask<span class="token punctuation">,</span> labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token comment"># 初始化输入ID、注意力掩码和标签列表</span>
    instruction <span class="token operator">=</span> <span class="token string">"\n"</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span>example<span class="token punctuation">[</span><span class="token string">"instruction"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> example<span class="token punctuation">[</span><span class="token string">"input"</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>     <span class="token comment"># prompt</span>
    instruction <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>build_chat_input<span class="token punctuation">(</span>instruction<span class="token punctuation">,</span> history<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> role<span class="token operator">=</span><span class="token string">"user"</span><span class="token punctuation">)</span>  <span class="token comment"># [gMASK]sop&lt;|user|&gt; \n prompt &lt;|assistant|&gt;</span>
    response <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token string">"\n"</span> <span class="token operator">+</span> example<span class="token punctuation">[</span><span class="token string">"output"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> add_special_tokens<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        <span class="token comment"># \n response</span>
    input_ids <span class="token operator">=</span> instruction<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> response<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>tokenizer<span class="token punctuation">.</span>eos_token_id<span class="token punctuation">]</span> <span class="token comment">#eos token</span>
    attention_mask <span class="token operator">=</span> instruction<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> response<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
    labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">100</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>instruction<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> response<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>tokenizer<span class="token punctuation">.</span>eos_token_id<span class="token punctuation">]</span>
    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span> <span class="token operator">&gt;</span> MAX_LENGTH<span class="token punctuation">:</span>
        input_ids <span class="token operator">=</span> input_ids<span class="token punctuation">[</span><span class="token punctuation">:</span>MAX_LENGTH<span class="token punctuation">]</span>
        attention_mask <span class="token operator">=</span> attention_mask<span class="token punctuation">[</span><span class="token punctuation">:</span>MAX_LENGTH<span class="token punctuation">]</span>
        labels <span class="token operator">=</span> labels<span class="token punctuation">[</span><span class="token punctuation">:</span>MAX_LENGTH<span class="token punctuation">]</span>
    <span class="token keyword">return</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">"input_ids"</span><span class="token punctuation">:</span> input_ids<span class="token punctuation">,</span>
        <span class="token string">"attention_mask"</span><span class="token punctuation">:</span> attention_mask<span class="token punctuation">,</span>
        <span class="token string">"labels"</span><span class="token punctuation">:</span> labels
    <span class="token punctuation">}</span> 
</code></pre> 
<h4><a id="3_183"></a>3）对数据进行预处理</h4> 
<pre><code class="prism language-python">tokenized_ds <span class="token operator">=</span> ds<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>process_func<span class="token punctuation">,</span> remove_columns<span class="token operator">=</span>ds<span class="token punctuation">.</span>column_names<span class="token punctuation">)</span>
tokenized_ds
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">Dataset<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span>
    features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">,</span> <span class="token string">'attention_mask'</span><span class="token punctuation">,</span> <span class="token string">'labels'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    num_rows<span class="token punctuation">:</span> <span class="token number">26858</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>

</code></pre> 
<h4><a id="4input_ids_200"></a>4）解码检查input_ids的格式</h4> 
<pre><code class="prism language-python">tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>tokenized_ds<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python"><span class="token string">'[gMASK]sop&lt;|user|&gt; \n 解释为什么以下分数等同于1/4\n输入：4/16&lt;|assistant|&gt; \n4/16等于1/4是因为我们可以约分分子分母都除以他们的最大公约数4，得到（4÷4）/ (16÷4）=1/4。分数的约分是用分子和分母除以相同的非零整数，来表示分数的一个相同的值，这因为分数实际上表示了分子除以分母，所以即使两个数同时除以同一个非零整数，分数的值也不会改变。所以4/16 和1/4是两种不同的书写形式，但它们的值相等。'</span>
</code></pre> 
<h4><a id="5labels_212"></a>5）检查labels数据格式</h4> 
<pre><code class="prism language-python">tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">filter</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">100</span><span class="token punctuation">,</span> tokenized_ds<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"labels"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python"><span class="token string">'\n4/16等于1/4是因为我们可以约分分子分母都除以他们的最大公约数4，得到（4÷4）/ (16÷4）=1/4。分数的约分是用分子和分母除以相同的非零整数，来表示分数的一个相同的值，这因为分数实际上表示了分子除以分母，所以即使两个数同时除以同一个非零整数，分数的值也不会改变。所以4/16 和1/4是两种不同的书写形式，但它们的值相等。'</span>
</code></pre> 
<h3><a id="4__224"></a>步骤4 创建模型</h3> 
<p>然后，我们实例化一个预训练模型，这个模型将作为微调的基础。对于大型模型，我们可能还需要进行一些特定的配置，以适应可用的计算资源。（<code>为了节省资源，这里设置为半精度torch_dtype=torch.half</code>）</p> 
<h4><a id="1_226"></a>1、创建模型实例</h4> 
<h5><a id="1_227"></a>1）创建模型</h5> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"/root/autodl-tmp/modelscope/Llama-2-7b-ms"</span><span class="token punctuation">,</span> 
                                             low_cpu_mem_usage<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                             torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>half<span class="token punctuation">,</span>
                                             device_map<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="2_236"></a>2）精度查看确认</h5> 
<pre><code class="prism language-python">model<span class="token punctuation">.</span>dtype
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>float16
</code></pre> 
<h5><a id="3_248"></a>3）查看模型参数</h5> 
<p>检查模型有哪些参数，确认在哪一层添加LoRA微调</p> 
<pre><code class="prism language-python"><span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">)</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">transformer<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>word_embeddings<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">24</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">24</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">24</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">24</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">24</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">24</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">24</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">25</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">25</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">25</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">25</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">25</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">25</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">25</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">26</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">26</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">26</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">26</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">26</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">26</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">26</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">27</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">27</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">27</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>query_key_value<span class="token punctuation">.</span>bias
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">27</span><span class="token punctuation">.</span>self_attention<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">27</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">27</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_h_to_4h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">27</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>dense_4h_to_h<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>final_layernorm<span class="token punctuation">.</span>weight
transformer<span class="token punctuation">.</span>output_layer<span class="token punctuation">.</span>weight
</code></pre> 
<p><code>下面2个部分是LoRA相关的配置。</code></p> 
<h4><a id="2PEFT_1__462"></a>2、PEFT 步骤1 配置文件</h4> 
<p>在使用PEFT进行微调时，我们首先需要创建一个配置文件，该文件定义了微调过程中的各种设置，如学习率调度、优化器选择等。<br> <code>提前安装peft：pip install peft</code></p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> peft <span class="token keyword">import</span> LoraConfig<span class="token punctuation">,</span> TaskType<span class="token punctuation">,</span> get_peft_model<span class="token punctuation">,</span> PeftModel

config <span class="token operator">=</span> LoraConfig<span class="token punctuation">(</span>target_modules<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"query_key_value"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
config
</code></pre> 
<p>输出</p> 
<pre><code class="prism language-python">LoraConfig<span class="token punctuation">(</span>peft_type<span class="token operator">=</span><span class="token operator">&lt;</span>PeftType<span class="token punctuation">.</span>LORA<span class="token punctuation">:</span> <span class="token string">'LORA'</span><span class="token operator">&gt;</span><span class="token punctuation">,</span> auto_mapping<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> base_model_name_or_path<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> revision<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> task_type<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> inference_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> r<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> target_modules<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token string">'query_key_value'</span><span class="token punctuation">}</span><span class="token punctuation">,</span> lora_alpha<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> lora_dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> fan_in_fan_out<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">,</span> modules_to_save<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> init_lora_weights<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> layers_to_transform<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> layers_pattern<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> rank_pattern<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">,</span> alpha_pattern<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">,</span> megatron_config<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> megatron_core<span class="token operator">=</span><span class="token string">'megatron.core'</span><span class="token punctuation">,</span> loftq_config<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="3PEFT_2__479"></a>3、PEFT 步骤2 创建模型</h4> 
<p>接下来，我们使用PEFT和预训练模型来创建一个微调模型。这个模型将包含原始的预训练模型以及由PEFT引入的低秩参数。</p> 
<h5><a id="1_481"></a>1）创建微调模型</h5> 
<pre><code class="prism language-python">model <span class="token operator">=</span> get_peft_model<span class="token punctuation">(</span>model<span class="token punctuation">,</span> config<span class="token punctuation">)</span>
config
</code></pre> 
<p>输出</p> 
<pre><code class="prism language-python">LoraConfig<span class="token punctuation">(</span>peft_type<span class="token operator">=</span><span class="token operator">&lt;</span>PeftType<span class="token punctuation">.</span>LORA<span class="token punctuation">:</span> <span class="token string">'LORA'</span><span class="token operator">&gt;</span><span class="token punctuation">,</span> auto_mapping<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> base_model_name_or_path<span class="token operator">=</span><span class="token string">'/root/autodl-tmp/modelscope/Llama-2-7b-ms'</span><span class="token punctuation">,</span> revision<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> task_type<span class="token operator">=</span><span class="token operator">&lt;</span>TaskType<span class="token punctuation">.</span>CAUSAL_LM<span class="token punctuation">:</span> <span class="token string">'CAUSAL_LM'</span><span class="token operator">&gt;</span><span class="token punctuation">,</span> inference_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> r<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> target_modules<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token string">'q_proj'</span><span class="token punctuation">,</span> <span class="token string">'v_proj'</span><span class="token punctuation">}</span><span class="token punctuation">,</span> lora_alpha<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> lora_dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> fan_in_fan_out<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">,</span> modules_to_save<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> init_lora_weights<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> layers_to_transform<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> layers_pattern<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> rank_pattern<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">,</span> alpha_pattern<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">,</span> megatron_config<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> megatron_core<span class="token operator">=</span><span class="token string">'megatron.core'</span><span class="token punctuation">,</span> loftq_config<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="2LoRA_494"></a>2）查看LoRA层添加情况</h5> 
<p>打印所有的模型参数，查看LoRA添加到了哪一层</p> 
<pre><code class="prism language-python"><span class="token keyword">for</span> name<span class="token punctuation">,</span> parameter <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">)</span>
</code></pre> 
<h5><a id="3_503"></a>3）查看模型中可训练参数的数量</h5> 
<pre><code class="prism language-python">model<span class="token punctuation">.</span>print_trainable_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#打印出模型中可训练参数的数量</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">trainable params<span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span><span class="token number">949</span><span class="token punctuation">,</span><span class="token number">696</span> <span class="token operator">|</span><span class="token operator">|</span> <span class="token builtin">all</span> params<span class="token punctuation">:</span> <span class="token number">6</span><span class="token punctuation">,</span><span class="token number">245</span><span class="token punctuation">,</span><span class="token number">533</span><span class="token punctuation">,</span><span class="token number">696</span> <span class="token operator">|</span><span class="token operator">|</span> trainable<span class="token operator">%</span><span class="token punctuation">:</span> <span class="token number">0.031217444255383614</span>
</code></pre> 
<h3><a id="5__513"></a>步骤5 配置训练参数</h3> 
<p>在这一步，我们定义训练参数，这些参数包括输出目录、学习率、权重衰减、梯度累积步数、训练周期数等。这些参数将被用来配置训练过程。(<code>设置adam_epsilon=1e-4避免精度溢出，半精度的情况下才需要设置</code>）</p> 
<pre><code class="prism language-python">args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
    output_dir<span class="token operator">=</span><span class="token string">"/root/autodl-tmp/chatglm2output"</span><span class="token punctuation">,</span> <span class="token comment">#输出目录，用于存储模型和日志文件。</span>
    per_device_train_batch_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token comment"># 每个设备的训练批次大小，即每个设备每次处理的数据量，批次越大，训练时需要资源越多</span>
    gradient_accumulation_steps<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token comment"># 指定梯度累积步数。用于控制梯度更新的频率。在每个累积步中，模型会计算多个批次的梯度，然后一次性更新权重。这可以减少内存占用并提高训练速度。在本例子中，每16个步骤进行一次梯度更新。</span>
    logging_steps<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token comment">#日志记录步数，用于控制每隔多少步记录一次训练日志。</span>
    num_train_epochs<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token comment">#训练轮数，即模型在整个训练集上进行迭代的次数。正常情况会训练很多轮</span>
    learning_rate<span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">,</span> <span class="token comment">#学习率，控制模型参数更新的速度。较小的学习率会使模型收敛得更快，但可能需要更多的训练轮数</span>
    adam_epsilon<span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">,</span> <span class="token comment">#Adam优化器的epsilon值，用于防止除以零的情况。</span>
    remove_unused_columns<span class="token operator">=</span><span class="token boolean">False</span> <span class="token comment">#是否移除未使用的列，如果设置为False，则保留所有列，否则只保留模型所需的列。</span>
<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="6__530"></a>步骤6 创建训练器</h3> 
<p>最后，我们创建一个训练器实例，它封装了训练循环。训练器将负责运行训练过程，并根据我们之前定义的参数进行优化。</p> 
<pre><code class="prism language-python">trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>
    model<span class="token operator">=</span>model<span class="token punctuation">,</span>
    args<span class="token operator">=</span>args<span class="token punctuation">,</span>
    train_dataset<span class="token operator">=</span>tokenized_ds<span class="token punctuation">,</span>
    data_collator<span class="token operator">=</span>DataCollatorForSeq2Seq<span class="token punctuation">(</span>tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

</code></pre> 
<h3><a id="7__543"></a>步骤7 模型训练</h3> 
<p>通过调用训练器的<code>train()</code>方法，我们启动模型的训练过程。这将根据之前定义的参数执行模型的训练。</p> 
<pre><code class="prism language-python">trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="8__550"></a>步骤8 模型推理</h3> 
<p>训练完成后，我们可以使用训练好的模型进行推理。</p> 
<pre><code class="prism language-python">model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> <span class="token string">"如何写简历？"</span><span class="token punctuation">,</span> history<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">写简历时，您可以按照以下步骤进行：

<span class="token number">1.</span> 个人信息：在开头部分，写上您的姓名、联系方式和地址。

<span class="token number">2.</span> 教育背景：列出您所获得的所有学历，包括学校名称、学位和毕业日期。

<span class="token number">3.</span> 工作经历：在简历中，按照时间顺序列出您过去的工作经历，包括公司名称、职位、职责和时间。

<span class="token number">4.</span> 技能和证书：在您的简历中，列出您掌握的技能和证书，例如编程语言、软件、证书和证书等级。

<span class="token number">5.</span> 个人亮点：在您的简历中，列出您的个人亮点，例如您在团队合作、创新、解决问题和沟通方面的优势。

<span class="token number">6.</span> 总结：在结尾部分，简要总结您在各个领域的工作经验和技能，并表达您对未来的职业发展的期望。

请注意，简历应该简洁明了，突出您的优势和技能，同时避免使用过于复杂的语言和词汇。您可以参考模板或在线简历生成器来帮助您创建一份简历。
</code></pre> 
<h2><a id="_577"></a>总结</h2> 
<p>本文深入探讨了大语言模型ChatGLM3的微调实践，从架构设计到预训练任务，再到实际的微调操作步骤。文章首先回顾了Transformer架构的重要性，接着分析了ChatGLM3融合不同预训练模型优势的独特动机和核心机制，如自回归填空和2D位置编码。此外，文中详细描述了基于LoRA技术的微调过程，包括数据准备、模型构建、训练以及推理等关键步骤。这些内容旨在为大家提供宝贵的参考，帮助大家在自然语言处理项目中有效应用大型预训练语言模型。</p> 
<p><img src="https://images2.imgbox.com/b8/cf/rf8R2Guh_o.png" alt="在这里插入图片描述"></p> 
<p>🎯🔖更多专栏系列文章：<a href="https://blog.csdn.net/xiaobing259/category_12628007.html?spm=1001.2014.3001.5482"><strong>AIGC-AI大模型探索之路</strong></a></p> 
<blockquote> 
 <p>如果文章内容对您有所触动，别忘了<font color="red"><strong>点赞、⭐关注，收藏</strong></font>！加入我，让我们携手同行AI的探索之旅，一起开启智能时代的大门！</p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5089fd683ac3fe1866cf760627f75b31/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">限流算法(令牌桶&amp;漏桶&amp;计数器)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e62856c9b3aab7bbd1f20b0fef6323b8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【C#】.net core 6.0 ApiController，API控制器方法，API接口以实体类作为接收参数应该注意的点</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程学习者.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>