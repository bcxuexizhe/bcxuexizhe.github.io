<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Spark环境搭建和使用方法 - 编程学习者</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcxuexizhe.github.io/posts/7416b78a5a4c0d410c655472b7629acd/">
  <meta property="og:site_name" content="编程学习者">
  <meta property="og:title" content="Spark环境搭建和使用方法">
  <meta property="og:description" content="目录
一、安装Spark
（一）基础环境
（二）安装Python3版本
（三）下载安装Spark
（四）配置相关文件
二、在pyspark中运行代码
（一）pyspark命令
（二）启动pyspark 三、开发Spark独立应用程序
（一）编写程序
（二）通过spark-submit运行程序 一、安装Spark （一）基础环境 安装Spark之前需要安装Linux系统、Java环境（Java8或JDK1.8以上版本）和Hadoop环境。
可参考本专栏前面的博客：
大数据软件基础（3） —— 在VMware上安装Linux集群-CSDN博客
大数据存储技术（1）—— Hadoop简介及安装配置-CSDN博客
（二）安装Python3版本 1、查看当前环境下的Python版本
[root@bigdata zhc]# python --version Python 2.7.5 版本已经不能满足当前编程环境需求，所以要安装较高版本的Python3，但Python 2.7.5 版本不能卸载。
2、连网下载Python3
[root@bigdata zhc]# yum install -y python3 如图所示，Python3安装完成。
安装的版本为Python 3.6.8。
（三）下载安装Spark 1、Spark安装包下载地址：https://spark.apache.org/
进入下载页面后，点击主页的“Download”按钮进入下载页面，下载页面中提供了几个下载选项，主要是Spark release及Package type的选择，如下图所示。
我这里下的是Spark 2.4.0版本，没有此版本的，也可以下载Spark 3.2.4或更高版本的。
2、解压安装包spark-2.4.0-bin-without-hadoop.tgz至路径 /usr/local
[root@bigdata uploads]# tar -zxvf spark-2.4.0-bin-without-hadoop.tgz -C /usr/local 更改文件目录名：
[root@bigdata local]# mv spark-2.4.0-bin-without-hadoop/ spark （四）配置相关文件 1、配置Spark的classpath
先切换到 /usr/local/spark/conf 目录下，复制spark-env.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-12-14T13:28:32+08:00">
    <meta property="article:modified_time" content="2023-12-14T13:28:32+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程学习者" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程学习者</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Spark环境搭建和使用方法</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%E5%AE%89%E8%A3%85Spark-toc" style="margin-left:40px;"><a href="#%E4%B8%80%E3%80%81%E5%AE%89%E8%A3%85Spark" rel="nofollow">一、安装Spark</a></p> 
<p id="%EF%BC%88%E4%B8%80%EF%BC%89%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83-toc" style="margin-left:80px;"><a href="#%EF%BC%88%E4%B8%80%EF%BC%89%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83" rel="nofollow">（一）基础环境</a></p> 
<p id="%EF%BC%88%E4%BA%8C%EF%BC%89%E5%AE%89%E8%A3%85Python3%E7%89%88%E6%9C%AC-toc" style="margin-left:80px;"><a href="#%EF%BC%88%E4%BA%8C%EF%BC%89%E5%AE%89%E8%A3%85Python3%E7%89%88%E6%9C%AC" rel="nofollow">（二）安装Python3版本</a></p> 
<p id="%EF%BC%88%E4%B8%89%EF%BC%89%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85Spark-toc" style="margin-left:80px;"><a href="#%EF%BC%88%E4%B8%89%EF%BC%89%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85Spark" rel="nofollow">（三）下载安装Spark</a></p> 
<p id="%EF%BC%88%E5%9B%9B%EF%BC%89%E9%85%8D%E7%BD%AE%E7%9B%B8%E5%85%B3%E6%96%87%E4%BB%B6-toc" style="margin-left:80px;"><a href="#%EF%BC%88%E5%9B%9B%EF%BC%89%E9%85%8D%E7%BD%AE%E7%9B%B8%E5%85%B3%E6%96%87%E4%BB%B6" rel="nofollow">（四）配置相关文件</a></p> 
<p id="%E4%BA%8C%E3%80%81%E5%9C%A8pyspark%E4%B8%AD%E8%BF%90%E8%A1%8C%E4%BB%A3%E7%A0%81-toc" style="margin-left:40px;"><a href="#%E4%BA%8C%E3%80%81%E5%9C%A8pyspark%E4%B8%AD%E8%BF%90%E8%A1%8C%E4%BB%A3%E7%A0%81" rel="nofollow">二、在pyspark中运行代码</a></p> 
<p id="%EF%BC%88%E4%B8%80%EF%BC%89pyspark%E5%91%BD%E4%BB%A4-toc" style="margin-left:80px;"><a href="#%EF%BC%88%E4%B8%80%EF%BC%89pyspark%E5%91%BD%E4%BB%A4" rel="nofollow">（一）pyspark命令</a></p> 
<p id="%EF%BC%88%E4%BA%8C%EF%BC%89%E5%90%AF%E5%8A%A8pyspark%C2%A0-toc" style="margin-left:80px;"><a href="#%EF%BC%88%E4%BA%8C%EF%BC%89%E5%90%AF%E5%8A%A8pyspark%C2%A0" rel="nofollow">（二）启动pyspark </a></p> 
<p id="%E4%B8%89%E3%80%81%E5%BC%80%E5%8F%91Spark%E7%8B%AC%E7%AB%8B%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F-toc" style="margin-left:40px;"><a href="#%E4%B8%89%E3%80%81%E5%BC%80%E5%8F%91Spark%E7%8B%AC%E7%AB%8B%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F" rel="nofollow">三、开发Spark独立应用程序</a></p> 
<p id="%EF%BC%88%E4%B8%80%EF%BC%89%E7%BC%96%E5%86%99%E7%A8%8B%E5%BA%8F-toc" style="margin-left:80px;"><a href="#%EF%BC%88%E4%B8%80%EF%BC%89%E7%BC%96%E5%86%99%E7%A8%8B%E5%BA%8F" rel="nofollow">（一）编写程序</a></p> 
<p id="%EF%BC%88%E4%BA%8C%EF%BC%89%E9%80%9A%E8%BF%87spark-submit%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F%C2%A0-toc" style="margin-left:80px;"><a href="#%EF%BC%88%E4%BA%8C%EF%BC%89%E9%80%9A%E8%BF%87spark-submit%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F%C2%A0" rel="nofollow">（二）通过spark-submit运行程序 </a></p> 
<hr id="hr-toc"> 
<p></p> 
<h3>一、安装Spark</h3> 
<h4 id="%EF%BC%88%E4%B8%80%EF%BC%89%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83">（一）基础环境</h4> 
<p>安装Spark之前需要安装Linux系统、Java环境（Java8或JDK1.8以上版本）和Hadoop环境。</p> 
<p>可参考本专栏前面的博客：<br><a href="https://blog.csdn.net/Morse_Chen/article/details/134809529" title="大数据软件基础（3） —— 在VMware上安装Linux集群-CSDN博客">大数据软件基础（3） —— 在VMware上安装Linux集群-CSDN博客</a><br><a href="https://blog.csdn.net/Morse_Chen/article/details/134833801" title="大数据存储技术（1）—— Hadoop简介及安装配置-CSDN博客">大数据存储技术（1）—— Hadoop简介及安装配置-CSDN博客</a></p> 
<h4 id="%EF%BC%88%E4%BA%8C%EF%BC%89%E5%AE%89%E8%A3%85Python3%E7%89%88%E6%9C%AC">（二）安装Python3版本</h4> 
<p>1、查看当前环境下的Python版本</p> 
<pre><code class="language-css">[root@bigdata zhc]# python --version</code></pre> 
<p><img alt="" height="152" src="https://images2.imgbox.com/f4/b5/cBjGc1z1_o.png" width="500"></p> 
<p> Python 2.7.5 版本已经不能满足当前编程环境需求，所以要安装较高版本的Python3，但Python 2.7.5 版本不能卸载。</p> 
<p>2、连网下载Python3</p> 
<pre><code class="language-css">[root@bigdata zhc]# yum install -y python3</code></pre> 
<p> 如图所示，Python3安装完成。</p> 
<p><img alt="" height="1028" src="https://images2.imgbox.com/f4/67/Vf1phCh7_o.png" width="1200"></p> 
<p> 安装的版本为Python 3.6.8。</p> 
<h4 id="%EF%BC%88%E4%B8%89%EF%BC%89%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85Spark">（三）下载安装Spark</h4> 
<p>1、<strong>Spark安装包下载地址</strong>：<a href="https://spark.apache.org/" rel="nofollow" title="https://spark.apache.org/">https://spark.apache.org/</a></p> 
<p>进入下载页面后，点击主页的“Download”按钮进入下载页面，下载页面中提供了几个下载选项，主要是Spark release及Package type的选择，如下图所示。</p> 
<p><img alt="" height="300" src="https://images2.imgbox.com/e1/f2/m4TGPLlo_o.png" width="960"></p> 
<p>我这里下的是Spark 2.4.0版本，没有此版本的，也可以下载Spark 3.2.4或更高版本的。</p> 
<p>2、<strong>解压安装包spark-2.4.0-bin-without-hadoop.tgz至路径 /usr/local</strong></p> 
<p><img alt="" height="201" src="https://images2.imgbox.com/50/45/i12dWKkx_o.png" width="782"></p> 
<pre><code class="language-css">[root@bigdata uploads]# tar -zxvf spark-2.4.0-bin-without-hadoop.tgz -C /usr/local</code></pre> 
<p>更改文件目录名：</p> 
<pre><code class="language-css">[root@bigdata local]# mv spark-2.4.0-bin-without-hadoop/ spark </code></pre> 
<p><img alt="" height="466" src="https://images2.imgbox.com/30/3a/QWpY8TmJ_o.png" width="600"></p> 
<h4 id="%EF%BC%88%E5%9B%9B%EF%BC%89%E9%85%8D%E7%BD%AE%E7%9B%B8%E5%85%B3%E6%96%87%E4%BB%B6"><strong>（四）配置相关文件</strong></h4> 
<p>1、<strong>配置Spark的classpath</strong></p> 
<p>先切换到 /usr/local/spark/conf 目录下，复制spark-env.sh.template重命名为spark-env.sh。</p> 
<pre><code class="language-css">[root@bigdata local]# cd /usr/local/spark/conf
[root@bigdata conf]# cp spark-env.sh.template spark-env.sh
[root@bigdata conf]# ll
总用量 44
-rw-r--r-- 1 zhc  zhc   996 10月 29 2018 docker.properties.template
-rw-r--r-- 1 zhc  zhc  1105 10月 29 2018 fairscheduler.xml.template
-rw-r--r-- 1 zhc  zhc  2025 10月 29 2018 log4j.properties.template
-rw-r--r-- 1 zhc  zhc  7801 10月 29 2018 metrics.properties.template
-rw-r--r-- 1 zhc  zhc   865 10月 29 2018 slaves.template
-rw-r--r-- 1 zhc  zhc  1292 10月 29 2018 spark-defaults.conf.template
-rwxr-xr-x 1 root root 4221 12月 13 20:23 spark-env.sh
-rwxr-xr-x 1 zhc  zhc  4221 10月 29 2018 spark-env.sh.template
[root@bigdata conf]# vi spark-env.sh</code></pre> 
<p>将如下内容加到spark-env.sh文件的第一行。</p> 
<pre><code class="language-css">export SPARK_DIST_CLASSPATH=$(/usr/local/servers/hadoop/bin/hadoop  classpath)</code></pre> 
<p><img alt="" height="862" src="https://images2.imgbox.com/10/66/k6z6tCX6_o.png" width="1200"></p> 
<p>实现了Spark和Hadoop的交互。</p> 
<p>2、<strong>配置 /etc/profile 文件</strong></p> 
<p>将如下内容添加到 /etc/profile 文件最后，并使其生效。</p> 
<pre><code class="language-css">[root@bigdata conf]# vi /etc/profile
[root@bigdata conf]# source /etc/profile</code></pre> 
<pre><code class="language-css">export SPARK_HOME=/usr/local/spark
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH
export PYSPARK_PYTHON=python3
export PATH=$HADOOP_HOME/bin:$SPARK_HOME/bin:$PATH</code></pre> 
<p>如下图所示。 </p> 
<p><img alt="" height="863" src="https://images2.imgbox.com/44/46/rz0Qva0V_o.png" width="1200"></p> 
<p>至此，Spark环境就安装配置好了。</p> 
<p>输入实例SparkPi验证Spark环境。为了从大量的输出信息中快速找到我们想要的自行结果，可以使用grep命令进行过滤。命令如下：</p> 
<pre><code class="language-css">[root@bigdata spark]# run-example SparkPi 2&gt;&amp;1 |grep "Pi is"</code></pre> 
<p><img alt="" height="69" src="https://images2.imgbox.com/29/34/aMaPB9OP_o.png" width="500"> </p> 
<h3 id="%E4%BA%8C%E3%80%81%E5%9C%A8pyspark%E4%B8%AD%E8%BF%90%E8%A1%8C%E4%BB%A3%E7%A0%81">二、在pyspark中运行代码</h3> 
<h4 id="%EF%BC%88%E4%B8%80%EF%BC%89pyspark%E5%91%BD%E4%BB%A4">（一）pyspark命令</h4> 
<p>pyspark命令及其常用的参数如下：</p> 
<blockquote> 
 <p>pyspark --master &lt;master-url&gt;</p> 
</blockquote> 
<p>Spark的运行模式取决于传递给SparkContext的Master URL的值。Master URL可以是以下任一种形式：</p> 
<p>        （1）local 使用一个Worker线程本地化运行SPARK(完全不并行)<br>         （2）local[*] 使用逻辑CPU个数数量的线程来本地化运行Spark<br>         （3）local[K] 使用K个Worker线程本地化运行Spark（理想情况下，K应该根据运行机器的CPU核数设定)<br>         （4）spark://HOST:PORT 连接到指定的Spark standalone master。默认端口是7077<br>         （5）yarn-client 以客户端模式连接YARN集群。集群的位置可以在HADOOP_CONF_DIR环境变量中找到<br>         （6）yarn-cluster 以集群模式连接YARN集群。集群的位置可以在HADOOP_CONF_DIR环境变量中找到<br>         （7）mesos://HOST:PORT 连接到指定的Mesos集群。默认接口是5050</p> 
<p>在Spark中采用本地模式启动pyspark的命令主要包含以下参数：<br> --master：这个参数表示当前的pyspark要连接到哪个master，如果是local[*]，就是使用本地模式启动pyspark，其中，中括号内的星号表示需要使用几个CPU核心(core)，也就是启动几个线程模拟Spark集群<br> --jars： 这个参数用于把相关的JAR包添加到CLASSPATH中；如果有多个jar包，可以使用逗号分隔符连接它们。</p> 
<p>比如，要采用本地模式，在4个CPU核心上运行pyspark：</p> 
<pre><code class="language-bash">$ cd /usr/local/spark
$ ./bin/pyspark --master local[4]</code></pre> 
<p>或者，可以在CLASSPATH中添加code.jar，命令如下：</p> 
<pre><code class="language-bash">$ cd /usr/local/spark
$ ./bin/pyspark --master local[4] --jars code.jar </code></pre> 
<p> 可以执行“pyspark --help”命令，获取完整的选项列表，具体如下：</p> 
<pre><code class="language-bash">$ cd /usr/local/spark
$ ./bin/pyspark --help</code></pre> 
<h4 id="%EF%BC%88%E4%BA%8C%EF%BC%89%E5%90%AF%E5%8A%A8pyspark%C2%A0">（二）启动pyspark </h4> 
<p>执行如下命令启动pyspark（默认是local模式）：</p> 
<pre><code class="language-css">[root@bigdata zhc]# cd /usr/local/spark
[root@bigdata spark]# pyspark</code></pre> 
<p><img alt="" height="336" src="https://images2.imgbox.com/9f/83/EClP5HCw_o.png" width="1200"></p> 
<p>可以在里面输入scala代码进行调试：</p> 
<pre><code class="language-bash">&gt;&gt;&gt; 8*2+5
21
</code></pre> 
<p> 可以使用命令“exit()”退出pyspark：</p> 
<pre><code class="language-bash">&gt;&gt;&gt; exit()
</code></pre> 
<h3 id="%E4%B8%89%E3%80%81%E5%BC%80%E5%8F%91Spark%E7%8B%AC%E7%AB%8B%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F">三、开发Spark独立应用程序</h3> 
<h4 id="%EF%BC%88%E4%B8%80%EF%BC%89%E7%BC%96%E5%86%99%E7%A8%8B%E5%BA%8F">（一）编写程序</h4> 
<pre><code class="language-python"># /home/zhc/mycode/WordCount.py
from pyspark import SparkConf, SparkContext
conf = SparkConf().setMaster("local").setAppName("My App")
sc = SparkContext(conf = conf)
logFile = "file:///usr/local/spark/README.md"
logData = sc.textFile(logFile, 2).cache()
numAs = logData.filter(lambda line: 'a' in line).count()
numBs = logData.filter(lambda line: 'b' in line).count()
print('Lines with a: %s, Lines with b: %s' % (numAs, numBs))
</code></pre> 
<p>对于这段Python代码，可以直接使用如下命令执行：</p> 
<pre><code class="language-css">[root@bigdata zhc]# cd /home/zhc/mycode
[root@bigdata mycode]# vi WordCount.py
[root@bigdata mycode]# ll
总用量 8
-rw-r--r-- 1 root root 430 12月 14 12:54 WordCount.py
-rw-r--r-- 1 root root  56 12月  9 18:55 word.txt
[root@bigdata mycode]# python3 WordCount.py</code></pre> 
<p>执行该命令以后，可以得到如下结果：</p> 
<p><img alt="" height="328" src="https://images2.imgbox.com/78/6f/NvQjgKvZ_o.png" width="1200"></p> 
<h4 id="%EF%BC%88%E4%BA%8C%EF%BC%89%E9%80%9A%E8%BF%87spark-submit%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F%C2%A0">（二）通过spark-submit运行程序 </h4> 
<p>可以通过spark-submit提交应用程序，该命令的格式如下：</p> 
<blockquote> 
 <p>spark-submit  </p> 
 <p>        --master &lt;master-url&gt;  </p> 
 <p>        --deploy-mode &lt;deploy-mode&gt;   #部署模式  </p> 
 <p>        ... #其他参数  </p> 
 <p>        &lt;application-file&gt;  #Python代码文件  </p> 
 <p>        [application-arguments]  #传递给主类的主方法的参数</p> 
</blockquote> 
<p>可以执行“spark-submit  --help”命令，获取完整的选项列表，具体如下：</p> 
<pre><code class="language-bash">$ cd /usr/local/spark
$ ./bin/spark-submit --help</code></pre> 
<p>以通过 spark-submit 提交到 Spark 中运行，命令如下：<br> 注意要在 /home/zhc/mycode/ 路径下执行spark-submit，否则要使用绝对路径。</p> 
<pre><code class="language-css">[root@bigdata mycode]# spark-submit WordCount.py</code></pre> 
<pre><code class="language-css">[root@bigdata zhc]# spark-submit /home/zhc/mycode/WordCount.py    #绝对路径</code></pre> 
<p>运行结果如图所示： </p> 
<p><img alt="" height="827" src="https://images2.imgbox.com/a7/d1/KYJh22w6_o.png" width="1200"></p> 
<p>此时我们发现有大量的INFO信息，这些信息属于干扰信息，对于我们有用的只有“Lines with a: 62, Lines with b: 30”这一行。为了避免其他多余信息对运行结果的干扰，可以修改log4j的日志信息显示级别，具体方法如下：</p> 
<pre><code class="language-css">[root@bigdata spark]# cd /usr/local/spark/conf
[root@bigdata conf]# ll
总用量 44
-rw-r--r-- 1 zhc  zhc   996 10月 29 2018 docker.properties.template
-rw-r--r-- 1 zhc  zhc  1105 10月 29 2018 fairscheduler.xml.template
-rw-r--r-- 1 zhc  zhc  2025 10月 29 2018 log4j.properties.template
-rw-r--r-- 1 zhc  zhc  7801 10月 29 2018 metrics.properties.template
-rw-r--r-- 1 zhc  zhc   865 10月 29 2018 slaves.template
-rw-r--r-- 1 zhc  zhc  1292 10月 29 2018 spark-defaults.conf.template
-rwxr-xr-x 1 root root 4300 12月 13 20:33 spark-env.sh
-rwxr-xr-x 1 zhc  zhc  4221 10月 29 2018 spark-env.sh.template
[root@bigdata conf]# cp log4j.properties.template log4j.properties
[root@bigdata conf]# vi log4j.properties</code></pre> 
<p>打开 log4j.properties 文件后，可以发现包含如下一行信息：</p> 
<blockquote> 
 <p>log4j.rootCategory=INFO, console</p> 
</blockquote> 
<p>将其修改为： </p> 
<blockquote> 
 <p>log4j.rootCategory=ERROR, console</p> 
</blockquote> 
<p><img alt="" height="820" src="https://images2.imgbox.com/63/de/GSVb0KqE_o.png" width="1200"></p> 
<p>再次回到 /home/zhc/mycode/ 路径下执行spark-submit，就会发现没有INFO信息了。</p> 
<pre><code class="language-css">[root@bigdata mycode]# spark-submit WordCount.py</code></pre> 
<p><img alt="" height="160" src="https://images2.imgbox.com/a4/06/fsy7wfQB_o.png" width="1200"></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/30d40c164988b607fb7d7302c5f93467/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">python之tkinter库的grid布局</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2c7e760df009a026d64b87de0865c85f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">c# 数据保存为PDF（三） （PdfSharp篇）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程学习者.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>