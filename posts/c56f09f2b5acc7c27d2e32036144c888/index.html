<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AIGC原理：扩散模型diffusion综述一：面向视觉计算的扩散模型研究进展 - 编程学习者</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcxuexizhe.github.io/posts/c56f09f2b5acc7c27d2e32036144c888/">
  <meta property="og:site_name" content="编程学习者">
  <meta property="og:title" content="AIGC原理：扩散模型diffusion综述一：面向视觉计算的扩散模型研究进展">
  <meta property="og:description" content="论文地址：State of the Art on Diffusion Models for Visual Computing
👉 贴一幅SGM（Score-based Generative Model）的原因是宋飏博士将他2019年提出的SMLD模型和2020年Jonathan Ho提出的DDPM采用SDE进行一统这两大极为相似的生成式模型。殊途同归，基于概率的扩散模型DDPM和基于分数的扩散模型SMLD都是通过利用Unet训练一个通过不同时间步控制的不同噪声图片的噪声预测器、分数预测器，最终通过DDPM的采样公式或者退火的郎之万动力学采样公式进行生成图片。
🔥 摘要 由于生成式AI的出现，计算机视觉领域正在迅速发展，它为图像、视频和3D场景的生成、编辑和重建解锁了前所未有的能力。在这些领域中，扩散模型diffusion是首选的生成式AI架构。仅在去年，关于基于扩散的工具和应用的文献就呈指数级增长，每天计算机图形学、计算机视觉和AI社区都有新的作品出现在arXiv上。这一领域的迅速发展使它很难跟上最近的所有发展。这份最新报告(STAR)的目标是介绍扩散模型的基本数学概念、流行的稳定扩散模型的实现细节和设计选择，以及概述这些生成式AI工具的重要方面，包括个性化、条件调节、反演等。对快速增长的基于扩散的生成和编辑进行了全面的概述，按生成媒体的类型进行分类，包括2D图像、视频、3D物体、运动和4D场景。讨论了可用的数据集、指标、公开挑战和社会影响。这为研究人员、艺术家和从业人员等探索这个领域提供了一个直观的起点。
1. 简介 几十年来，计算机图形学和3D计算机视觉界一直在努力开发准确的物理模型，以计算机生成的图像或从照片中推断场景的物理属性。该方法包括渲染，仿真，几何处理和摄影测量，构成了多个行业的基石，包括视觉效果，游戏，图像和视频处理，计算机辅助设计，虚拟和增强现实，数据可视化，机器人，自动驾驶汽车，遥感等。
生成性(AI)的出现标志着计算机视觉的范式转变。生成式AI工具可以生成和编辑逼真和风格化的像、视频或3D对象，只有文本提示或高级用户指导作为输入。这些工具使计算机视觉中的许多费力的过程自动化，这些过程以前是由具有专业领域知识的专家生成的，现在可以更广泛地使用它们。
生成式AI以前所未有的能力被计算机视觉领域解锁为基础模型，如稳定扩散 [RBL∗22], Imagen [SCS∗22], Midjourney[Mid23]，或DALL-E 2 [Ope23a]和DALL-E 3 [Ope23b]。通过对数亿到数十亿的文本-图像对进行训练，这些模型“看到了一切”，并且估计有数十亿个可学习参数。在经过大量高端图形处理单元(gpu)的训练后，这些模型构成了上述生成式AI工具的基础。通常用于图像、视频和3D对象生成。基于卷积神经网络(CNN)的扩散模型是典型，这些模型以多模态方式与基与transformer的架构的文本编码器相结合，如CLIP [RKH∗21]。
虽然许多2D图像生成基础模型的成功和训练来自资金雄厚的行业参与者，使用了大量的资源，但学术界仍然有空间以主要方式为图形和视觉工具的开发做出贡献。例如，不清楚如何将现有的图像基础模型扩展到其他更高维的领域，如视频和3D场景生成。这在很大程度上是由于缺乏某些类
型的训练数据。例如，网络包含数十亿张2D图像，但高质量和多样化的3D物体或场景的实例要少得多。此外，如何缩放2D图像生成架构以处理更高维度，如视频、3D场景或4D多视图一致场景生成所需要的。当前一个主要的限制，由于网络的规模和迭代性质，扩散模型在推理时相当缓慢，即使网络上存在大量(未标记)视频数据，当前的网络架构往往效率太低，无法在合理的时间或合理的计算资源上进行训练。
尽管仍然存在的挑战，但最近的发展在过去一年中刺激了计算机视觉学者们,使得扩散模型的爆炸式增长(见图1中的代表性示例)。这份最新报告(STAR)的目标是介绍扩散模型的基本原理，结构化地概述许多最近关于扩散模型在视觉计算中的应用的工作，并概述公开的挑战。
STAR的结构如下:
1.简介
2.概述了范围，并向感兴趣的读者介绍了这里没有涉及的密切相关主题的调查;
3.概述了二维扩散的数学基础;
4.讨论了从2D图像向视频、3D和更高维扩散模型转移的挑战;
5.概述了基于扩散的视频合成和编辑的方法;
6.总结了最近的3D对象和场景生成方法;
7.包括对多视图一致视频的4D时空扩散的讨论，人体运动和场景生成(例如，使用参数化人体模型);
8.包括对现有训练数据的简要讨论;
9.用于各种生成内容的评论指标;
10.概述了公开挑战;
11.讨论社会影响和伦理问题;
12.STAR结尾。
2. STAR的范围 本文重点介绍了扩散模型在计算机视觉中应用的最新进展。讨论了扩散模型在生成和编辑图像、视频、3D物体或场景以及多视图一致的4D动态场景中的作用。我们首先建立扩散模型的数学基础。这包括应用于2D图像的一般扩散过程的简要介绍。深入研究了这些技术如何实现高维信号的生成建模，对视频、3D和4D数据的扩散模型的主要方法进行了全面的概述。本报告旨在强调利用扩散模型解决图像域以外数据问题的技术。考虑到这一点，我们不会涵盖只适用于2D数据的每一种方法。本文也没有讨论利用扩散模型以外的生成的工作。
其他生成方法，如GANs，与扩散模型密切相关。但是，我们认为它们超出了本报告的范围。我们建议读者参考 [GSW∗21]以获得关于GANs的深入讨论， [BTLLW21,LZW∗23, SPX∗22]以获得其他生成方法的更广泛的回顾，或使用不同的生成模型架构进行多模态图像合成和编辑[ZYW∗23]。最近，基础模型已经变得类似于在互联网规模的数据图像 [RBL∗22]上训练的扩散模型。虽然本报告讨论了利用此类模型的方法，但请参阅 [BHA∗21]以了解自然语言处理、计算机视觉和其他领域中基础模型的介绍和概述。最后但并非最不重要的是，文本到图像(T2I)生成的爆炸性进展导致了大型语言模型(LLM)和扩散模型之间的内在联系;有兴趣的读者可以参考 [ZZL∗23]获取关于LLM的全面调查。
本报告涵盖了发表在《主要计算机视觉、机器学习和计算机图形会议论文集》上的论文，以及在arXiv上发布的预印本(2021- 2023)。本报告的作者根据其与本调查范围的相关性选择了论文，因为我们的目的是对视觉计算背景下扩散模型的快速进展提供一个全面的概述。然而，尽管本报告是特定领域中最先进的方法的列表，但我们不声称完整，并强烈建议读者参考引用的作品以进行深入的讨论和细节。
3. 扩散模型的基础 在本节中，我们简要概述扩散模型的基本原理。介绍了数学基础，以流行的稳定扩散模型为例，讨论了实际实现，然后概述了条件和指导的重要概念，然后讨论了与反演、图像编辑和定制相关的概念。本节涵盖了大量的参考资料，所以我们侧重于给读者一个清晰和高层次的概述，介绍二维图像背景下基于扩散的生成和编辑的最重要的概念。
3.1. 数学基础 假设我们有一个训练样本数据集，其中数据中的每个示例都独立于基础数据分布Pdata(x),我们希望用一个模型拟合到Pdata(x)，以便通过从该分布中采样来合成新的图像。
去噪扩散模型推理一般流程是依次将随机噪声样本去噪为数据分布中的样本。考虑一系列的噪声水平σmax &gt;… &gt; σ0 = 0和相应的噪声图像分布p(x,σ)定义为向数据中添加具有方差σ^2的高斯噪声分布。对于足够大的σmax，噪声几乎完全掩盖了数据，p(x,σmax)实际上与高斯噪声无法区分。因此，我们可以对初始噪声图像xT ∼ N (0,σ2max)进行采样，并依次对其进行去噪，以便在每一步中xi ∼ p(x,σi)。这个采样链的端点x0根据数据分布。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-11-25T19:59:40+08:00">
    <meta property="article:modified_time" content="2023-11-25T19:59:40+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程学习者" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程学习者</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AIGC原理：扩散模型diffusion综述一：面向视觉计算的扩散模型研究进展</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>论文地址：<a href="https://arxiv.org/pdf/2310.07204.pdf" rel="nofollow">State of the Art on Diffusion Models for Visual Computing</a><br> 👉 贴一幅SGM（Score-based Generative Model）的原因是宋飏博士将他2019年提出的SMLD模型和2020年Jonathan Ho提出的DDPM采用SDE进行一统这两大极为相似的生成式模型。殊途同归，基于概率的扩散模型DDPM和基于分数的扩散模型SMLD都是通过利用Unet训练一个通过不同时间步控制的不同噪声图片的噪声预测器、分数预测器，最终通过DDPM的采样公式或者退火的郎之万动力学采样公式进行生成图片。<br> <img src="https://images2.imgbox.com/46/57/dsbFXgwk_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/de/3a/MlbKuwS0_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/30/48/wthqD0KH_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a8/3b/lR6IlOJn_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="fire__9"></a>🔥 摘要</h3> 
<p>  由于生成式AI的出现，计算机视觉领域正在迅速发展，它为图像、视频和3D场景的生成、编辑和重建解锁了前所未有的能力。在这些领域中，扩散模型diffusion是首选的生成式AI架构。仅在去年，关于基于扩散的工具和应用的文献就呈指数级增长，每天计算机图形学、计算机视觉和AI社区都有新的作品出现在arXiv上。<strong>这一领域的迅速发展使它很难跟上最近的所有发展</strong>。这份最新报告(STAR)的目标是介绍扩散模型的基本数学概念、流行的稳定扩散模型的实现细节和设计选择，以及概述这些生成式AI工具的重要方面，包括个性化、条件调节、反演等。对快速增长的基于扩散的生成和编辑进行了全面的概述，按生成媒体的类型进行分类，包括2D图像、视频、3D物体、运动和4D场景。讨论了可用的数据集、指标、公开挑战和社会影响。这为研究人员、艺术家和从业人员等探索这个领域提供了一个直观的起点。</p> 
<h3><a id="1__12"></a>1. 简介</h3> 
<p>  几十年来，计算机图形学和3D计算机视觉界一直在努力开发准确的物理模型，以计算机生成的图像或从照片中推断场景的物理属性。该方法包括渲染，仿真，几何处理和摄影测量，构成了多个行业的基石，包括视觉效果，游戏，图像和视频处理，计算机辅助设计，虚拟和增强现实，数据可视化，机器人，自动驾驶汽车，遥感等。<br>   生成性(AI)的出现标志着计算机视觉的范式转变。生成式AI工具可以生成和编辑逼真和风格化的像、视频或3D对象，只有文本提示或高级用户指导作为输入。这些工具使计算机视觉中的许多费力的过程自动化，这些过程以前是由具有专业领域知识的专家生成的，现在可以更广泛地使用它们。<br>   生成式AI以前所未有的能力被计算机视觉领域解锁为基础模型，如稳定扩散 [RBL∗22], Imagen [SCS∗22], Midjourney[Mid23]，或DALL-E 2 [Ope23a]和DALL-E 3 [Ope23b]。通过对数亿到数十亿的文本-图像对进行训练，这些模型“看到了一切”，并且估计有数十亿个可学习参数。在经过大量高端图形处理单元(gpu)的训练后，这些模型构成了上述生成式AI工具的基础。通常用于图像、视频和3D对象生成。基于卷积神经网络(CNN)的扩散模型是典型，这些模型以多模态方式与基与transformer的架构的文本编码器相结合，如CLIP [RKH∗21]。<br>   虽然许多2D图像生成基础模型的成功和训练来自资金雄厚的行业参与者，使用了大量的资源，但学术界仍然有空间以主要方式为图形和视觉工具的开发做出贡献。例如，不清楚如何将现有的图像基础模型扩展到其他更高维的领域，如视频和3D场景生成。这在很大程度上是由于缺乏某些类<br> 型的训练数据。例如，网络包含数十亿张2D图像，但高质量和多样化的3D物体或场景的实例要少得多。此外，如何缩放2D图像生成架构以处理更高维度，如视频、3D场景或4D多视图一致场景生成所需要的。当前一个主要的限制，由于网络的规模和迭代性质，扩散模型在推理时相当缓慢，即使网络上存在大量(未标记)视频数据，当前的网络架构往往效率太低，无法在合理的时间或合理的计算资源上进行训练。<br>   尽管仍然存在的挑战，但最近的发展在过去一年中刺激了计算机视觉学者们,使得扩散模型的爆炸式增长(见图1中的代表性示例)。这份最新报告(STAR)的目标是介绍扩散模型的基本原理，结构化地概述许多最近关于扩散模型在视觉计算中的应用的工作，并概述公开的挑战。<br> <img src="https://images2.imgbox.com/48/13/MY6KJwYD_o.png" alt="在这里插入图片描述"><br>   <br>   <br> <strong>STAR的结构如下:<br> 1.简介<br> 2.概述了范围，并向感兴趣的读者介绍了这里没有涉及的密切相关主题的调查;<br> 3.概述了二维扩散的数学基础;<br> 4.讨论了从2D图像向视频、3D和更高维扩散模型转移的挑战;<br> 5.概述了基于扩散的视频合成和编辑的方法;<br> 6.总结了最近的3D对象和场景生成方法;<br> 7.包括对多视图一致视频的4D时空扩散的讨论，人体运动和场景生成(例如，使用参数化人体模型);<br> 8.包括对现有训练数据的简要讨论;<br> 9.用于各种生成内容的评论指标;<br> 10.概述了公开挑战;<br> 11.讨论社会影响和伦理问题;<br> 12.STAR结尾。</strong><br>   </p> 
<h3><a id="2_STAR_37"></a>2. STAR的范围</h3> 
<p>  本文重点介绍了扩散模型在计算机视觉中应用的最新进展。讨论了扩散模型在生成和编辑图像、视频、3D物体或场景以及多视图一致的4D动态场景中的作用。我们首先建立扩散模型的数学基础。这包括应用于2D图像的一般扩散过程的简要介绍。深入研究了这些技术如何实现高维信号的生成建模，对视频、3D和4D数据的扩散模型的主要方法进行了全面的概述。本报告旨在强调利用扩散模型解决图像域以外数据问题的技术。考虑到这一点，我们不会涵盖只适用于2D数据的每一种方法。本文也没有讨论利用扩散模型以外的生成的工作。<br>   其他生成方法，如GANs，与扩散模型密切相关。但是，我们认为它们超出了本报告的范围。我们建议读者参考 [GSW∗21]以获得关于GANs的深入讨论， [BTLLW21,LZW∗23, SPX∗22]以获得其他生成方法的更广泛的回顾，或使用不同的生成模型架构进行多模态图像合成和编辑[ZYW∗23]。最近，基础模型已经变得类似于在互联网规模的数据图像 [RBL∗22]上训练的扩散模型。虽然本报告讨论了利用此类模型的方法，但请参阅 [BHA∗21]以了解自然语言处理、计算机视觉和其他领域中基础模型的介绍和概述。最后但并非最不重要的是，文本到图像(T2I)生成的爆炸性进展导致了大型语言模型(LLM)和扩散模型之间的内在联系;有兴趣的读者可以参考 [ZZL∗23]获取关于LLM的全面调查。<br>   本报告涵盖了发表在《主要计算机视觉、机器学习和计算机图形会议论文集》上的论文，以及在arXiv上发布的预印本(2021- 2023)。本报告的作者根据其与本调查范围的相关性选择了论文，因为我们的目的是对视觉计算背景下扩散模型的快速进展提供一个全面的概述。然而，尽管本报告是特定领域中最先进的方法的列表，但我们不声称完整，并强烈建议读者参考引用的作品以进行深入的讨论和细节。</p> 
<h3><a id="3__42"></a>3. 扩散模型的基础</h3> 
<p>  在本节中，我们简要概述扩散模型的基本原理。介绍了数学基础，以流行的稳定扩散模型为例，讨论了实际实现，然后概述了条件和指导的重要概念，然后讨论了与反演、图像编辑和定制相关的概念。本节涵盖了大量的参考资料，所以我们侧重于给读者一个清晰和高层次的概述，介绍二维图像背景下基于扩散的生成和编辑的最重要的概念。</p> 
<h5><a id="31__44"></a>3.1. 数学基础</h5> 
<p>  假设我们有一个训练样本数据集，其中数据中的每个示例都独立于基础数据分布<strong>Pdata(x)</strong>,我们希望用一个模型拟合到<strong>Pdata(x)</strong>，以便通过从该分布中采样来合成新的图像。<br>   去噪扩散模型推理一般流程是依次将随机噪声样本去噪为数据分布中的样本。考虑一系列的噪声水平σmax &gt;… &gt; σ0 = 0和相应的噪声图像分布p(x,σ)定义为向数据中添加具有方差σ^2的高斯噪声分布。对于足够大的σmax，噪声几乎完全掩盖了数据，p(x,σmax)实际上与高斯噪声无法区分。因此，我们可以对初始噪声图像xT ∼ N (0,σ2max)进行采样，并依次对其进行去噪，以便在每一步中xi ∼ p(x,σi)。这个采样链的端点x0根据数据分布。<br>   然而，与其通过离散的噪声水平集合来考虑去噪，不如将噪声水平视为一个连续的、时间依赖的函数σ(t)(一个常见的选择是σ(t) = t)。噪声图像样本x可以在噪声水平下连续移动，遵循一个轨迹——或者在时间上向前移动，逐渐添加噪声，或者在时间上向后移动，逐渐去除噪声(见图2 (B))。<br> <img src="https://images2.imgbox.com/5d/f3/qAW2UhAt_o.png" alt="在这里插入图片描述"><br>   Song等人 [SSDK∗20]引入了一种随机微分方程(SDE)框架来对这些轨迹进行建模。常微分方程为我们提供了解决初值问题——给定初始状态和描述函数的微分方程，我们可以在不同的时间求解该函数。举个例子，给定一个物体的初始位置和已知的速度，我们可以求解出物体在未来任何时候的位置。正如图2 (A)所描述的，以非常相同的方式，对图像进行噪声处理可以被认为是从图像域选择一个初始图像x0，并在时间上向前求解微分方程;图像去噪可以认为是选择一个初始噪声图像xT ∼ N (0,σ^2max)并在时间上向后求解微分方程。<br> <img src="https://images2.imgbox.com/3d/a0/uLBPZujZ_o.png" alt="在这里插入图片描述"></p> 
<p>  随着时间的推移，带有噪声的图像的逐渐损坏是一个扩散过程，可以由Itô建模随机微分方程(SDE;Eq. 1) [Itô50,Itô51]，其中f(·,t) : Rd → Rd是一个向量值函数，称为漂移系数，g(·) :R → R是一个标量值函数，称为扩散系数，w是标准的维纳过程:<br>                   <strong>dx = f(x,t)dt +g(t)dw.</strong> 实现扩散模型需要选择f和g, [SSDK∗20]已经探索了几个具体的选择。f(x,t) = 0和<span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          g 
         
        
          ( 
         
        
          t 
         
        
          ) 
         
        
          = 
         
         
          
          
            2 
           
          
            σ 
           
          
            ( 
           
          
            t 
           
          
            ) 
           
           
            
            
              d 
             
            
              σ 
             
            
              ( 
             
            
              t 
             
            
              ) 
             
            
              ) 
             
            
            
            
              d 
             
            
              t 
             
            
           
          
         
        
       
         g(t) = \sqrt{2\sigma (t)\frac{d\sigma (t))}{dt}} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">g</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 2.44em; vertical-align: -0.7356em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.7044em;"><span class="svg-align" style="top: -4.4em;"><span class="pstrut" style="height: 4.4em;"></span><span class="mord" style="padding-left: 1em;"><span class="mord">2</span><span class="mord mathnormal" style="margin-right: 0.0359em;">σ</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.427em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="mord mathnormal">t</span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="mord mathnormal" style="margin-right: 0.0359em;">σ</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">))</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span class="" style="top: -3.6644em;"><span class="pstrut" style="height: 4.4em;"></span><span class="hide-tail" style="min-width: 1.02em; height: 2.48em;"> 
            <svg width="400em" height="2.48em" viewbox="0 0 400000 2592" preserveaspectratio="xMinYMin slice"> 
             <path d="M424,2478
c-1.3,-0.7,-38.5,-172,-111.5,-514c-73,-342,-109.8,-513.3,-110.5,-514
c0,-2,-10.7,14.3,-32,49c-4.7,7.3,-9.8,15.7,-15.5,25c-5.7,9.3,-9.8,16,-12.5,20
s-5,7,-5,7c-4,-3.3,-8.3,-7.7,-13,-13s-13,-13,-13,-13s76,-122,76,-122s77,-121,77,-121
s209,968,209,968c0,-2,84.7,-361.7,254,-1079c169.3,-717.3,254.7,-1077.7,256,-1081
l0 -0c4,-6.7,10,-10,18,-10 H400000
v40H1014.6
s-87.3,378.7,-272.6,1166c-185.3,787.3,-279.3,1182.3,-282,1185
c-2,6,-10,9,-24,9
c-8,0,-12,-0.7,-12,-2z M1001 80
h400000v40h-400000z"></path> 
            </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.7356em;"><span class=""></span></span></span></span></span></span></span></span></span></span>的选择产生一个SDE，描述通过添加方差的高斯噪声σ^2(t)来对图像进行噪声处理。这个SDE被称为方差爆炸SDE(等式2)，之所以这样叫是因为方差随着t的增加而不断增加。对图像进行噪声处理可以被认为是选择一个初始干净的图像并及时解决Eq. 2<br> <img src="https://images2.imgbox.com/46/14/CTgWCptE_o.png" alt="在这里插入图片描述"><br>   在Eq. 4中，∇x log p(x;σ(t))被称为分数函数，它是一个指向较高数据似然区域的向量场。为了解决Eq. 4，我们需要用神经网络预测得分函数。值得注意的是，对于最小化L2去噪误差Ey∼pdataEn∼N(0,σ2I)∥D(y+n;σ)−y∥22的去噪函数D，可以很容易地从模型输出中获得分数函数∇x log p(x;σ(t)) =(D(x;σ) − x)/σ2。这意味着通过简单地训练一个去噪图像的模型，我们可以提取评分函数的预测,这被称为去噪分数匹配。上述关系是由 [Vin11]导出的。请注意，通常使用参数φ或θ对神经网络进行参数化，作为噪声预测网络ϵφ，而不是作为去噪器Dθ;但是，每个都可以很容易地从另一个中恢复，如ϵφ(x;σ) = x−Dθ(x;σ)。<br>   为 了 对 图 像 进 行 采 样 ， 我 们 只 需 要 从 一 些 初 始xT ∼N (0,σ^2maxI)开始，我们可以及时向后解决Eq. 4，以到达来自Pdata(x)的样本。然而，就像常微分方程(ODE)的情况一样，只有一小部分有闭合形式的解。幸运的是，作为替代方案，我们可以从数值上近似SDE的解。<br> <img src="https://images2.imgbox.com/7a/69/eEw1A2eG_o.png" alt="$$"><br>   欧拉-丸山法(Alg。1)是一个近似SDEs数值解的算法。它是欧拉方法的简单扩展，欧拉方法是最基本的数值ODE求解器，也是许多人都熟悉的技术。与欧拉方法类似，欧拉-丸山方法通过与轨迹相切的小步来近似轨迹。步长越小，近似精度越高。许多常见扩散模型 [SSDK∗20, SME20, ND21]的采样技术可以看作是欧拉-丸山模型的改进。<br>   通过数值SDE求解器的视角来进行扩散模型的图像合成，可以让我们对不同采样方案有直观的认识，并对某些工作如何提高生成图像的计算效率有一些了解。从经验上观察到，用扩散模型生成高质量图像需要多次(通常数百次)迭代;迭代次数越少，样本质量越差。用数值微分方程求解器的语言来说，很少迭代的低质量结果是截断误差的结果——我们使我们的时间步长越小∆t，我们的数值逼近越准确。出于同样的原因，高阶微分方程求解器 [KAAL22, DVK22]可以减少我们数值逼近中的误差，使我们能够以更高的精度采样，或以更少的网络评估实现相同的质量。<br>   对于任何扩散过程，都存在一个相应的确定性过程，可 以 用ODE描 述 ， 该 过 程 可 以 恢 复 相 同 的 边 缘 概 率 密度P(x,σ)。Song等人 [SSDK∗20]定义了一个描述确定性过程的ODE，并将其命名为概率流ODE(等式5，图2 b):<br> <img src="https://images2.imgbox.com/75/b8/VAY6bObn_o.png" alt="在这里插入图片描述"><br>   这种概率流ODE使确定性图像合成成为可能——而不是采样随机噪声并模拟逆时SDE来合成图像，我们可以替代采样随机噪声并解决反向确定性概率流ODE;这样做可以恢复相同的图像分布。与随机采样不同的是，最终图像由初始噪声图像xt和每次迭代时注入的噪声决定(对应于等式4中的dw术语)，确定性地创建的图像仅由初始噪声定义。<br>   但我们也可以做相反的事情:我们可以绘制任意图像并遵循正向概率流ODE将图像编码为噪声，而不是采样噪声并生成图像。事实上，概率流ODE定义了图像和(噪声)潜之间的双射映射。在ODE求解器中有足够的精度，可以通过正向前求解概率流ODE将图像编码为潜空间，得到一个潜空间xT，并及时向后求解概率流ODE以恢复原始图像。此外，人们可以通过操纵相应的潜空间来编辑图像。例如，潜空间中的插值可能会在图像空间中产生引人注目的插值，缩放潜空间可以影响生成图像的温度 [SSDK∗20]。最近的一些图像编辑工作是建立在图像与潜变量之间确定性映射的前提下，并依靠前向概率流ODE将真实图像映射到扩散模型的潜空间 [HMT∗22,MHA∗23,SSME22]。<br>   确定性采样的另一个优点是，与相应的SDE相比，数值求解器的迭代次数较少[SSDK∗20, SME20]也可以足够准确地求解概率流ODE;因此，确定性采样可以用比随机采样更少的网<br> 络评估来产生高质量的图像，从而加速推理。<br>   尽管有这些优点，但在多次(通常是数百到数千次)去噪迭代时，随机采样仍然经常优于确定性采样，并且图像质量至关重要。直观地说，随机性可以被看作是一种纠正力，它修复了在采样早期所犯的错误。因此，当随机采样器与许多去噪迭代相结合时，通常会根据指标产生最佳评价的图像。<br>   Karras等 人 。 [KAAL22]通 过 将 逆 SDE分 解 为 概 率流ODE(确定性地在噪声水平之间移动样本)和朗之万动力学扩散SDE(通过添加和删除少量噪声，在固定的噪声水平上随机地“搅动”样本)的来说明这一点。在这里，朗之万动力学扩散SDE，即随机性分量，将样本推向边缘分布p(x,σ)，纠正可能在纯确定性环境中遗留下来的误差。在实践中，确定性抽样和随机抽样都有各自的优缺点，最有效的随机性水平取决于任务。</p> 
<h5><a id="32__69"></a>3.2. 使用稳定扩散模型的潜在扩散</h5> 
<p>  与通过单次前向传递生成图像的生成模型(如变分自编码器(VAE)或生成对抗网络(GANs))不同，扩散模型需要递归的前向传递。这种特性在训练过程中施加了更高的计算负担，因为模型必须学习跨多个噪声尺度的去噪。此外，多阶段去噪过程的迭代性延长了推理时间，使扩散模型在计算上比它们的生成模型更低效 [KAAL22]。<br>   由于内存需求过大，使用扩散模型生成高分辨率图像在消费级GPU上通常是不可行的。即使在高端GPU上，对批量大小的限制也会延长训练过程，这对于很大一部分研究界来说是不切实际的。<br>   为了解决这些挑战，Rombach等人 [RBL∗22]引入了在压缩隐空间中训练潜在的扩散模型，而不是直接在图像像素上运行。该方法保留了感知上相关的细节，同时显著降低了计算成本。使用编码器-解码器结构获得压缩图像的隐空间。在各种技术中，VQ-GAN [ERO21]由于其令人印象深刻的压缩能力和保持感知质量，已成为最常见的选择。这些潜在扩散模型(LDMs)由两个阶段的过程组成:用于图像重建的初始自编码器和对潜在空间进行扩散去噪模型，在减少计算需求的情况下实现卓越的性能(见图3)。<font color="red">额外添加潜在扩散模型在音频领域的文生音频模型Tango.</font><br> <img src="https://images2.imgbox.com/59/d7/FCBWGWDQ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/90/ec/2xFcDM9q_o.png" alt="在这里插入图片描述"><br>   Rombach等人提出的架构[RBL∗22]建立在U-Net框架[RFB15, HJA20,JMPTdCM20]上。它在U-Net的不同阶段注入了注意力机制 [VSP∗17]，特别是<strong>self-attention</strong>和<strong>cross-attention</strong>。<br>   在自注意力块中，从中间U-Net输出派生的特征被投影到查询Q、键K和值V。该块的输出如下:<br> <img src="https://images2.imgbox.com/16/b9/QuWPbjT6_o.png" alt="在这里插入图片描述"></p> 
<p>在这里，注意力机制通过d维Q和V投影矩阵之间的上下文信息<br> <img src="https://images2.imgbox.com/36/5f/O4MlECuT_o.png" alt="在这里插入图片描述"></p> 
<p>交叉注意力块的运作方式类似，通过注入条件信号(如文本提示)来实现受控生成(见3.3节)。<br>    <strong>Retrieval-augmentation Mechanism (RDM).</strong> 为 了 进 一 步 优 化 计 算 效 率 ，Blattmann等 人[BRO∗22]引入了一种检索增强机制(RDM)，在生成过程中从外部数据库中获取相关的图像块。这些补丁是根据预训练自编码器的潜编码选择的，并进行简单的增强。由于检索到的图像块的细节不再需要保存在模型参数中，这种机制导致了更精简的去噪模型，从而加速了训练和推理，尽管代价是增加了计算复杂性和对训练良好的自动编码器的依赖性。</p> 
<h5><a id="33__86"></a>3.3. 调节和指导</h5> 
<p><strong>Conditioning.</strong> 生成模型最重要的属性是通过用户定义的条件控制生成的能力。这些条件包括文本 [RAY∗16]、语义地图[PLWZ19]、草图 [VACO23]、条件的多模态组合 [ZYW∗<br> 23]和其他图像到图像的翻译任务 [IZZE18, SCC∗22]。形式上，我们不是从无条件分布中采样数据p(x)，而是从条件分布中采样p(x|c)给出一些条件信号c。<br>    为了适应各种调节模式，一套灵活的调节机制已被开发用于扩散模型。这些方法中最简单的是连接 [SCC∗22]，其中条件直接与中间去噪目标连接，并通过分数估计器作为输入。在模型架构的不同阶段，连接可以与扩散模型输入一起执行。它也适用于几乎所有条件反射模式。最值得注意的是，连接 [SCC∗22]处理各种图像到图像的转换任务，如使用连接调节进行修复，着色，取消裁剪和图像恢复。<br>    另 一 种 有 效 的 方 法 是 通 过Cross-attention注入调节信号。Rombach等人 [RBL∗22]修改了U-Net架构 [RFB15]，用于交叉注意力机制的调节控制。为了控制图像合成，条件信号c(例如指导文本提示)首先由特定于领域的编码器τ预处理到中间投影T(\c)。然后，通过Cross-attention [VSP∗17]，通过Eq.7，将投影的条件信号注入到去噪U-Net的中间层中<br> <img src="https://images2.imgbox.com/0f/9e/eJqiRqP5_o.png" alt="在这里插入图片描述"><br>    其中WQ,WK,WV是可学习的投影矩阵，ϕ(zt)表示去噪UNet的中间结果;详情见图3。直观地说，Q是中间U-Net层激活的投影，而K和V是通过给定条件的投影得到。<br>    在推理过程中，可以应用其他技术以不同的模态对网络进行条件设置，包括草图 [VACO23]和空间布局 [ZA23,MWX∗23]。其中，以流行的ControlNet <a href="%E8%A7%81%E5%9B%BE4" rel="nofollow">ZA23</a>为例的适配器方法，通过将新的模块层嵌入到现有网络架构中，提供了一种有效而灵活的路径，可以在不改变预训练扩散模型的情况下添加新的条件模态。提出了一个backbone，通过添加辅助网络模块来学习大型预训练扩散模型的不同控制手段。新网络模块通过从预训练网络中复制编码层来初始化，并通过“零卷积”连接到原始模型，这是一种将层参数初始化为零的机制，确保在微调过程中没有学习到有害的噪声。辅助网络在给定的一组条件输出对上进行微调，而原始网络层保持不变，如图4所示。<br> <img src="https://images2.imgbox.com/a4/b7/UJdNOOnU_o.png" alt="在这里插入图片描述"><br>    ControlNet [ZA23]通过复制网络块并通过零卷积连接它们来修改现有的网络架构。辅助模块进行些调节c，允许模型学习额外的控制手段。<br> <strong>Guidance.</strong> 虽然调节提供了对采样分布的一定程度的控制，但在微调模型中调节信号的强度方面存在不足。引导作为一种替代方法出现，通常应用于训练后，以更精确地控制扩散轨迹。<br>    Dhariwal等人[DN21]观察到辅助分类器可以引导无条件生成模型。这种称为分类器指导的技术，通过合并预训练分类器模型po(c|x)的对数似然梯度(该模型从给定图像x估计c)来改变原始扩散分数。利用贝叶斯定理，p(x|c)的分数估计器如下:<br> <img src="https://images2.imgbox.com/3f/b5/t3QyONGI_o.png" alt="在这里插入图片描述"></p> 
<p>   其中w是控制引导强度的可调参数。尽管具有多功能性，但分类器指导也有局限性，例如需要抗噪辅助分类器，以及由于xt中的无关信息而导致梯度定义不明确的风险。<br>    为了规避分类器指导的局限性，Ho和Salimans引入了无分类器指导[HS22]。这种方法直接改变了训练方案，利用单个神经网络来表示无条件模型和条件模型。这些模型是联合训练的，无条件模型由空牌c= 0参数化。然后，可以使用前面介绍的模型进行如下采样指导量表w:<br> <img src="https://images2.imgbox.com/64/53/5EISpk3G_o.png" alt="在这里插入图片描述"><br>    控制条件信号的强度导致多样性和样本质量之间的权衡[HS22]。随着指导尺度w的增加，结果样本的多样性降低，以换取更高的样本质量。然而，人们经常观察到，使用无分类器指导训练的模型往往会在很低或很高的指导尺度下生成低质量的样本。例如，当仅使用无条件分数估计器(w = −1)进行采样时，稳定扩散 [RBL∗22]生成灰度图像，并在较高的指导值(w &gt; 10)下输出具有饱和伪影的图像。</p> 
<h5><a id="34__105"></a>3.4. 编辑、倒置和定制</h5> 
<p>   预训练扩散模型本质上提供了一个富有表现力的生成先验，可以利用它让普通用户在没有任何像素级制作技能经验的情况下执行各种图像处理任务。许多最近的工作研究了使用文本到图像扩散模型进行编辑，以及生成个性化图像，也称为定制。本节调查了这两类的主要工作，以及一个关键的技术组件，反转，它经常被用作编辑的基石。<br> **Editing.**由于其固有的渐进式和基于注意力的架构，扩散模型为细粒度图像编辑提供了一个独特的平台，通过便于调整各种网络阶段和组件来操纵空间布局和视觉美学。该领域的研究轨迹旨在增强编辑可控性和灵活性，同时确保直观的用户界面。一个常见的用例涉及在保留图像空间配置的同时改变图像的视觉属性。在这种情况下，SDEdit [MHS∗21]提出了一种直接的方法，该方法将校准的噪声水平引入到图像中，产生部分噪声图像，然后用新的条件信号作为指导进行反向扩散过程。 [KKY22]进一步扩展了这种基本方法，通过直接修改文本提示来操纵全局特征，而通过在扩散过程中合并辅助掩<br> 码来完成本地化编辑 [ALF22, NDR∗21]。类似地，一个额外的引导信号(即，任何梯度函数，就像分类器在分类器指导中一样)可以用来修改采样轨迹以执行操纵性编辑，如改变对象外观或重新安排场景中的内容 [EJP∗23]。另一种编辑策略涉及约束来自不同生成过程的特定特征图。例如，Prompt-toPrompt [HMT∗22]采用固定的交叉注意力层来选择性地修改与特定文本线索相对应的图像区域。即插即用 [TGBD23]探索了空间特征和自注意力映射的注入，以保持图像的整体结构完整性。[CWQ∗23]主张利用自注意力机制来实现一致的、非刚性的图像编辑，而不需要手动调整。此外，文本提示本身是编辑质量的关键决定因素。image [KZL∗23]通过对文本嵌入的优化与模型微调来细化文本提示，从而实现多样化的、空间非刚性的图像编辑。Delta去噪分数 [HACO23]巧妙地将文本到图像(T2I)扩散模型的生成先验作为优化框架中的损失项，以指导基于文本指令的图像转换。InstructPix2Pix<br> [BHE23]通过在生成的与编辑指令对齐的图像对数据集上微调T2I模型，将用户的文本输入从描述性的目标图像注释简化为更直观的编辑指令，该数据集是使用提示到提示和大型语言模型的组合创建的。 [PKSZ∗23]通过引入一种从样本图像对中发现编辑方向的自动机制，完全避免了对文本提示的需要。<br> 根据GAN文献中的类似工作 [PTL∗23]。<br>   许多基于扩散的方法旨在执行稀疏用户注释对应关系驱动的编辑，其中对象的外观和身份被保留，只改变其布局或方向 [MWS∗23,SXP∗23]。<br> <strong>Inversion.</strong> 许多通过生成模型编辑现有(即真实)图像的方法通常涉及“反转”任务，该任务识别特定的输入潜代码或潜序列，当将其输入模型时，将重现给定的图像。反转允许在潜空间中进行操作，这使得可以使用模型已经学习的生成先验。在扩散的背景下，DDIM反演 [SME20]是一种基本的技术，它向给定的图像添加小的噪声增量以近似相应的输入噪声。当使用带有该噪声的DDIM运行反向扩散时，原始图像将被重现。在文本到图像扩散的情况下，当提供特定的文本-图像配对时，DDIM反演方法往往会积累小误差，特别是在无分类器指导下 [HS22]。空文本反转 [MHA∗23]通过优化每个时间步的输入空文本嵌入来补偿时间步长漂移。敕书 [WGN23]利用两个耦合的噪声矢量实现了精确的DDIM反演。 [WDlT22]展示了一种DDPM-反演方法，在DDPM采样框架内恢复噪声向量以进行准确的图像重建。<br>   除了图像到噪声的反演，在文本到图像模型的背景下，“文本反演” [GAA∗22]提供了一个将图像转换为token嵌入的框架。最初的工作提出使用优化将出现在几个图像中的概念转换为单个token。后续工作还演示了使用编码器将单个图像反转为token [GAA∗23b]，或将概念转换为每个层token的序列，以改进概念的重建 [VCCOA23]。<br> <strong>Customization.</strong> 最近的工作广泛探索了T2I扩散模型的定制，即自适应预训练扩散模型，为特定的人或物体生成更好的输出。开创性工作DreamBooth [RLJ∗22]通过优化网络权重来实现这一点，以通过唯一的令牌来表示一组图像中显示的主题。一系列后续工作专注于微调网络的特定部分。CustomDiffusion[KZZ∗23]仅修改交叉注意力层，SVDiff [HLZ∗23]细化权重的奇异值，LoRA [HSW∗<br> 21]目标优化权重残差的低秩近似[HSW∗21], StyleDrop [SRL∗23]采用适配器调优 [HGJ∗19]对选<br> 定的一组适配器权重进行微调以进行样式定制。除了文本到图像生成之外，类似的技术也被应用于其他问题陈述，例如图像修复或修复 [TRC∗23]。<br>   另 一 个 研 究 方 向 是 加 速 定 制 过 程 。 [GAA∗23a]和[WZJ∗23]采用编码器来确定初始文本嵌入，随后对其进行微调，以提高主题保真度。 [RLJ∗23]直接预测针对特定主题定制的低秩网络残差。SuTI [CHL∗23]最初使用DreamBooth制作的图像及其重新上下文化的对应对象构建了一个全面的配对数据集，并使用它来训练一个可以以前馈方式执行个性化图像生成的网络。InstantBooth [SXLJ23]和Taming Encoder [JZC∗23]为扩散模型引入了条件分支，允许使用最小的图像集甚至只有一个图像集进行条件调节，促进各种风格的个性化输出的生成。Break-A-Scene [AAF∗23]自定义模型以支持单个图像中描述的几个主题，而FastComposer [XYF∗23]利用图像编码器将特定主题的嵌入项目用于多主题生成。</p> 
<h3><a id="4__119"></a>4. 超越图像的挑战</h3> 
<p>  扩散模型在图像处理领域获得了极大的关注和成功，这是由于各种因素的共同作用，使其特别适合于这个领域。其有效性的主要原因之一在于为图像处理，特别是去噪领域量身定制的网络架构的成熟。2D图像域的扩散模型利用了这些进展，将定义良好的构建块，如卷积层、注意力机制和UNet结构作为其骨干。transformer的进展[VSP∗17]和大型语言模型 [DCLT18]通过实现自然语言提示的近似可控性，进一步增强了这些模型，由图像与文本描述的配对促进 [RKH∗21]。<br>   此外，无处不在的手机和社会规范使2D图像的捕获、存储和共享变得民主化，导致免费提供的图像几乎无限。总而言之，到2010年底，2D扩散模型成功的所有必要因素都已到位:一个定义良好的数学框架，灵活的函数逼近器，能够学习富有表现力的图像变换，以及丰富的训练数据供应。<br>   尽管2D图像合成得益于技术进步和可用数据的幸运巧合，但对于更高维的信号来说情况并非如此。合成更高维内容(如视频和3D内容)的任务大大比2D图像合成困难，受本节进一步描述的许多其他问题的限制。<br> <strong>Models.</strong> 处理高维数据的网络结构仍然是一个开放的问题。与可以使用离散像素值有效表示和处理的图像不同，高维数据通常需要更复杂的表示。处理长程信息流的需求加剧了这种复杂性，这对理解视频中的时间动态或3D结构中的空间关系至关重要。到目前为止，对于一种可以作为这些领域中扩散模型的可靠和可扩展主干的网络架构还没有达成共识。与处理高维数据相关的计算和内存成本进一步使问题复杂化，使找到一个高效且具有表现力的解决方案具有挑战性。<br> <strong>Data.</strong> 数据的可用性和质量是另一个重大挑战。对于三维结构，创建单个三维模型的过程涉及多个步骤，包括扫描、处理和重建，每个步骤都需要专门的专业知识和资源。这使得数据采集过程既耗时又昂贵。在视频的情况下，尽管原始数据可能很丰富，但注释这些数据，特别是捕捉运动和时间依赖<br> 性，远非微不足道。高质量标注数据的缺乏阻碍了高维域鲁棒和可泛化扩散模型的训练。</p> 
<h3><a id="5__126"></a>5. 视频生成与编辑</h3> 
<p>  尽管图像扩散模型取得了巨大进展，T2I生成也取得了显著突破，但由于两个主要挑战，将这一进展扩展到视频领域仍处于初期阶段.<br>   首先，由于动态世界的复杂性和多样性，从视频中学习需要的数据比图像多几个数量级。虽然在线视频非常丰富，但策划高质量的视频数据集仍然是一项艰巨的任务，通常涉及大量的工程工作，并需要专用的自动策划工具。<br>   另一个重大挑战来自原始视频数据的高维性(例如，具有30fps的两分钟视频比单帧包含3600 ×多像素)。这使得将二维架构扩展到时域非常具有挑战性，并且计算成本很高。接下来讨论在视频生成的扩散模型的背景下如何解决这些挑战。</p> 
<h5><a id="51__130"></a>5.1. 无条件和有文字条件的视频扩散</h5> 
<p>  已有大量研究工作。Ho等人 [HSG∗22]引入了第一个视频扩散模型(VDM)，将2D U-Net骨干扩展到时域。这是通过分解空间和时间模块实现的，使更有效的计算以及对单个图像、视频和文本的联合训练成为可能。这种方法已经被Imagen Video [HCS∗22]扩展，这是一个具有110亿参数的级联文本到视频(T2V)模型，包括一个低时空分辨率基础模型，然后是多个级联超分辨率模型，提高了空间分辨率和有效帧率。Imagen Video使用大量高质量视频语料库和相应的字幕以及大量文本-图像数据集从头开始训练。<br>   为了将学习到的图像先验用于视频生成，Make-A-Video[SPH∗22]在预训练的T2I模型上构建了他们的框架，通过在现有的T2I中添加时空卷积和注意力层将其扩展到视频模型，其次是空间和时间超分辨率模型。这种方法的一个关键特性是每个组件都可以单独训练:T2I模型在图像-文本对上进行预训练，而整个T2V在大规模的未标记视频语料库上进行微调，从而绕过了对视频-字幕配对训练数据的需求。膨胀和微调用于视频生成的T2I模型的有效性也在基于自回归transformer的模型中得到了证明(例如， [HDZ∗22])。<br> <img src="https://images2.imgbox.com/27/41/sKGq1S0l_o.png" alt="在这里插入图片描述"><br>   这两个组件——使用可分解/可分离的时空模块(图5)和建立在预训练的文本到图像模型之上——也被用于将图像潜在扩散模型(LDM)，例如，稳定扩散 [RBL∗22]扩展到视频，即在低维空间(例如 [BRL∗23,LCW∗23,ZWY∗22,WYC∗23])中学习视频分布。同样，与普通视频潜模型(例如， [HYZ∗22,YSKS23])相比，利用预训练的图像模型可以有效地进行训练(使用较少的数据)，同时利用T2I ldm学习到的丰富的2D先验。这种方法的另一个优点是能够将学习到的运动模块迁移到由同样的基础，T2I。例如，插入T2I模型的个性化调整版本，从而以特定风格合成视频或描述特定对象 [BRL∗<br> 23,GYR∗23]。<br>   如图5所示，分解的时空模块在时间上通过一维卷积扩展了卷积块，也将自注意力块扩展到模型动力学。将自注意力“膨胀”到多个帧(参见3节)的一种常见方法，即扩展或跨帧注意力，将自注意力扩展到视频的所有帧或子集<br> <img src="https://images2.imgbox.com/dc/14/xow8WvWy_o.png" alt="在这里插入图片描述"><br>   其中Qi,Ki,Vi是帧i = {1…I}的查询、键和值。这种膨胀机制在本节讨论的许多方法中很普遍。虽然它促进了时间一致性，但它不能保证时间一致性。<br>   其他视频生成方法包括MCVD [VJMP22]，它通过在先前 生 成 的 帧 上 调 节 新 帧 ， 在3D潜 空 间 中 自 回 归 生 成 视频;VideoFusion [LCZ∗23]，将噪声视频潜分解为所有帧共享的(静态)基噪声和每帧噪声(动态)残差的总和; 生成图像动力学 [LTSH23]，它不是直接预测视频内容，而是学习为图像中的像素生成运动轨迹，这样图像可以动画为具有振荡运动的任意长度的视频。<br> <img src="https://images2.imgbox.com/bb/c5/9dtVo324_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_142"></a>未完待续…</h3>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3edb013d51da3fe46985435d1c059557/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">量子计算 | 解密著名量子算法Shor算法和Grover算法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/57187f5d7c00f3e8b77a0c304ae9c423/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Missing classes detected while running R8. Please add the missing classes or apply additional keep r</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程学习者.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>