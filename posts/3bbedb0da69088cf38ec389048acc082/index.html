<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AI大模型探索之路-训练篇8：大语言模型Transformer库-预训练流程编码体验 - 编程学习者</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcxuexizhe.github.io/posts/3bbedb0da69088cf38ec389048acc082/">
  <meta property="og:site_name" content="编程学习者">
  <meta property="og:title" content="AI大模型探索之路-训练篇8：大语言模型Transformer库-预训练流程编码体验">
  <meta property="og:description" content="系列篇章💥 AI大模型探索之路-训练篇1：大语言模型微调基础认知
AI大模型探索之路-训练篇2：大语言模型预训练基础认知
AI大模型探索之路-训练篇3：大语言模型全景解读
AI大模型探索之路-训练篇4：大语言模型训练数据集概览
AI大模型探索之路-训练篇5：大语言模型预训练数据准备-词元化
AI大模型探索之路-训练篇6：大语言模型预训练数据准备-预处理
AI大模型探索之路-训练篇7：大语言模型Transformer库之HuggingFace介绍
目录 系列篇章💥前言案例场景准备工作1）学术加速2）安装LFS3）下载数据集(原始语料库)4）下载模型到本地 步骤1：导入相关依赖步骤2：获取数据集步骤3：构建数据集步骤4：划分数据集步骤5：创建DataLoader步骤6：创建模型及其优化器步骤7：训练与验证步骤8：模型预测总结 前言 在深入探索Transformer库及其高级组件之前，我们先手工编写一个预训练流程代码。这一过程不仅有助于理解预训练的步骤和复杂性，而且能让您体会到后续引入高级组件所带来的开发便利性。通过实践，我们将构建一个情感分类模型，该模型能够接收文本评价并预测其是正面还是负面的情感倾向。
案例场景 想象一下，我们有一个原始数据集，其中包含了酒店顾客的评价文本。我们的目标是训练一个模型，当输入类似“昨天我在酒店睡觉发现被子有一股霉味。”的评价时，模型能够预测出“差评”。
准备工作 本次仍是采用云服务器autodl调试运行
1）学术加速 source /etc/network_turbo 2）安装LFS 从 Hugging Face Hub 下载模型需要先安装Git LFS
安装git-lfs是为了确保从Hugging Face拉取模型时能够高效且完整地下载所有相关文件，尤其是那些大型的模型文件。
Ubuntu系统操作命令：
curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash
sudo apt-get install git-lfs
Centos命令参考：
curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash sudo yum install git-lfs 执行：git lfs install
3）下载数据集(原始语料库) 创建一个pretrains目录，将数据集下载到这个目录，下载到本地后可以提高执行效率
git clone https://huggingface.co/datasets/dirtycomputer/ChnSentiCorp_htl_all
注意！重要！！：下载后请记得和Huggingface上的文件对比，尤其是大文件，确保下载完整
4）下载模型到本地 git clone https://huggingface.co/hfl/rbt3
下载到本地后,从本地加载执行效率更高
注意！重要！！：下载后请记得和Huggingface上的文件对比，尤其是大文件，确保下载完整
步骤1：导入相关依赖 首先，我们需要设置Python环境，并导入必要的库
from transformers import AutoTokenizer, AutoModelForSequenceClassification 步骤2：获取数据集 获取数据集是预训练中关键一步。我们使用前面从Huggingface下载的包含酒店评价的文本数据集。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-30T19:09:04+08:00">
    <meta property="article:modified_time" content="2024-04-30T19:09:04+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程学习者" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程学习者</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AI大模型探索之路-训练篇8：大语言模型Transformer库-预训练流程编码体验</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_2"></a>系列篇章💥</h2> 
<p><a href="https://xundaomalu.blog.csdn.net/article/details/138107946" rel="nofollow">AI大模型探索之路-训练篇1：大语言模型微调基础认知</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138143923" rel="nofollow">AI大模型探索之路-训练篇2：大语言模型预训练基础认知</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138161057" rel="nofollow">AI大模型探索之路-训练篇3：大语言模型全景解读</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138205204" rel="nofollow">AI大模型探索之路-训练篇4：大语言模型训练数据集概览</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138225299" rel="nofollow">AI大模型探索之路-训练篇5：大语言模型预训练数据准备-词元化</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138267915" rel="nofollow">AI大模型探索之路-训练篇6：大语言模型预训练数据准备-预处理</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138294519" rel="nofollow">AI大模型探索之路-训练篇7：大语言模型Transformer库之HuggingFace介绍</a></p> 
<hr> 
<p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#_2" rel="nofollow">系列篇章💥</a></li><li><a href="#_16" rel="nofollow">前言</a></li><li><a href="#_19" rel="nofollow">案例场景</a></li><li><a href="#_24" rel="nofollow">准备工作</a></li><li><ul><li><a href="#1_26" rel="nofollow">1）学术加速</a></li><li><a href="#2LFS_33" rel="nofollow">2）安装LFS</a></li><li><a href="#3_54" rel="nofollow">3）下载数据集(原始语料库)</a></li><li><a href="#4_60" rel="nofollow">4）下载模型到本地</a></li></ul> 
  </li><li><a href="#1_68" rel="nofollow">步骤1：导入相关依赖</a></li><li><a href="#2_75" rel="nofollow">步骤2：获取数据集</a></li><li><a href="#3_90" rel="nofollow">步骤3：构建数据集</a></li><li><a href="#4_121" rel="nofollow">步骤4：划分数据集</a></li><li><a href="#5DataLoader_131" rel="nofollow">步骤5：创建DataLoader</a></li><li><a href="#6_165" rel="nofollow">步骤6：创建模型及其优化器</a></li><li><a href="#7_192" rel="nofollow">步骤7：训练与验证</a></li><li><a href="#8_247" rel="nofollow">步骤8：模型预测</a></li><li><a href="#_278" rel="nofollow">总结</a></li></ul> 
</div> 
<p></p> 
<hr> 
<h2><a id="_16"></a>前言</h2> 
<p>在深入探索Transformer库及其高级组件之前，我们先手工编写一个预训练流程代码。这一过程不仅有助于理解预训练的步骤和复杂性，而且能让您体会到后续引入高级组件所带来的开发便利性。通过实践，我们将构建一个情感分类模型，该模型能够接收文本评价并预测其是正面还是负面的情感倾向。</p> 
<h2><a id="_19"></a>案例场景</h2> 
<p>想象一下，我们有一个原始数据集，其中包含了酒店顾客的评价文本。我们的目标是训练一个模型，当输入类似“昨天我在酒店睡觉发现被子有一股霉味。”的评价时，模型能够预测出“差评”。<br> <img src="https://images2.imgbox.com/1d/70/sFe3lKEW_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_24"></a>准备工作</h2> 
<p>本次仍是采用云服务器<a href="https://www.autodl.com" rel="nofollow">autodl</a>调试运行</p> 
<h3><a id="1_26"></a>1）学术加速</h3> 
<pre><code class="prism language-bash"><span class="token builtin class-name">source</span> /etc/network_turbo
</code></pre> 
<p><img src="https://images2.imgbox.com/5f/47/H5KWxKrG_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="2LFS_33"></a>2）安装LFS</h3> 
<p>从 Hugging Face Hub 下载模型需要先安装Git LFS<br> 安装git-lfs是为了确保从Hugging Face拉取模型时能够高效且完整地下载所有相关文件，尤其是那些大型的模型文件。<br> Ubuntu系统操作命令：<br> curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash<br> <img src="https://images2.imgbox.com/70/fe/ZIEuS6qD_o.png" alt="在这里插入图片描述"></p> 
<p>sudo apt-get install git-lfs<br> <img src="https://images2.imgbox.com/75/b0/wVY2v2tl_o.png" alt="在这里插入图片描述"></p> 
<p>Centos命令参考：</p> 
<pre><code class="prism language-bash"><span class="token function">curl</span> <span class="token parameter variable">-s</span> https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh <span class="token operator">|</span> <span class="token function">sudo</span> <span class="token function">bash</span>
<span class="token function">sudo</span> yum <span class="token function">install</span> git-lfs
</code></pre> 
<p>执行：git lfs install<br> <img src="https://images2.imgbox.com/11/0b/clOXDQwL_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="3_54"></a>3）下载数据集(原始语料库)</h3> 
<p>创建一个pretrains目录，将数据集下载到这个目录，下载到本地后可以提高执行效率<br> git clone https://huggingface.co/datasets/dirtycomputer/ChnSentiCorp_htl_all<br> <img src="https://images2.imgbox.com/c4/e6/xxHeLRgM_o.png" alt="在这里插入图片描述"></p> 
<p><font color="red">注意！重要！！</font>：下载后请记得和Huggingface上的文件对比，尤其是大文件，确保下载完整</p> 
<h3><a id="4_60"></a>4）下载模型到本地</h3> 
<p>git clone https://huggingface.co/hfl/rbt3<br> 下载到本地后,从本地加载执行效率更高<br> <img src="https://images2.imgbox.com/02/7a/evpQxe4p_o.png" alt="在这里插入图片描述"><br> <font color="red">注意！重要</font>！！：下载后请记得和Huggingface上的文件对比，尤其是大文件，确保下载完整</p> 
<h2><a id="1_68"></a>步骤1：导入相关依赖</h2> 
<p>首先，我们需要设置Python环境，并导入必要的库</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForSequenceClassification
</code></pre> 
<h2><a id="2_75"></a>步骤2：获取数据集</h2> 
<p>获取数据集是预训练中关键一步。我们使用前面从Huggingface下载的包含酒店评价的文本数据集。<br> 1）加载本地的数据集，查看读取内容</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">"/root/pretrains/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv"</span><span class="token punctuation">)</span>
data<span class="token punctuation">.</span>dropna<span class="token punctuation">(</span><span class="token punctuation">)</span>
data
</code></pre> 
<p>执行输出如下：<br> <img src="https://images2.imgbox.com/f5/d7/U4dv0t0r_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="3_90"></a>步骤3：构建数据集</h2> 
<p>创建一个自定义的数据集类，它将负责读取原始数据，可以执行必要的预处理步骤（例如清洗、分词、向量化），并将数据划分为训练集和验证集。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset

<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd

<span class="token keyword">class</span> <span class="token class-name">MyDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">"/root/pretrains/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv"</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> self<span class="token punctuation">.</span>data<span class="token punctuation">.</span>dropna<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>index<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>data<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"review"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>data<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span>
    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">)</span> 

dataset <span class="token operator">=</span> MyDataset<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>dataset<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token punctuation">(</span><span class="token string">'距离川沙公路较近,但是公交指示不对,如果是"蔡陆线"的话,会非常麻烦.建议用别的路线.房间较为简单.'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
<span class="token punctuation">(</span><span class="token string">'商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
<span class="token punctuation">(</span><span class="token string">'早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
<span class="token punctuation">(</span><span class="token string">'宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小，但加上低价位因素，还是无超所值的；环境不错，就在小胡同内，安静整洁，暖气好足-_-||。。。呵还有一大优势就是从宾馆出发，步行不到十分钟就可以到梅兰芳故居等等，京味小胡同，北海距离好近呢。总之，不错。推荐给节约消费的自助游朋友~比较划算，附近特色小吃很多~'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
<span class="token punctuation">(</span><span class="token string">'CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

</code></pre> 
<h2><a id="4_121"></a>步骤4：划分数据集</h2> 
<p>对数据集进行划分，语料库中90%作为预训练数据，10%作为验证数据；这确保了模型在未见过的数据上进行验证和测试。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span>  random_split

trainset<span class="token punctuation">,</span> validset <span class="token operator">=</span> random_split<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span>lengths<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.9</span><span class="token punctuation">,</span><span class="token number">0.1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token builtin">len</span><span class="token punctuation">(</span>trainset<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>validset<span class="token punctuation">)</span>
</code></pre> 
<p>输出：(6989, 776)</p> 
<h2><a id="5DataLoader_131"></a>步骤5：创建DataLoader</h2> 
<p>1）加载数据集<br> 利用分词器进行数据加载（即将文本数据转化为机器能识别的数字序列矩阵）<br> 为了高效地加载数据，采用批量的方式加载预训练数据和校验数据，加载时最大长度为128，多了会进行截取，少了会自动补0</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader

tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"/root/pretrains/rbt3"</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">collate_func</span><span class="token punctuation">(</span>batch<span class="token punctuation">)</span><span class="token punctuation">:</span>
    texts<span class="token punctuation">,</span>labels<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> item <span class="token keyword">in</span> batch<span class="token punctuation">:</span>
        texts<span class="token punctuation">.</span>append<span class="token punctuation">(</span>item<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span>item<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token comment">## return_tensors="pt" 返回的是pytorch tensor类型。</span>
        <span class="token comment">## 吃葡萄不吐葡萄皮</span>
        <span class="token comment">## 不吃葡萄到吐葡萄皮</span>
    inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>texts<span class="token punctuation">,</span>max_length<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">"max_length"</span><span class="token punctuation">,</span>truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span>
    inputs<span class="token punctuation">[</span><span class="token string">"labels"</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>labels<span class="token punctuation">)</span>
    <span class="token keyword">return</span> inputs
<span class="token comment">## dataloader中设置shuffle值为True，表示每次加载的数据都是随机的，将输入数据的顺序打乱。shuffle值为False，</span>
<span class="token comment">## 表示输入数据顺序固定。</span>

trainloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>trainset<span class="token punctuation">,</span>batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>collate_fn<span class="token operator">=</span>collate_func<span class="token punctuation">)</span>
validloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>validset<span class="token punctuation">,</span>batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>collate_fn<span class="token operator">=</span>collate_func<span class="token punctuation">)</span>

<span class="token builtin">next</span><span class="token punctuation">(</span><span class="token builtin">enumerate</span><span class="token punctuation">(</span>validloader<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
</code></pre> 
<p>输出如下：（下面tensor就是转化后的序列矩阵）<br> <img src="https://images2.imgbox.com/b7/9c/GhHtxMvt_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="6_165"></a>步骤6：创建模型及其优化器</h2> 
<p>根据本地下载的模型地址，创建模型对象<br> 基于Transformer架构，定义一个情感分类模型。选择合适的优化器（如AdamW或RMSprop）以调整模型权重，从而最小化损失函数。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">import</span> Adam

<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForSequenceClassification

model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"/root/pretrains/rbt3"</span><span class="token punctuation">)</span>

<span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    model <span class="token operator">=</span> model<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
当我们训练一个机器学习模型时，我们需要选择一个优化算法来帮助我们找到模型参数的最佳值。这个优化算法就是优化器（optimizer）。

在这行代码中，我们选择了一种叫做Adam的优化算法作为我们的优化器。Adam算法是一种常用的优化算法，
它根据每个参数的梯度（即参数的变化率）和学习率（lr）来更新参数的值。

"model.parameters()"表示我们要优化的是模型的参数。模型的参数是模型中需要学习的权重和偏置等变量。

"lr=2e-5"表示学习率的值被设置为2e-5（即0.00002）。学习率是控制模型在每次迭代中更新参数的步长。较大的学习率可能导致模型无法收敛，
而较小的学习率可能需要更长的训练时间
"""</span>
optimizer <span class="token operator">=</span> Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">2e-5</span><span class="token punctuation">)</span>
</code></pre> 
<h2><a id="7_192"></a>步骤7：训练与验证</h2> 
<pre><code class="prism language-python">定义一个训练和评估的函数
设定训练循环，包括前向传播、计算损失、反向传播和权重更新。同时，定期在验证集上检查模型性能，以监控过拟合情况并及时停止训练。
<span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment">## 将模型设置为评估模式</span>
    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    acc_num<span class="token operator">=</span><span class="token number">0</span>
    <span class="token comment">#将训练模型转化为推理模型，模型将使用转换后的推理模式进行评估</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>inference_mode<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> batch <span class="token keyword">in</span> validloader<span class="token punctuation">:</span>
            <span class="token comment">## 检查是否有可用的GPU，如果有，则将数据批次转移到GPU上进行加速</span>
            <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                batch <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>k<span class="token punctuation">:</span> v<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> k<span class="token punctuation">,</span>v <span class="token keyword">in</span> batch<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
            <span class="token comment">##对数据批次进行前向传播，得到模型的输出</span>
            output <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>batch<span class="token punctuation">)</span>
            <span class="token comment">## 对模型输出进行预测，通过torch.argmax选择概率最高的类别。</span>
            pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>output<span class="token punctuation">.</span>logits<span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment">## 计算正确预测的数量，将预测值与标签进行比较，并使用.float()将比较结果转换为浮点数，使用.sum()进行求和操作</span>
            acc_num <span class="token operator">+=</span> <span class="token punctuation">(</span>pred<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> batch<span class="token punctuation">[</span><span class="token string">"labels"</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment">## 返回正确预测数量与验证集样本数量的比值，这表示模型在验证集上的准确率</span>
    <span class="token keyword">return</span> acc_num <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>validset<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>epoch<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>log_sep<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    global_step <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">for</span> ep <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">## 开启训练模式</span>
        model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> batch <span class="token keyword">in</span> trainloader<span class="token punctuation">:</span>
            <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                batch <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>k<span class="token punctuation">:</span> v<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> batch<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
            <span class="token comment">## 梯度归0</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment">## 对数据批次进行前向传播，得到模型的输出</span>
            output<span class="token operator">=</span>model<span class="token punctuation">(</span><span class="token operator">**</span>batch<span class="token punctuation">)</span>
            <span class="token comment">## 计算损失函数梯度并进行反向传播</span>
            output<span class="token punctuation">.</span>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment">## 优化器更新</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span><span class="token punctuation">(</span>global_step <span class="token operator">%</span> log_sep <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"ep:</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>ep<span class="token punctuation">}</span></span><span class="token string">,global_step:</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>global_step<span class="token punctuation">}</span></span><span class="token string">,loss:</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>output<span class="token punctuation">.</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
            global_step <span class="token operator">+=</span> <span class="token number">1</span>
        <span class="token comment">## 准确率</span>
        acc <span class="token operator">=</span> evaluate<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">## 第几轮</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"ep:</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>ep<span class="token punctuation">}</span></span><span class="token string">,acc:</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>acc<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

<span class="token comment"># 训练</span>
train<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出3轮训练结果，准确率在88%-89%左右<br> <img src="https://images2.imgbox.com/30/41/4ndyCiOM_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="8_247"></a>步骤8：模型预测</h2> 
<p>完成训练后，利用训练好的模型对新输入的评价进行情感分类。展示模型如何接收新文本，并输出预测结果。</p> 
<pre><code class="prism language-python"><span class="token comment">#sen = "我昨晚在酒店里睡得非常好"</span>
sen <span class="token operator">=</span><span class="token string">"昨天我在酒店睡觉发现被子有一股霉味"</span>

id2label <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token number">0</span><span class="token punctuation">:</span><span class="token string">"差评"</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token string">"好评"</span><span class="token punctuation">}</span>
<span class="token comment">## 将模型设置为评估模式</span>
model<span class="token punctuation">.</span><span class="token builtin">eval</span>

 <span class="token comment">#将训练模型转化为推理模型，模型将使用转换后的推理模式进行评估</span>
<span class="token keyword">with</span> torch<span class="token punctuation">.</span>inference_mode<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment">## 分词&amp;&amp;向量化</span>
    inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>sen<span class="token punctuation">,</span>return_tensors <span class="token operator">=</span> <span class="token string">"pt"</span><span class="token punctuation">)</span>
    <span class="token comment">## GPU加速</span>
    inputs <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>k<span class="token punctuation">:</span>v<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> k<span class="token punctuation">,</span>v <span class="token keyword">in</span> inputs<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
    <span class="token comment">## 进行预测</span>
    logits<span class="token operator">=</span>model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span><span class="token punctuation">.</span>logits
    <span class="token comment">## 在logits的最后一个维度上找到最大值，并返回其所在的索引。这相当于选择模型认为最有可能的类别</span>
    pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"输入：</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>sen<span class="token punctuation">}</span></span><span class="token string"> \n模型的预测结果：</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>id2label<span class="token punctuation">.</span>get<span class="token punctuation">(</span>pred<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</code></pre> 
<p>1）<strong>第1次预测</strong>：（sen =“昨天我在酒店睡觉发现被子有一股霉味”）<br> 输入：昨天我在酒店睡觉发现被子有一股霉味<br> 模型的预测结果：差评<br> 2）<strong>第2次预测</strong>：（sen =“我昨晚在酒店里睡得非常好”）<br> 输入：我昨晚在酒店里睡得非常好<br> 模型的预测结果：好评</p> 
<h2><a id="_278"></a>总结</h2> 
<p>通过上述步骤，我们手工完成了基于Transformer库的情感分类模型预训练流程。虽然这个过程涉及了大量细节和代码编写，但它为我们提供了宝贵的洞见，让我们了解了从原始数据处理到模型训练和验证的整个流程。在后续篇章中，我们将引入更多的Transformer组件，这些高级工具将显著简化我们的开发流程，使我们能够更快捷、更高效地进行模型开发和实验</p> 
<p><img src="https://images2.imgbox.com/51/77/oORRBehm_o.png" alt="在这里插入图片描述"></p> 
<p>🎯🔖更多专栏系列文章：<a href="https://blog.csdn.net/xiaobing259/category_12628007.html?spm=1001.2014.3001.5482"><strong>AIGC-AI大模型探索之路</strong></a></p> 
<blockquote> 
 <p>如果文章内容对您有所触动，别忘了<font color="red"><strong>点赞、⭐关注，收藏</strong></font>！加入我，让我们携手同行AI的探索之旅，一起开启智能时代的大门！</p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/fdf61e6a5333d37d05fa608ee7f8fa4e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Spring boot &#43; dubbo 项目启动报错 ClassNotFoundException WebServerFactoryCustomizer</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b347a6b140a07db68c8a51769e72e36c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">C# Web控件与数据感应之 CheckBoxList 类</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程学习者.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>