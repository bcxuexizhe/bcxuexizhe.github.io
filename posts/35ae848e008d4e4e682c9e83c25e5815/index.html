<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>新测试基准发布，最强开源Llama 3尴尬了 - 编程学习者</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcxuexizhe.github.io/posts/35ae848e008d4e4e682c9e83c25e5815/">
  <meta property="og:site_name" content="编程学习者">
  <meta property="og:title" content="新测试基准发布，最强开源Llama 3尴尬了">
  <meta property="og:description" content="梦晨 发自 凹非寺
量子位 | 公众号 QbitAI 如果试题太简单，学霸和学渣都能考90分，拉不开差距……
随着Claude 3、Llama 3甚至之后GPT-5等更强模型发布，业界急需一款更难、更有区分度的基准测试。
大模型竞技场背后组织LMSYS推出下一代基准测试Arena-Hard，引起广泛关注。
Llama 3的两个指令微调版本实力到底如何，也有了最新参考。
与之前大家分数都相近的MT Bench相比，Arena-Hard区分度从22.6%提升到87.4%，孰强孰弱一目了然。
Arena-Hard利用竞技场实时人类数据构建，与人类偏好一致率也高达89.1%。
除了上面两个指标都达到SOTA之外，还有一个额外的好处：
实时更新的测试数据包含人类新想出的、AI在训练阶段从未见过的提示词，减轻潜在的数据泄露。
并且新模型发布后，无需再等待一周左右时间让人类用户参与投票，只需花费25美元快速运行测试管线，即可得到结果。
有网友评价，使用真实用户提示词而不是高中考试来测试，真的很重要。
新基准测试如何运作？ 简单来说，通过大模型竞技场20万个用户查询中，挑选500个高质量提示词作为测试集。
首先，挑选过程中确保多样性，也就是测试集应涵盖广泛的现实世界话题。
为了确保这一点，团队采用BERTopic中主题建模管道，首先使用OpenAI的嵌入模型（text-embedding-3-small）转换每个提示，使用 UMAP 降低维度，并使用基于层次结构的模型聚类算法 (HDBSCAN) 来识别聚类，最后使用GPT-4-turbo进行汇总。
同时确保入选的提示词具有高质量，有七个关键指标来衡量：
具体性：提示词是否要求特定的输出？
领域知识：提示词是否涵盖一个或多个特定领域？
复杂性：提示词是否有多层推理、组成部分或变量？
解决问题：提示词是否直接让AI展示主动解决问题的能力？
创造力：提示词是否涉及解决问题的一定程度的创造力？
技术准确性：提示词是否要求响应具有技术准确性？
实际应用：提示词是否与实际应用相关？
使用GPT-3.5-Turbo和GPT-4-Turbo对每个提示进行从 0 到 7 的注释，判断满足多少个条件。然后根据提示的平均得分给每个聚类评分。
高质量的问题通常与有挑战性的话题或任务相关，比如游戏开发或数学证明。
新基准测试准吗？ Arena-Hard目前还有一个弱点：使用GPT-4做裁判更偏好自己的输出。官方也给出了相应提示。
可以看出，最新两个版本的GPT-4分数高过Claude 3 Opus一大截，但在人类投票分数中差距并没有那么明显。
其实关于这一点，最近已经有研究论证，前沿模型都会偏好自己的输出。
研究团队还发现，AI天生就可以判断出一段文字是不是自己写的，经过微调后自我识别的能力还能增强，并且自我识别能力与自我偏好线性相关。
那么使用Claude 3来打分会使结果产生什么变化？LMSYS也做了相关实验。
首先，Claude系列的分数确实会提高。
但令人惊讶的是，它更喜欢几种开放模型如Mixtral和零一万物Yi，甚至对GPT-3.5的评分都有明显提高。
总体而言，使用Claude 3打分的区分度和与人类结果的一致性都不如GPT-4。
所以也有很多网友建议，使用多个大模型来综合打分。
除此之外，团队还做了更多消融实验来验证新基准测试的有效性。
比如在提示词中加入“让答案尽可能详尽”，平均输出长度更高，分数确实会提高。
但把提示词换成“喜欢闲聊”，平均输出长度也有提高，但分数提升就不明显。
此外在实验过程中还有很多有意思的发现。
比如GPT-4来打分非常严格，如果回答中有错误会狠狠扣分；而Claude 3即使识别出小错误也会宽大处理。
对于代码问题，Claude 3倾向于提供简单结构、不依赖外部代码库，能帮助人类学习编程的答案；而GPT-4-Turbo更倾向最实用的答案，不管其教育价值如何。
另外即使设置温度为0，GPT-4-Turbo也可能产生略有不同的判断。
从层次结构可视化的前64个聚类中也可以看出，大模型竞技场用户的提问质量和多样性确实是高。
这里面也许就有你的贡献。
Arena-Hard GitHub：">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-22T18:50:17+08:00">
    <meta property="article:modified_time" content="2024-04-22T18:50:17+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程学习者" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程学习者</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">新测试基准发布，最强开源Llama 3尴尬了</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <h6>梦晨 发自 凹非寺<br>量子位 | 公众号 QbitAI</h6> 
 <p style="text-align:left;">如果试题太简单，学霸和学渣都能考90分，拉不开差距……</p> 
 <p style="text-align:left;">随着Claude 3、Llama 3甚至之后GPT-5等更强模型发布，业界急需一款<strong>更难、更有区分度的基准测试</strong>。</p> 
 <p style="text-align:left;">大模型竞技场背后组织LMSYS推出下一代基准测试<strong>Arena-Hard</strong>，引起广泛关注。</p> 
 <p style="text-align:left;">Llama 3的两个指令微调版本实力到底如何，也有了最新参考。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/52/84/hq09sJO7_o.png" alt="af8868c433431a61b099baeff70d3ca6.png"></p> 
 <p style="text-align:left;">与之前大家分数都相近的MT Bench相比，Arena-Hard<strong>区分度从22.6%提升到87.4%</strong>，孰强孰弱一目了然。</p> 
 <p style="text-align:left;">Arena-Hard利用竞技场实时人类数据构建，<strong>与人类偏好一致率也高达89.1%</strong>。</p> 
 <p style="text-align:left;">除了上面两个指标都达到SOTA之外，还有一个额外的好处：</p> 
 <p style="text-align:left;">实时更新的测试数据包含人类新想出的、AI在训练阶段从未见过的提示词，<strong>减轻</strong><strong>潜在的数据泄露</strong>。</p> 
 <p style="text-align:left;">并且新模型发布后，无需再等待一周左右时间让人类用户参与投票，只需花费25美元快速运行测试管线，即可得到结果。</p> 
 <p style="text-align:left;">有网友评价，<strong>使用真实用户提示词而不是高中考试来测试，真的很重要。</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/91/08/944DszUK_o.png" alt="31e040f5e724623b5ff8c7e7f846f7eb.png"></p> 
 <h3>新基准测试如何运作？</h3> 
 <p style="text-align:left;">简单来说，通过大模型竞技场20万个用户查询中，挑选500个高质量提示词作为测试集。</p> 
 <p style="text-align:left;">首先，挑选过程中确保<strong>多样性</strong>，也就是测试集应涵盖广泛的现实世界话题。</p> 
 <p style="text-align:left;">为了确保这一点，团队采用BERTopic中主题建模管道，首先使用OpenAI的嵌入模型（text-embedding-3-small）转换每个提示，使用 UMAP 降低维度，并使用基于层次结构的模型聚类算法 (HDBSCAN) 来识别聚类，最后使用GPT-4-turbo进行汇总。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/75/ea/pYgn9tYm_o.png" alt="53cd49b0d09f9652ac66ebdd9d40bf0e.png"></p> 
 <p style="text-align:left;">同时确保入选的提示词具有<strong>高质量</strong>，有七个关键指标来衡量：</p> 
 <ul><li><p><strong>具体性：</strong>提示词是否要求特定的输出？</p></li><li><p><strong>领域知识：</strong>提示词是否涵盖一个或多个特定领域？</p></li><li><p><strong>复杂性：</strong>提示词是否有多层推理、组成部分或变量？</p></li><li><p><strong>解决问题：</strong>提示词是否直接让AI展示主动解决问题的能力？</p></li><li><p><strong>创造力：</strong>提示词是否涉及解决问题的一定程度的创造力？</p></li><li><p><strong>技术准确性：</strong>提示词是否要求响应具有技术准确性？</p></li><li><p><strong>实际应用：</strong>提示词是否与实际应用相关？</p></li></ul> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/89/91/v5KDvzEE_o.png" alt="733acfc829f4ba618dc19847d4ef913b.png"></p> 
 <p style="text-align:left;">使用GPT-3.5-Turbo和GPT-4-Turbo对每个提示进行从 0 到 7 的注释，判断满足多少个条件。然后根据提示的平均得分给每个聚类评分。</p> 
 <p style="text-align:left;">高质量的问题通常与有挑战性的话题或任务相关，比如游戏开发或数学证明。</p> 
 <p style="text-align:left;"><img src="https://images2.imgbox.com/97/a1/E00jL9kR_o.jpg" alt="d14ca16ce874c0f27881ed24182c8076.jpeg"></p> 
 <h3>新基准测试准吗？</h3> 
 <p style="text-align:left;">Arena-Hard目前还有一个弱点：使用GPT-4做裁判更偏好自己的输出。官方也给出了相应提示。</p> 
 <p style="text-align:left;">可以看出，最新两个版本的GPT-4分数高过Claude 3 Opus一大截，但在人类投票分数中差距并没有那么明显。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/06/50/xGkTWgu1_o.png" alt="938512c408789be3525ea3dbaf52abe9.png"></p> 
 <p style="text-align:left;">其实关于这一点，最近已经有研究论证，<strong>前沿模型都会偏好自己的输出</strong>。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/9c/97/GPVnoHri_o.png" alt="af64b93b2ac5d64d2a9fc49d6cf9f2ba.png"></p> 
 <p style="text-align:left;">研究团队还发现，AI天生就可以判断出一段文字是不是自己写的，经过微调后自我识别的能力还能增强，并且<strong>自我识别能力与自我偏好线性相关</strong>。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/29/9b/jnvpFvCP_o.png" alt="f07382702dab3b4f02ecd64c22e64260.png"></p> 
 <p style="text-align:left;">那么使用Claude 3来打分会使结果产生什么变化？LMSYS也做了相关实验。</p> 
 <p style="text-align:left;">首先，Claude系列的分数确实会提高。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/9f/d8/QJBRs0BD_o.png" alt="b60a8dcfa9364638447274f2710368c3.png"></p> 
 <p style="text-align:left;">但令人惊讶的是，它更喜欢几种开放模型如Mixtral和零一万物Yi，甚至对GPT-3.5的评分都有明显提高。</p> 
 <p style="text-align:left;">总体而言，使用Claude 3打分的区分度和与人类结果的一致性都不如GPT-4。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/d2/c4/gpSAEPcT_o.png" alt="a72db7e326aa8208d291f8f284510863.png"></p> 
 <p style="text-align:left;">所以也有很多网友建议，<strong>使用多个大模型来综合打分</strong>。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/f4/bc/RxWFwDKG_o.png" alt="6a481785a1868a52aadc8aa2f358b66b.png"></p> 
 <p style="text-align:left;">除此之外，团队还做了更多消融实验来验证新基准测试的有效性。</p> 
 <p style="text-align:left;">比如在提示词中加入“让答案尽可能详尽”，平均输出长度更高，分数确实会提高。</p> 
 <p style="text-align:left;">但把提示词换成“喜欢闲聊”，平均输出长度也有提高，但分数提升就不明显。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/0a/a5/yLvXXdHY_o.png" alt="512b3bb2017a1a95a70c9e39b823df40.png"></p> 
 <p style="text-align:left;">此外在实验过程中还有很多有意思的发现。</p> 
 <p style="text-align:left;">比如GPT-4来打分非常严格，如果回答中有错误会狠狠扣分；而Claude 3即使识别出小错误也会宽大处理。</p> 
 <p style="text-align:left;">对于代码问题，Claude 3倾向于提供简单结构、不依赖外部代码库，能帮助人类学习编程的答案；而GPT-4-Turbo更倾向最实用的答案，不管其教育价值如何。</p> 
 <p style="text-align:left;">另外即使设置温度为0，GPT-4-Turbo也可能产生略有不同的判断。</p> 
 <p style="text-align:left;">从层次结构可视化的前64个聚类中也可以看出，大模型竞技场用户的提问质量和多样性确实是高。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/97/5a/YjDzk40R_o.png" alt="c14b3f7de8de3d37f2c65f033800ddc6.png"></p> 
 <p style="text-align:left;">这里面也许就有你的贡献。</p> 
 <p style="text-align:left;">Arena-Hard GitHub：<br>https://github.com/lm-sys/arena-hard<br>Arena-Hard HuggingFace：<br>https://huggingface.co/spaces/lmsys/arena-hard-browser<br>大模型竞技场：<br>https://arena.lmsys.org</p> 
 <p style="text-align:left;">参考链接：<br>[1]https://x.com/lmsysorg/status/1782179997622649330<br>[2]https://lmsys.org/blog/2024-04-19-arena-hard/</p> 
 <p style="text-align:center;">— <strong>完</strong> —</p> 
 <p style="text-align:center;"><strong>点这里👇关注我，记得标星哦～</strong></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e9a1d4c3e0cd268973c44604a9ee9774/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Android studio配置Flutter（看这一篇就够了）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/67003c56a9ff638f9dea25f54d7f68c1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">JavaScript变量及数据类型</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程学习者.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>