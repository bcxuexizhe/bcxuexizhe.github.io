<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>python 队列生产者消费者爬虫 - 编程学习者</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcxuexizhe.github.io/posts/1d7d5d997fd0327babfeb5c6f8bf71fb/">
  <meta property="og:site_name" content="编程学习者">
  <meta property="og:title" content="python 队列生产者消费者爬虫">
  <meta property="og:description" content="当使用Python编写一个基于队列的生产者消费者爬虫时，我们通常会使用threading或multiprocessing模块来处理并发，并使用queue模块来管理数据队列。下面是一个详细的示例，该示例展示了如何使用生产者线程生成URL，消费者线程爬取这些URL的内容。
请注意，这里为了简化示例，我们将不会实际进行网页爬取，而是模拟这个过程。在实际应用中，我们可能需要使用如requests库来发送HTTP请求，并使用如BeautifulSoup或lxml来解析HTML内容。
（1）安装必要的库（如果尚未安装）
bash复制代码 ​ pip install requests beautifulsoup4 （2）示例代码
import threading import queue import time import random from urllib.parse import urljoin from bs4 import BeautifulSoup # 导入BeautifulSoup，但在此示例中不会实际使用 import requests # 导入requests，但在此示例中不会实际发送请求 # 模拟的起始URL和要爬取的网站域名 START_URL = &#39;http://example.com&#39; BASE_DOMAIN = &#39;http://example.com&#39; # 队列，用于在生产者和消费者之间传递URL url_queue = queue.Queue() # 生产者函数，生成并添加URL到队列中 def producer(url_queue, num_urls): print(&#39;Producer started.&#39;) urls_seen = set() urls_to_add = [START_URL] while urls_to_add and num_urls &gt; 0: current_url = urls_to_add.pop(0) if current_url not in urls_seen: urls_seen.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-28T19:17:23+08:00">
    <meta property="article:modified_time" content="2024-05-28T19:17:23+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程学习者" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程学习者</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">python 队列生产者消费者爬虫</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>当使用Python编写一个基于队列的生产者消费者爬虫时，我们通常会使用<code>threading</code>或<code>multiprocessing</code>模块来处理并发，并使用<code>queue</code>模块来管理数据队列。下面是一个详细的示例，该示例展示了如何使用生产者线程生成URL，消费者线程爬取这些URL的内容。</p> 
<p>请注意，这里为了简化示例，我们将不会实际进行网页爬取，而是模拟这个过程。在实际应用中，我们可能需要使用如<code>requests</code>库来发送HTTP请求，并使用如<code>BeautifulSoup</code>或<code>lxml</code>来解析HTML内容。</p> 
<p>（1）安装必要的库（如果尚未安装）</p> 
<pre><code class="language-bash">bash复制代码
​
pip install requests beautifulsoup4</code></pre> 
<p>（2）示例代码</p> 
<pre><code class="language-python">import threading  
import queue  
import time  
import random  
from urllib.parse import urljoin  
from bs4 import BeautifulSoup  # 导入BeautifulSoup，但在此示例中不会实际使用  
import requests  # 导入requests，但在此示例中不会实际发送请求  
  
# 模拟的起始URL和要爬取的网站域名  
START_URL = 'http://example.com'  
BASE_DOMAIN = 'http://example.com'  
  
# 队列，用于在生产者和消费者之间传递URL  
url_queue = queue.Queue()  
  
# 生产者函数，生成并添加URL到队列中  
def producer(url_queue, num_urls):  
    print('Producer started.')  
    urls_seen = set()  
    urls_to_add = [START_URL]  
      
    while urls_to_add and num_urls &gt; 0:  
        current_url = urls_to_add.pop(0)  
        if current_url not in urls_seen:  
            urls_seen.add(current_url)  
            url_queue.put(current_url)  
            num_urls -= 1  
              
            # 模拟从当前URL生成新的URL（这里只是简单地添加了一些随机路径）  
            for _ in range(random.randint(1, 3)):  
                new_path = f"/some/random/path/{random.randint(1, 100)}"  
                new_url = urljoin(BASE_DOMAIN, new_path)  
                urls_to_add.append(new_url)  
      
    print('Producer finished generating', num_urls, 'URLs.')  
  
# 消费者函数，从队列中获取URL并“爬取”内容（模拟）  
def consumer(url_queue):  
    print('Consumer started.')  
    while not url_queue.empty():  
        url = url_queue.get()  
        print(f'Crawling {url}...')  
          
        # 在这里，我们会使用requests发送HTTP请求，并使用BeautifulSoup解析内容  
        # 但为了简化示例，我们只是模拟这个过程  
        time.sleep(random.uniform(0.5, 1.5))  # 模拟网络延迟  
        print(f'Crawled {url}. Content: (Simulated content)')  
          
        # 标记URL为已处理（在实际应用中可能不需要）  
        url_queue.task_done()  
      
    print('Consumer finished.')  
  
# 创建并启动生产者线程  
producer_thread = threading.Thread(target=producer, args=(url_queue, 10))  # 生成10个URL作为示例  
producer_thread.start()  
  
# 创建并启动多个消费者线程  
num_consumers = 3  
consumer_threads = []  
for _ in range(num_consumers):  
    consumer_thread = threading.Thread(target=consumer, args=(url_queue,))  
    consumer_thread.start()  
    consumer_threads.append(consumer_thread)  
  
# 等待所有消费者线程完成  
for t in consumer_threads:  
    t.join()  
  
# 等待生产者线程完成（如果需要的话）  
producer_thread.join()  
  
# 当队列为空时，所有任务都已完成  
print('All tasks completed.')</code></pre> 
<p>这个示例展示了如何使用线程和队列来实现生产者消费者模式。生产者线程生成URL并将其添加到队列中，而消费者线程从队列中获取URL并模拟爬取过程。请注意，由于线程共享内存，因此我们需要小心处理对队列的访问，但Python的<code>queue</code>模块是线程安全的，因此我们可以安全地在多个线程之间传递数据。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/29a674e82d04db81258181f250171335/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">小程序使用vant组件库</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/25507d30d624b72beccd45e9c2f67ec7/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">外包干了3天，技术退步明显.......</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程学习者.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>