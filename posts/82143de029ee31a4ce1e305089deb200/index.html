<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>本地环境运行Llama 3大型模型：可行性与实践指南 - 编程学习者</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcxuexizhe.github.io/posts/82143de029ee31a4ce1e305089deb200/">
  <meta property="og:site_name" content="编程学习者">
  <meta property="og:title" content="本地环境运行Llama 3大型模型：可行性与实践指南">
  <meta property="og:description" content="简介： Llama 是由 Meta（前身为 Facebook）的人工智能研究团队开发并开源的大型语言模型（LLM），它对商业用途开放，对整个人工智能领域产生了深远的影响。继之前发布的、支持4096个上下文的Llama 2模型之后，Meta 进一步推出了性能更卓越的 Meta Llama 3系列语言模型，包括一个8B（80亿参数）模型和一个70B（700亿参数）模型。Llama 3 70B 的性能媲美 Gemini 1.5 Pro，全面超越 Claude 大杯，而 400B&#43; 的模型则有望与 Claude 超大杯和新版 GPT-4 Turbo 掰手腕
在各种测试基准中，Llama 3系列模型展现了其卓越的性能，它们在实用性和安全性评估方面与市场上其他流行的闭源模型相媲美，甚至在某些方面有所超越。Meta Llama 3系列的发布，不仅巩固了其在大型语言模型领域的竞争地位，而且为研究人员、开发者和企业提供了强大的工具，以推动语言理解和生成技术的进一步发展。
项目地址： https://github.com/meta-llama/llama3
llama2和llama3的差异 llama3和GPT4的差异 指标Llama 3GPT-4模型规模70B、400B&#43;100B、175B、500B参数类型TransformerTransformer训练目标Masked Language Modeling、PerplexityMasked Language Modeling、Perplexity训练数据Books、WebTextBooks、WebText性能SOTA（问答、文本摘要、机器翻译等）SOTA（问答、文本摘要、机器翻译等）开源是否 Llama 3 的亮点 面向所有人开放：Meta 通过开源 Llama 3 的轻量版本，让前沿的 AI 技术变得触手可及。无论是开发者、研究人员还是对 AI 技术好奇的小伙伴，都可以自由地探索、创造和实验。 Llama 3 提供了易于使用的 API，方便研究人员和开发者使用。
模型规模大：Llama 3 400B&#43; 模型的参数规模达到了 4000 亿，属于大型语言模型。
即将融入各种应用： Llama 3 目前已经赋能 Meta AI，Meta AI体验地址：https://www.meta.ai/
在 Windows 上使用 Ollama，运行Llama3模型 访问https://ollama.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-23T20:18:33+08:00">
    <meta property="article:modified_time" content="2024-04-23T20:18:33+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程学习者" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程学习者</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">本地环境运行Llama 3大型模型：可行性与实践指南</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><img src="https://images2.imgbox.com/a2/6f/Xp3TbbhH_o.png" alt="llama3.png"></p> 
<h2 id="简介：">简介：</h2> 
<p>Llama 是由 Meta（前身为 Facebook）的人工智能研究团队开发并开源的大型语言模型（LLM），它对商业用途开放，对整个人工智能领域产生了深远的影响。继之前发布的、支持4096个上下文的Llama 2模型之后，Meta 进一步推出了性能更卓越的 Meta Llama 3系列语言模型，包括一个8B（80亿参数）模型和一个70B（700亿参数）模型。Llama 3 70B 的性能媲美 <strong>Gemini 1.5 Pro</strong>，全面超越 Claude 大杯，而 400B+ 的模型则有望与 Claude 超大杯和新版 <strong>GPT-4 Turbo</strong> 掰手腕</p> 
<p>在各种测试基准中，Llama 3系列模型展现了其卓越的性能，它们在实用性和安全性评估方面与市场上其他流行的闭源模型相媲美，甚至在某些方面有所超越。Meta Llama 3系列的发布，不仅巩固了其在大型语言模型领域的竞争地位，而且为研究人员、开发者和企业提供了强大的工具，以推动语言理解和生成技术的进一步发展。</p> 
<h3 id="项目地址：">项目地址：</h3> 
<p><a href="https://github.com/meta-llama/llama3">https://github.com/meta-llama/llama3</a></p> 
<h3 id="llama2和llama3的差异">llama2和llama3的差异</h3> 
<p><img src="https://images2.imgbox.com/a2/4f/52Tl9Hes_o.png" alt="llama3and3diff.webp"></p> 
<h3 id="llama3和gpt4的差异">llama3和GPT4的差异</h3> 
<table><thead><tr><th>指标</th><th>Llama 3</th><th>GPT-4</th></tr></thead><tbody><tr><td>模型规模</td><td>70B、400B+</td><td>100B、175B、500B</td></tr><tr><td>参数类型</td><td>Transformer</td><td>Transformer</td></tr><tr><td>训练目标</td><td>Masked Language Modeling、Perplexity</td><td>Masked Language Modeling、Perplexity</td></tr><tr><td>训练数据</td><td>Books、WebText</td><td>Books、WebText</td></tr><tr><td>性能</td><td>SOTA（问答、文本摘要、机器翻译等）</td><td>SOTA（问答、文本摘要、机器翻译等）</td></tr><tr><td>开源</td><td>是</td><td>否</td></tr></tbody></table> 
<h3 id="llama-3-的亮点">Llama 3 的亮点</h3> 
<ul><li><p>面向所有人开放：Meta 通过开源 Llama 3 的轻量版本，让前沿的 AI 技术变得触手可及。无论是开发者、研究人员还是对 AI 技术好奇的小伙伴，都可以自由地探索、创造和实验。 Llama 3 提供了易于使用的 API，方便研究人员和开发者使用。</p> </li><li><p>模型规模大：Llama 3 400B+ 模型的参数规模达到了 4000 亿，属于大型语言模型。</p> </li><li><p>即将融入各种应用： Llama 3 目前已经赋能 Meta AI，Meta AI体验地址：<a href="https://www.meta.ai/" rel="nofollow">https://www.meta.ai/</a></p> </li></ul> 
<p><img src="https://images2.imgbox.com/c1/dc/nH2g6J2R_o.png" alt="llama3-pre-trained.png"></p> 
<p><img src="https://images2.imgbox.com/99/81/tBQZSXnW_o.png" alt="llama3-8b-70b.webp"></p> 
<p><img src="https://images2.imgbox.com/c9/22/pmQmDLl3_o.png" alt="llam3-15T-tokens.png"></p> 
<h3 id="在-windows-上使用-ollama，运行llama3模型">在 Windows 上使用 Ollama，运行Llama3模型</h3> 
<p>访问<a href="https://ollama.com/download/windows" rel="nofollow">https://ollama.com/download/windows</a>页面，下载<code>OllamaSetup.exe</code>安装程序。</p> 
<p>安装后，根据自身电脑配置，选择对应模型参数安装(运行 7B 至少需要 8GB 内存，运行 13B 至少需要 16GB 内存)</p> 
<blockquote> 
 <p>我这里运行的是Llama3:8b，可以看出，中文还是有点问题</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/94/b9/yKp10qbh_o.png" alt="ollama3.png"></p> 
<table><thead><tr><th>Model</th><th>Parameters</th><th>Size</th><th>Download</th></tr></thead><tbody><tr><td>Llama 3</td><td>8B</td><td>4.7GB</td><td><code>ollama run llama3</code></td></tr><tr><td>Llama 3</td><td>70B</td><td>40GB</td><td><code>ollama run llama3:70b</code></td></tr><tr><td>Mistral</td><td>7B</td><td>4.1GB</td><td><code>ollama run mistral</code></td></tr><tr><td>Dolphin Phi</td><td>2.7B</td><td>1.6GB</td><td><code>ollama run dolphin-phi</code></td></tr><tr><td>Phi-2</td><td>2.7B</td><td>1.7GB</td><td><code>ollama run phi</code></td></tr><tr><td>Neural Chat</td><td>7B</td><td>4.1GB</td><td><code>ollama run neural-chat</code></td></tr><tr><td>Starling</td><td>7B</td><td>4.1GB</td><td><code>ollama run starling-lm</code></td></tr><tr><td>Code Llama</td><td>7B</td><td>3.8GB</td><td><code>ollama run codellama</code></td></tr><tr><td>Llama 2 Uncensored</td><td>7B</td><td>3.8GB</td><td><code>ollama run llama2-uncensored</code></td></tr><tr><td>Llama 2 13B</td><td>13B</td><td>7.3GB</td><td><code>ollama run llama2:13b</code></td></tr><tr><td>Llama 2 70B</td><td>70B</td><td>39GB</td><td><code>ollama run llama2:70b</code></td></tr><tr><td>Orca Mini</td><td>3B</td><td>1.9GB</td><td><code>ollama run orca-mini</code></td></tr><tr><td>LLaVA</td><td>7B</td><td>4.5GB</td><td><code>ollama run llava</code></td></tr><tr><td>Gemma</td><td>2B</td><td>1.4GB</td><td><code>ollama run gemma:2b</code></td></tr><tr><td>Gemma</td><td>7B</td><td>4.8GB</td><td><code>ollama run gemma:7b</code></td></tr><tr><td>Solar</td><td>10.7B</td><td>6.1GB</td><td><code>ollama run solar</code></td></tr></tbody></table> 
<h3 id="hugging-face-使用">Hugging Face 使用</h3> 
<p>访问：<a href="https://huggingface.co/chat/" rel="nofollow">https://huggingface.co/chat/</a> 然后切换<code>Models</code> </p> 
<h3 id="replicate-使用">Replicate 使用</h3> 
<p>8B 模型：<a href="https://replicate.com/meta/meta-llama-3-8b" rel="nofollow">https://replicate.com/meta/meta-llama-3-8b</a></p> 
<p>70B 模型：<a href="https://replicate.com/meta/meta-llama-3-70b" rel="nofollow">https://replicate.com/meta/meta-llama-3-70b</a></p> 
<blockquote> 
 <p>本文由博客一文多发平台 <a href="https://openwrite.cn?from=article_bottom" rel="nofollow">OpenWrite</a> 发布！</p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/acb05b2bc8af2f45cd5e897d1e87597a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【Java--数据结构】模拟实现ArrayList</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/32e5f6575ca50321b9c6278617f14d05/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">LLama的激活函数SwiGLU 解释</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程学习者.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>