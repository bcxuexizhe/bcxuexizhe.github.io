<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>10分钟教你用Python爬取Baidu文库全格式内容，Flutter尽然还能有这种操作 - 编程学习者</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcxuexizhe.github.io/posts/fed182d18aad686df0e93c18c6a9ccc3/">
  <meta property="og:site_name" content="编程学习者">
  <meta property="og:title" content="10分钟教你用Python爬取Baidu文库全格式内容，Flutter尽然还能有这种操作">
  <meta property="og:description" content="driver = webdriver.Chrome(r’F:\driver\chromedriver.exe’)
driver.get(url)
怎么样，是不是浏览器自动打开了?现在我们尝试输出这个driver，就可以看见，网页的正确源代码已经在里面了。
现在我们仔细研究一下源代码就可以看到，我们需要的内容在下面这个位置。
现在正确的源代码也有了，内容的位置也知道了，直接解析，爬取，完事就好了。
想得美，经过这样的爬取之后，对内容进行解析，让我们看看究竟爬到没有。
from lxml import etree
import re
html=etree.HTML(driver.page_source)
links=html.xpath(“//div[@class=‘reader-pic-item’]/@style”)
part = re.compile(r’url([)]&#39;)
qa=“”.join(links)
z=part.findall(qa)
我们可以知道，其实我们只爬到3张PDF，其他的都没有爬到。这是为什么呢？
这是百度文库为了防止大家去爬，专门设置的一个小机关。
返回百度文库，我们仔细看看源代码，其实我们可以发现，随着页面的变化，源代码是不断改变的，每次都只有3张图片的url。并且这个页码数也有一定的规律，如果在第二页，那么图片就是1，2，3，如果在第三页，图片就是2，3，4。
那么我们的疑惑一下就解决了，只需要不断地进行换页的爬取，就可以了。接下来就是如何实现换页的操作了。
这个需要两个步骤，先是点击继续阅读，然后进行页面输入实现换页。先实现点击的操作，代码如下。
button = driver.find_element_by_xpath(“//*[@id=‘html-reader-go-more’]/div[2]/div[1]/span”)
button.click()
driver.execute_script(“arguments[0].click();”, button)
整个操作是通过JS来进行的，大家可以把这个记住，以后需要点击的时候直接用就可以。
然后就是输入页面实现换页，这个其实涉及的比较多，细分的话，步骤分为获取总页数，依次输入页面并点击。
import re
寻找页面 source = re.compile(r’/(.*?)&#39;)
number = int(source.findall(driver.page_source)[0])
输入页面并点击 driver.find_element_by_class_name(“page-input”).clear()
driver.find_element_by_class_name(“page-input”).send_keys(‘2’)
driver.find_element_by_class_name(“page-input”).send_keys(Keys.ENTER)
如果小伙伴成功实现了上面的操作，其实大体的爬取工作已经差不多了，接下来就是保存我们的PPT和PDF了。
因为爬取PDF和PPT的时候，我们是爬取的图片的源地址，那么我们要获得这张图片并保存下来就必须对这个地址发起请求，然后将返回头以二进制保存下来。
for m in range(3):
pic = requests.get(z[m]).content
方法一 file = open(f’./照片/{m&#43;1}.jpg’,‘wb’) file.write(pic) file.close() 方法二 with open(f’./照片/{m&#43;1}.jpg’,‘wb’) as f:
f.write(pic)
f.close()">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-03-22T00:13:03+08:00">
    <meta property="article:modified_time" content="2024-03-22T00:13:03+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程学习者" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程学习者</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">10分钟教你用Python爬取Baidu文库全格式内容，Flutter尽然还能有这种操作</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>driver = webdriver.Chrome(r’F:\driver\chromedriver.exe’)</p> 
<p>driver.get(url)</p> 
<p>怎么样，是不是浏览器自动打开了?现在我们尝试输出这个driver，就可以看见，网页的正确源代码已经在里面了。</p> 
<p>现在我们仔细研究一下源代码就可以看到，我们需要的内容在下面这个位置。</p> 
<p><img src="https://images2.imgbox.com/a9/83/3IVkkD8y_o.png" alt="在这里插入图片描述"></p> 
<p>现在正确的源代码也有了，内容的位置也知道了，直接解析，爬取，完事就好了。</p> 
<p>想得美，经过这样的爬取之后，对内容进行解析，让我们看看究竟爬到没有。</p> 
<p>from lxml import etree</p> 
<p>import re</p> 
<p>html=etree.HTML(driver.page_source)</p> 
<p>links=html.xpath(“//div[@class=‘reader-pic-item’]/@style”)</p> 
<p>part = re.compile(r’url<a href=".*?" rel="nofollow">(</a>[)]')</p> 
<p>qa=“”.join(links)</p> 
<p>z=part.findall(qa)</p> 
<p><img src="https://images2.imgbox.com/6a/92/icnmd5FZ_o.png" alt="在这里插入图片描述"></p> 
<p>我们可以知道，其实我们只爬到3张PDF，其他的都没有爬到。这是为什么呢？</p> 
<p>这是百度文库为了防止大家去爬，专门设置的一个小机关。</p> 
<p>返回百度文库，我们仔细看看源代码，其实我们可以发现，随着页面的变化，源代码是不断改变的，每次都只有3张图片的url。并且这个页码数也有一定的规律，如果在第二页，那么图片就是1，2，3，如果在第三页，图片就是2，3，4。</p> 
<p><img src="https://images2.imgbox.com/5f/7e/vAkh1Q8m_o.png" alt="在这里插入图片描述"></p> 
<p>那么我们的疑惑一下就解决了，只需要不断地进行换页的爬取，就可以了。接下来就是如何实现换页的操作了。</p> 
<p>这个需要两个步骤，先是点击继续阅读，然后进行页面输入实现换页。先实现点击的操作，代码如下。</p> 
<p>button = driver.find_element_by_xpath(“//*[@id=‘html-reader-go-more’]/div[2]/div[1]/span”)</p> 
<p>button.click()</p> 
<p>driver.execute_script(“arguments[0].click();”, button)</p> 
<p>整个操作是通过JS来进行的，大家可以把这个记住，以后需要点击的时候直接用就可以。</p> 
<p>然后就是输入页面实现换页，这个其实涉及的比较多，细分的话，步骤分为获取总页数，依次输入页面并点击。</p> 
<p>import re</p> 
<h2><a id="_97"></a>寻找页面</h2> 
<p>source = re.compile(r’<span class="page-count">/(.*?)</span>')</p> 
<p>number = int(source.findall(driver.page_source)[0])</p> 
<h2><a id="_103"></a>输入页面并点击</h2> 
<p>driver.find_element_by_class_name(“page-input”).clear()</p> 
<p>driver.find_element_by_class_name(“page-input”).send_keys(‘2’)</p> 
<p>driver.find_element_by_class_name(“page-input”).send_keys(Keys.ENTER)</p> 
<p>如果小伙伴成功实现了上面的操作，其实大体的爬取工作已经差不多了，接下来就是保存我们的PPT和PDF了。</p> 
<p><img src="https://images2.imgbox.com/e9/e7/2bhYF98j_o.png" alt="在这里插入图片描述"></p> 
<p>因为爬取PDF和PPT的时候，我们是爬取的图片的源地址，那么我们要获得这张图片并保存下来就必须对这个地址发起请求，然后将返回头以二进制保存下来。</p> 
<p>for m in range(3):</p> 
<p>pic = requests.get(z[m]).content</p> 
<h2><a id="_131"></a>方法一</h2> 
<h2><a id="file__openfm1jpgwb_133"></a>file = open(f’./照片/{m+1}.jpg’,‘wb’)</h2> 
<h2><a id="filewritepic_135"></a>file.write(pic)</h2> 
<h2><a id="fileclose_137"></a>file.close()</h2> 
<h2><a id="_139"></a>方法二</h2> 
<p>with open(f’./照片/{m+1}.jpg’,‘wb’) as f:</p> 
<p>f.write(pic)</p> 
<p>f.close()</p> 
<p>在这里，提醒大家一下一定要按照对图片用正确顺序进行命名，因为后面保存为PDF的时候，需要排序。</p> 
<p>在py文件的目录下，大家就可以看见保存下来的图片了。最后一步，将图片保存为PDF。</p> 
<p>from PIL import Image</p> 
<p>import os</p> 
<p>folderPath = “F:/TEST”</p> 
<p>filename = “test”</p> 
<p>files = os.listdir(folderPath)</p> 
<p>jpgFiles = []</p> 
<p>sources = []</p> 
<p>for file in files:</p> 
<p>if ‘jpg’ in file:</p> 
<p>jpgFiles.append(file)</p> 
<p>tep = []</p> 
<p>for i in jpgFiles:</p> 
<p>ex = i.split(‘.’)</p> 
<p>tep.append(int(ex[0]))</p> 
<p>tep.sort()</p> 
<p>jpgFiles=[folderPath +‘/’+ str(i) + ‘.jpg’ for i in tep]</p> 
<p>output = Image.open(jpgFiles[0])</p> 
<p>jpgFiles.pop(0)</p> 
<p>for file in jpgFiles:</p> 
<p>img = Image.open(file)</p> 
<p>img = img.convert(“P”)</p> 
<p>sources.append(img)</p> 
<p>output.save(f"./{filename}.pdf",“PDF”,save_all=True,append_images=sources)</p> 
<p>最终的结果就是生成了咱们的PDF文件。</p> 
<p><img src="https://images2.imgbox.com/14/1c/5jz1Th7l_o.png" alt="在这里插入图片描述"></p> 
<p>上述的操作看起来很多，很麻烦，其实并不是的。因为大部分的操作都是固定的，大家只需要记熟就可以了。</p> 
<p><a href="" rel="nofollow"></a>完整代码</p> 
<hr> 
<p>import requests</p> 
<p>from selenium import webdriver</p> 
<p>from lxml import etree</p> 
<p>import re</p> 
<p>from selenium.webdriver.common.keys import Keys</p> 
<p>import time</p> 
<p>from PIL import Image</p> 
<p>import os</p> 
<p>from bs4 import BeautifulSoup</p> 
<p>import bs4</p> 
<p>from docx import Document</p> 
<p>import sys</p> 
<p>def getHTMLText(url):</p> 
<p>header = {‘User-agent’: ‘Googlebot’}</p> 
<p>try:</p> 
<p>r = requests.get(url, headers = header, timeout = 30)</p> 
<p>r.raise_for_status()</p> 
<p>r.encoding = ‘gbk’</p> 
<h2><a id="rencoding__rapparent_encoding_269"></a>r.encoding = r.apparent_encoding</h2> 
<p>return r.text</p> 
<p>except:</p> 
<p>return ‘’</p> 
<p>def parse_type(content):</p> 
<p>return re.findall(r"docType.<em>?:.</em>?‘(.*?)’,", content)[0]</p> 
<p>def parse_txt(html):</p> 
<p>plist = []</p> 
<p>soup = BeautifulSoup(html, “html.parser”)</p> 
<p>plist.append(soup.title.string)</p> 
<p>for div in soup.find_all(‘div’, attrs={“class”: “bd doc-reader”}):</p> 
<p>plist.extend(div.get_text().split(‘\n’))</p> 
<p>plist = [c.replace(’ ', ‘’) for c in plist]</p> 
<p>plist = [c.replace(‘\x0c’, ‘’) for c in plist]</p> 
<p>return plist</p> 
<p>def print_docx(plist, filename):</p> 
<p>file = open(filename + ‘.txt’, ‘w’,encoding=‘utf-8’)</p> 
<p>for str in plist:</p> 
<p>file.write(str)</p> 
<p>file.write(‘\n’)</p> 
<p>file.close()</p> 
<p>with open(filename + ‘.txt’, encoding=‘utf-8’) as f:</p> 
<p>docu = Document()</p> 
<p>docu.add_paragraph(f.read())</p> 
<p>docu.save(filename + ‘.docx’)</p> 
<p>def parse_doc(url, folderPath):</p> 
<p>driver = webdriver.Chrome(r’./src/chromedriver.exe’)</p> 
<p>driver.get(url)</p> 
<h2><a id="__span_classmoreBtn_goBtnspan35spanspan_classfc2espanspan_333"></a>找到‘继续阅读’按钮 定位至<span class="moreBtn goBtn">还剩35页未读，<span class="fc2e">继续阅读</span></span></h2> 
<p>button = driver.find_element_by_xpath(“//*[@id=‘html-reader-go-more’]/div[2]/div[1]/span”)</p> 
<h2><a id="_337"></a>按下按钮</h2> 
<p>driver.execute_script(“arguments[0].click();”, button)</p> 
<p>time.sleep(1)</p> 
<p>source = re.compile(r’<span class="page-count">/(.*?)</span>')</p> 
<p>number = int(source.findall(driver.page_source)[0])</p> 
<h2><a id="_347"></a>获取页码数</h2> 
<h2><a id="number__total1_349"></a>number = total[1]</h2> 
<p>time.sleep(1)</p> 
<p>for i in range(2,number):</p> 
<p>driver.find_element_by_class_name(“page-input”).clear()</p> 
<p>driver.find_element_by_class_name(“page-input”).send_keys(f’{i}')</p> 
<p>driver.find_element_by_class_name(“page-input”).send_keys(Keys.ENTER)</p> 
<p>time.sleep(1)</p> 
<p>html=etree.HTML(driver.page_source)</p> 
<h2><a id="picture_365"></a>找到picture容器</h2> 
<p>links=html.xpath(“//div[@class=‘reader-pic-item’]/@style”)</p> 
<h2><a id="url_369"></a>找到图片对应的url</h2> 
<p>part = re.compile(r’url<a href=".*?" rel="nofollow">(</a>[)]')</p> 
<p>qa=“”.join(links)</p> 
<p>z=part.findall(qa)</p> 
<p>if i == 2:</p> 
<p>for m in range(3):</p> 
<p>pic = requests.get(z[m]).content</p> 
<p>with open(f’./照片/{m+1}.jpg’,‘wb’) as f:</p> 
<p>f.write(pic)</p> 
<p>f.close()</p> 
<p>else:</p> 
<p>pic = requests.get(z[2]).content</p> 
<p>with open(f’./照片/{i+1}.jpg’,‘wb’) as f:</p> 
<p>f.write(pic)</p> 
<p>f.close()</p> 
<p>time.sleep(1)</p> 
<p>driver.quit()</p> 
<p>def parse_other(url, folderPath):</p> 
<p>driver = webdriver.Chrome(r’./src/chromedriver.exe’)</p> 
<p>driver.get(url)</p> 
<h2><a id="__span_classmoreBtn_goBtnspan35spanspan_classfc2espanspan_411"></a>找到‘继续阅读’按钮 定位至<span class="moreBtn goBtn">还剩35页未读，<span class="fc2e">继续阅读</span></span></h2> 
<p>button = driver.find_element_by_xpath(“//*[@id=‘html-reader-go-more’]/div[2]/div[1]/span”)</p> 
<h2><a id="_415"></a>按下按钮</h2> 
<p>driver.execute_script(“arguments[0].click();”, button)</p> 
<p>time.sleep(1)</p> 
<p>source = re.compile(r’<span class="page-count">/(.*?)</span>')</p> 
<p>number = int(source.findall(driver.page_source)[0])</p> 
<h2><a id="_425"></a>获取页码数</h2> 
<h2><a id="number__total1_427"></a>number = total[1]</h2> 
<p>time.sleep(1)</p> 
<h2><a id="_431"></a>获取图片</h2> 
<p>for i in range(2,number):</p> 
<p>driver.find_element_by_class_name(“page-input”).clear()</p> 
<p>driver.find_element_by_class_name(“page-input”).send_keys(f’{i}')</p> 
<p>driver.find_element_by_class_name(“page-input”).send_keys(Keys.ENTER)</p> 
<p>time.sleep(1)</p> 
<p>html=etree.HTML(driver.page_source)</p> 
<h2><a id="picturedivclassreaderpicitemstyle_445"></a>找到picture容器"//div[@class=‘reader-pic-item’]/@style"</h2> 
<p>z=html.xpath(‘//div[@class=“ppt-image-wrap”]/img/@src’)</p> 
<h2><a id="printz_449"></a>print(z)</h2> 
<h2><a id="_451"></a>保存图片</h2> 
<p>if i == 2:</p> 
<p>for m in range(3):</p> 
<h4><a id="_458"></a>最后</h4> 
<p>Python崛起并且风靡，因为优点多、应用领域广、被大牛们认可。学习 Python 门槛很低，但它的晋级路线很多，通过它你能进入机器学习、数据挖掘、大数据，CS等更加高级的领域。Python可以做网络应用，可以做科学计算，数据分析，可以做网络爬虫，可以做机器学习、自然语言处理、可以写游戏、可以做桌面应用…Python可以做的很多，你需要学好基础，再选择明确的方向。这里给大家分享一份全套的 Python 学习资料，给那些想学习 Python 的小伙伴们一点帮助！</p> 
<h5><a id="Python_462"></a>👉Python所有方向的学习路线👈</h5> 
<p>Python所有方向的技术点做的整理，形成各个领域的知识点汇总，它的用处就在于，你可以按照上面的知识点去找对应的学习资源，保证自己学得较为全面。</p> 
<p><img src="https://images2.imgbox.com/01/d6/Of2nmh87_o.png" alt=""></p> 
<h5><a id="Python_468"></a>👉Python必备开发工具👈</h5> 
<p>工欲善其事必先利其器。学习Python常用的开发软件都在这里了，给大家节省了很多时间。</p> 
<p><img src="https://images2.imgbox.com/5e/41/4XqWbrDJ_o.png" alt=""></p> 
<h5><a id="Python_476"></a>👉Python全套学习视频👈</h5> 
<p>我们在看视频学习的时候，不能光动眼动脑不动手，比较科学的学习方法是在理解之后运用它们，这时候练手项目就很适合了。</p> 
<p><img src="https://images2.imgbox.com/32/d0/YYuZ6Rxn_o.png" alt=""></p> 
<h5><a id="_484"></a>👉实战案例👈</h5> 
<p>学python就与学数学一样，是不能只看书不做题的，直接看步骤和答案会让人误以为自己全都掌握了，但是碰到生题的时候还是会一筹莫展。</p> 
<p>因此在学习python的过程中一定要记得多动手写代码，教程只需要看一两遍即可。</p> 
<p><img src="https://images2.imgbox.com/31/22/n3v9dMf3_o.png" alt=""></p> 
<h5><a id="_498"></a>👉大厂面试真题👈</h5> 
<p>我们学习Python必然是为了找到高薪的工作，下面这些面试题是来自阿里、腾讯、字节等一线互联网大厂最新的面试资料，并且有阿里大佬给出了权威的解答，刷完这一套面试资料相信大家都能找到满意的工作。</p> 
<p><img src="https://images2.imgbox.com/15/e0/E2AAnvlO_o.png" alt=""></p> 
<p><strong>小编13年上海交大毕业，曾经在小公司待过，也去过华为、OPPO等大厂，18年进入阿里一直到现在。</strong></p> 
<p><strong>深知大多数初中级Python工程师，想要提升技能，往往是自己摸索成长或者是报班学习，但自己不成体系的自学效果低效又漫长，而且极易碰到天花板技术停滞不前！</strong></p> 
<p><strong>因此收集整理了一份《2024年Python爬虫全套学习资料》送给大家，初衷也很简单，就是希望能够帮助到想自学提升又不知道该从何学起的朋友，同时减轻大家的负担。</strong></p> 
<p><strong>由于文件比较大，这里只是将部分目录截图出来，每个节点里面都包含大厂面经、学习笔记、源码讲义、实战项目、讲解视频</strong></p> 
<p><strong>如果你觉得这些内容对你有帮助，可以添加下面V无偿领取！（备注：python）</strong><br> <img src="https://images2.imgbox.com/f2/81/zhlqGDU3_o.png" alt="img"></p> 
<p>的面试资料，并且有阿里大佬给出了权威的解答，刷完这一套面试资料相信大家都能找到满意的工作。</p> 
<p><img src="https://images2.imgbox.com/81/f6/BqeUQbtX_o.png" alt=""></p> 
<p><strong>小编13年上海交大毕业，曾经在小公司待过，也去过华为、OPPO等大厂，18年进入阿里一直到现在。</strong></p> 
<p><strong>深知大多数初中级Python工程师，想要提升技能，往往是自己摸索成长或者是报班学习，但自己不成体系的自学效果低效又漫长，而且极易碰到天花板技术停滞不前！</strong></p> 
<p><strong>因此收集整理了一份《2024年Python爬虫全套学习资料》送给大家，初衷也很简单，就是希望能够帮助到想自学提升又不知道该从何学起的朋友，同时减轻大家的负担。</strong></p> 
<p><strong>由于文件比较大，这里只是将部分目录截图出来，每个节点里面都包含大厂面经、学习笔记、源码讲义、实战项目、讲解视频</strong></p> 
<p><strong>如果你觉得这些内容对你有帮助，可以添加下面V无偿领取！（备注：python）</strong><br> [外链图片转存中…(img-2PchwTD2-1711037572044)]</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e4de5a5b8ba5a568f8fbf0535ab902e0/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【QT入门】 Qt槽函数五种常用写法介绍</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3828acc2b5124f3afcbab85bc54e0ae3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">前端Vue篇之Vue3响应式：Ref和Reactive</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程学习者.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>