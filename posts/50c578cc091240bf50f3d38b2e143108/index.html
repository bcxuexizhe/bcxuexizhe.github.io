<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ollama-python-Python快速部署Llama 3等大型语言模型最简单方法 - 编程学习者</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcxuexizhe.github.io/posts/50c578cc091240bf50f3d38b2e143108/">
  <meta property="og:site_name" content="编程学习者">
  <meta property="og:title" content="ollama-python-Python快速部署Llama 3等大型语言模型最简单方法">
  <meta property="og:description" content="ollama介绍 在本地启动并运行大型语言模型。运行Llama 3、Phi 3、Mistral、Gemma和其他型号。
Llama 3 Meta Llama 3 是 Meta Inc. 开发的一系列最先进的模型，提供8B和70B参数大小（预训练或指令调整）。
Llama 3 指令调整模型针对对话/聊天用例进行了微调和优化，并且在常见基准测试中优于许多可用的开源聊天模型。
安装
pip install ollama
高性价比GPU资源：https://www.ucloud.cn/site/active/gpu.html?ytag=gpu_wenzhang_tongyong_shemei
用法
import ollamaresponse = ollama.chat(model=&#39;llama2&#39;, messages=[ { &#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;Why is the sky blue?&#39;, },])print(response[&#39;message&#39;][&#39;content&#39;])
流式响应
可以通过设置stream=True、修改函数调用以返回 Python 生成器来启用响应流，其中每个部分都是流中的一个对象。
import ollama stream = ollama.chat( model=&#39;llama2&#39;, messages=[{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;Why is the sky blue?&#39;}], stream=True, ) for chunk in stream: print(chunk[&#39;message&#39;][&#39;content&#39;], end=&#39;&#39;, flush=True)
应用程序编程接口
Ollama Python 库的 API 是围绕Ollama REST API设计的">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-30T17:35:48+08:00">
    <meta property="article:modified_time" content="2024-04-30T17:35:48+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程学习者" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程学习者</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ollama-python-Python快速部署Llama 3等大型语言模型最简单方法</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h4 id="bjdsh"><b>ollama介绍</b></h4> 
<p><img src="https://images2.imgbox.com/b8/21/lvyWZfCU_o.jpg" alt="1dc063128ae8cf6bd1fd112dc1ff02fc.jpeg"></p> 
<p>在本地启动并运行大型语言模型。运行Llama 3、Phi 3、Mistral、Gemma和其他型号。</p> 
<h4 id="nba1t">Llama 3</h4> 
<p>Meta Llama 3 是 Meta Inc. 开发的一系列最先进的模型，提供<strong>8B</strong>和<strong>70B</strong>参数大小（预训练或指令调整）。</p> 
<p><img src="https://images2.imgbox.com/0d/47/dECwaIQY_o.jpg" alt="1dfa8ed32e3c40ee89aa47b5ee906f31.jpeg"></p> 
<p>Llama 3 指令调整模型针对对话/聊天用例进行了微调和优化，并且在常见基准测试中优于许多可用的开源聊天模型。</p> 
<p><img src="https://images2.imgbox.com/9a/7a/Z9bKFhOf_o.jpg" alt="0aefe68b6ea00d1b15cd2bcfd7e83f19.jpeg"><img src="https://images2.imgbox.com/d6/a4/NLNFK6Ty_o.jpg" alt="6ae4b7c3fc9f2961ea51a3b63698dea7.jpeg"></p> 
<p><b>安装</b></p> 
<p>pip install ollama</p> 
<p><b>高性价比GPU资源：<a href="https://www.ucloud.cn/site/active/gpu.html?ytag=gpu_wenzhang_tongyong_zhihu" rel="nofollow" style="background-color:rgb(255,255,255);font-size:14px;">https://www.ucloud.cn/site/active/gpu.html?ytag=gpu_wenzhang_tongyong_shemei</a></b></p> 
<p><b>用法</b></p> 
<p>import ollamaresponse = ollama.chat(model='llama2', messages=[ { 'role': 'user', 'content': 'Why is the sky blue?', },])print(response['message']['content'])</p> 
<p><b>流式响应</b></p> 
<p>可以通过设置stream=True、修改函数调用以返回 Python 生成器来启用响应流，其中每个部分都是流中的一个对象。</p> 
<p>import ollama stream = ollama.chat( model='llama2', messages=[{'role': 'user', 'content': 'Why is the sky blue?'}], stream=True, ) for chunk in stream: print(chunk['message']['content'], end='', flush=True)</p> 
<p><b>应用程序编程接口</b></p> 
<p>Ollama Python 库的 API 是围绕Ollama REST API设计的</p> 
<p><b>聊天</b></p> 
<p>ollama.chat(model='llama2',&amp;nbsp;messages=[{'role':&amp;nbsp;'user',&amp;nbsp;'content':&amp;nbsp;'Why&amp;nbsp;is&amp;nbsp;the&amp;nbsp;sky&amp;nbsp;blue?'}])</p> 
<p><b>新增</b></p> 
<p>ollama.generate(model='llama2',&amp;nbsp;prompt='Why&amp;nbsp;is&amp;nbsp;the&amp;nbsp;sky&amp;nbsp;blue?')</p> 
<p><b>列表</b></p> 
<p>ollama.list()</p> 
<p><b>展示</b></p> 
<p>ollama.show('llama2')</p> 
<p><b>创建</b></p> 
<p>modelfile=''' FROM llama2 SYSTEM You are mario from super mario bros. ''' ollama.create(model='example', modelfile=modelfile)</p> 
<p><b>复制</b></p> 
<p>ollama.copy('llama2', 'user/llama2')</p> 
<p><b>删除</b></p> 
<p>ollama.delete('llama2') Pull ollama.pull('llama2') push ollama.push('user/llama2')</p> 
<p><b>嵌入</b></p> 
<p>ollama.embeddings(model='llama2',&amp;nbsp;prompt='The&amp;nbsp;sky&amp;nbsp;is&amp;nbsp;blue&amp;nbsp;because&amp;nbsp;of&amp;nbsp;rayleigh&amp;nbsp;scattering')</p> 
<p><b>定制客户端</b></p> 
<p>可以使用以下字段创建自定义客户端：</p> 
<ul><li>host：要连接的 Ollama 主机</li><li>timeout: 请求超时时间</li></ul> 
<p>from ollama import Client client = Client(host='http://localhost:11434') response = client.chat(model='llama2', messages=[ { 'role': 'user', 'content': 'Why is the sky blue?', }, ])</p> 
<p><b>异步客户端</b></p> 
<p>import asyncio from ollama import AsyncClient async def chat(): message = {'role': 'user', 'content': 'Why is the sky blue?'} response = await AsyncClient().chat(model='llama2', messages=[message]) asyncio.run(chat())</p> 
<p>设置stream=True修改函数以返回 Python 异步生成器：</p> 
<p>import asyncio from ollama import AsyncClient async def chat(): message = {'role': 'user', 'content': 'Why is the sky blue?'} async for part in await AsyncClient().chat(model='llama2', messages=[message], stream=True): print(part['message']['content'], end='', flush=True) asyncio.run(chat())</p> 
<p><b>错误</b></p> 
<p>如果请求返回错误状态或在流式传输时检测到错误，则会引发错误。</p> 
<p>model = 'does-not-yet-exist'try: ollama.chat(model)except ollama.ResponseError as e: print('Error:', e.error)if e.status_code == 404: ollama.pull(model)</p> 
<p><br></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ccf1993ed78dc9a7f739dd7e668b49bd/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【Spring】SpringBoot整合Redis，用Redis实现限流（附Redis解压包）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/02f1757e1fb8253b3c2a88350a946901/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Java的逻辑控制和方法的使用介绍</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程学习者.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>