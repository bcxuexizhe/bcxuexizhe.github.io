<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>网络爬虫——urllib（5） - 编程学习者</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcxuexizhe.github.io/posts/d27c1e32b7a5652c8b01624efef02312/">
  <meta property="og:site_name" content="编程学习者">
  <meta property="og:title" content="网络爬虫——urllib（5）">
  <meta property="og:description" content="前言🍭 ❤️❤️❤️网络爬虫专栏更新中，各位大佬觉得写得不错，支持一下，感谢了！❤️❤️❤️
Python网络爬虫_热爱编程的林兮的博客-CSDN博客
上一篇我们讲解有关ajax的相关案例，下面我们来学习新的关于urllib的知识。
11、URLError\HTTPError🍉 简介：
HTTPError类是URLError类的子类导入的包urllib.error.HTTPError urllib.error.URLErrorhttp错误：http错误是针对浏览器无法连接到服务器而增加出来的错误提示。引导并告诉浏览者该页是哪里出 了问题。通过urllib发送请求的时候，有可能会发送失败，这个时候如果想让你的代码更加的健壮，可以通过try‐ except进行捕获异常，异常有两类，URLError\HTTPError 那我们下面来举一个例子，获取下面页面的网页源码
正常代码：
# 异常 import urllib.request url =&#34;https://blog.csdn.net/m0_63951142/article/details/134013573&#34; headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36&#39; } request = urllib.request.Request(url = url, headers = headers) response = urllib.request.urlopen(request) content =response.read().decode(&#39;utf-8&#39;) print(content) 运行代码： 我们这是不是就爬取成功了，但是如果我们万一有哪些地方出错了，比如url多了一个1，我们就需要添加try‐ except进行捕获异常
# 异常 import urllib.request import urllib.error url =&#34;https://blog.csdn.net/m0_63951142/article/details/1340135731&#34; headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-10-31T19:43:35+08:00">
    <meta property="article:modified_time" content="2023-10-31T19:43:35+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程学习者" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程学习者</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">网络爬虫——urllib（5）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>前言🍭</h2> 
<blockquote> 
 <p style="text-align:center;"> ❤️❤️❤️网络爬虫专栏更新中，各位大佬觉得写得不错，支持一下，感谢了！❤️❤️❤️</p> 
 <p style="text-align:center;"><a href="https://blog.csdn.net/m0_63951142/category_12449453.html" title="Python网络爬虫_热爱编程的林兮的博客-CSDN博客">Python网络爬虫_热爱编程的林兮的博客-CSDN博客</a></p> 
</blockquote> 
<p>上一篇我们讲解有关ajax的相关案例，下面我们来学习新的关于urllib的知识。</p> 
<h3 style="background-color:transparent;">11、URLError\HTTPError🍉</h3> 
<blockquote> 
 <p>简介：</p> 
 <ol><li><span style="color:#fe2c24;">HTTPError类是URLError类的子类</span></li><li>导入的包urllib.error.HTTPError urllib.error.URLError</li><li>http错误：http错误是针对浏览器无法连接到服务器而增加出来的错误提示。引导并告诉浏览者该页是哪里出 了问题。</li><li>通过urllib发送请求的时候，有可能会发送失败，这个时候如果想让你的代码更加的健壮，可以通过try‐ except进行捕获异常，异常有两类，URLError\HTTPError</li></ol> 
</blockquote> 
<p>那我们下面来举一个例子，获取下面页面的网页源码</p> 
<p class="img-center"><img alt="" height="286" src="https://images2.imgbox.com/70/1c/fz9NWrfl_o.png" width="574"></p> 
<p>正常代码：</p> 
<pre><code class="language-python"># 异常
import urllib.request

url ="https://blog.csdn.net/m0_63951142/article/details/134013573"

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'
}

request = urllib.request.Request(url = url, headers = headers)

response = urllib.request.urlopen(request)

content =response.read().decode('utf-8')

print(content)</code></pre> 
<p>运行代码： </p> 
<p class="img-center"><img alt="" height="366" src="https://images2.imgbox.com/4a/c3/426rGrpW_o.png" width="1200"></p> 
<p>我们这是不是就爬取成功了，但是如果我们万一有哪些地方出错了，比如url多了一个<span style="color:#fe2c24;">1</span><span style="color:#0d0016;">，我们就需要添加</span>try‐ except进行捕获异常</p> 
<pre><code class="language-python"># 异常
import urllib.request
import urllib.error

url ="https://blog.csdn.net/m0_63951142/article/details/1340135731"

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'
}

try:
    request = urllib.request.Request(url = url, headers = headers)
    
    response = urllib.request.urlopen(request)
    
    content =response.read().decode('utf-8')
    
    print(content)
except urllib.error.HTTPError:
    print('系统正在升级！')</code></pre> 
<p>我们这代码url错误，可以提供<span style="color:#0d0016;">添加</span>try‐ except进行捕获异常</p> 
<p>运行结果：</p> 
<p class="img-center"><img alt="" height="174" src="https://images2.imgbox.com/70/87/NygDzk3e_o.png" width="1130"></p> 
<p> 那么什么时候报URLError呢？一般是主机地址和参数地址错误</p> 
<pre><code class="language-python"># 异常
import urllib.request
import urllib.error

# url ="https://blog.csdn.net/m0_63951142/article/details/1340135731"
url = "https://www.linxi.com"

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'
}

try:
    request = urllib.request.Request(url = url, headers = headers)

    response = urllib.request.urlopen(request)

    content =response.read().decode('utf-8')

    print(content)
except urllib.error.HTTPError:
    print('系统正在升级！')</code></pre> 
<p>运行结果： </p> 
<p class="img-center"><img alt="" height="342" src="https://images2.imgbox.com/38/1e/uREZ4u9l_o.png" width="1200"></p> 
<p>这时候就会报URLError了</p> 
<p>那我们一样<span style="color:#0d0016;">添加</span>try‐ except进行捕获异常：</p> 
<pre><code class="language-python"># 异常
import urllib.request
import urllib.error

# url ="https://blog.csdn.net/m0_63951142/article/details/1340135731"
url = "https://www.linxi.com"

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'
}

try:
    request = urllib.request.Request(url = url, headers = headers)

    response = urllib.request.urlopen(request)

    content =response.read().decode('utf-8')

    print(content)
except urllib.error.HTTPError:
    print('系统正在升级！')
except urllib.error.URLError:
    print('系统真的在升级！！！！')</code></pre> 
<p> 运行结果：</p> 
<p class="img-center"><img alt="" height="181" src="https://images2.imgbox.com/4c/7f/IEnN1uTk_o.png" width="1200"></p> 
<h3 style="background-color:transparent;">12、cookie登录🍉</h3> 
<p>那什么是cookie登录呢？</p> 
<blockquote> 
 <p>在适用的场景下，数据采集的时候 需要绕过登陆 然后进入到某个页面</p> 
</blockquote> 
<p> 我们打算去获取下面这个页面的源码</p> 
<p class="img-center"><img alt="" height="815" src="https://images2.imgbox.com/6e/96/0lDbpeQl_o.png" width="1200"></p> 
<p>代码： </p> 
<pre><code class="language-python"># 适用的场景：数据采集的时候 需要绕过登陆 然后进入到某个页面

import urllib.request

url = 'https://weibo.cn/6451491586/info'

headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                  'Chrome/92.0.4515.159 Safari/537.36',
}
# 请求对象的定制
request = urllib.request.Request(url=url, headers=headers)
# 模拟浏览器向服务器发送请求
response = urllib.request.urlopen(request)
# 获取响应的数据
content = response.read().decode('utf-8')

# 将数据保存到本地
with open('weibo.html', 'w', encoding='utf-8') as fp:
    fp.write(content)
</code></pre> 
<p>运行结果： </p> 
<p><img alt="" height="246" src="https://images2.imgbox.com/8f/ef/hxOUzKBE_o.png" width="1200"></p> 
<p>发现报错了，提示字符编码不对，不是utf-8，难度页面字符编码不是utf-8吗？</p> 
<p>这是因为这是一个经典的反爬手段（个人信息页面是utf-8  但是还报错了编码错误 ，因为并没有进入到个人信息页面 而是跳转到了登陆页面 ）</p> 
<p class="img-center"><img alt="" height="976" src="https://images2.imgbox.com/8a/3c/95nk8gH1_o.png" width="1200"></p> 
<p>可以看到这不是utf-8，所以我们需要去修改我们的代码，获取的响应数据时的decode应该设置为（<span style="color:#fe2c24;">gb2312</span>） </p> 
<pre><code class="language-python">import urllib.request

url = 'https://weibo.cn/6451491586/info'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'
}
# 请求对象的定制
request = urllib.request.Request(url=url, headers=headers)
# 模拟浏览器向服务器发送请求
response = urllib.request.urlopen(request)
# 获取响应的数据
content = response.read().decode('gb2312')

# 将数据保存到本地
with open('weibo.html', 'w', encoding='gb2312') as fp:
    fp.write(content)</code></pre> 
<p>我们继续运行代码，就将源码下载下来了：</p> 
<p class="img-center"><img alt="" height="737" src="https://images2.imgbox.com/38/91/7Yflg8QS_o.png" width="1200"></p> 
<p>在浏览器打开html页面，但是它会一直在转圈圈：</p> 
<p class="img-center"><img alt="" height="256" src="https://images2.imgbox.com/03/4d/py8o11sd_o.png" width="585"></p> 
<p>这是因为什么呢？ 因为请求头的信息不够  所以访问不成功</p> 
<p class="img-center"><img alt="" height="930" src="https://images2.imgbox.com/72/81/3bszBHm0_o.png" width="1200"></p> 
<p>所以我们在Request Headers中需要添加新的信息：</p> 
<pre><code class="language-python"># 适用的场景：数据采集的时候 需要绕过登陆 然后进入到某个页面
# 个人信息页面是utf-8  但是还报错了编码错误  因为并没有进入到个人信息页面 而是跳转到了登陆页面
# 那么登陆页面不是utf-8  所以报错

# 什么情况下访问不成功？
# 因为请求头的信息不够  所以访问不成功

import urllib.request

url = 'https://weibo.cn/6451491586/info'

headers = {
    # 带冒号的不好使
    # ':authority': 'weibo.cn',
    # ':method': 'GET',
    # ':path': '/6451491586/info',
    # ':scheme': 'https',
    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,'
              'application/signed-exchange;v=b3;q=0.9',
    # 'accept-encoding': 'gzip, deflate, br',
    'accept-language': 'zh-CN,zh;q=0.9',
    'cache-control': 'max-age=0',
    #     cookie中携带着你的登陆信息   如果有登陆之后的cookie  那么我们就可以携带着cookie进入到任何页面
    'Cookie':
        '_T_WM=c30cd9c6bbd4e3f6963240e4ec5927e6; '
        'SCF=AmeKosmGUnLyr9H5qopjdzxVakJQ0XnKcsbBtXbpbfngNvC68bT8XtEFYNSLcmIZq5SekJex9dp6Cp7ElZCvRiA.; '
        'SUB=_2A25IRKWMDeRhGeFG71cU9SfLzjyIHXVrxsvErDV6PUJbktAGLUf_kW1NeWNVsDeUrMMUB6xCyXlFfJTZ01NU-9X5; '
        'SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WWIIQ848qLW89oxupijIfuB5JpX5K-hUgL.FoMRSh-fSK.NSK52dJLoIpRLxK'
        '-L1hqLBoMLxK-L1h'
        '-LB--LxK.L1-zLB-2peoet; SSOLoginState=1698747869; ALF=1701339869',
    # referer  判断当前路径是不是由上一个路径进来的    一般情况下 是做图片防盗链
    'referer': 'https://weibo.cn/',
    'sec-ch-ua': '"Chromium";v="92", " Not A;Brand";v="99", "Google Chrome";v="92"',
    'sec-ch-ua-mobile': '?0',
    'sec-fetch-dest': 'document',
    'sec-fetch-mode': 'navigate',
    'sec-fetch-site': 'same-origin',
    'sec-fetch-user': '?1',
    'upgrade-insecure-requests': '1',
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                  'Chrome/92.0.4515.159 Safari/537.36',
}
# 请求对象的定制
request = urllib.request.Request(url=url, headers=headers)
# 模拟浏览器向服务器发送请求
response = urllib.request.urlopen(request)
# 获取响应的数据
content = response.read().decode('utf-8')

# 将数据保存到本地
with open('weibo.html', 'w', encoding='utf-8') as fp:
    fp.write(content)
</code></pre> 
<p>注：字符编码需要重新修改为utf-8</p> 
<p>运行成功，我们继续在浏览器中打开wbibo.html:</p> 
<p><img alt="" height="430" src="https://images2.imgbox.com/3e/92/RiuIFhFn_o.png" width="1200"></p> 
<p>成了！ </p> 
<p class="img-center"><img alt="" height="1029" src="https://images2.imgbox.com/f9/39/yBiaK3QR_o.png" width="1200"></p> 
<p>通过以上步骤，我们就可以实现爬虫cookie登录，从而获取目标网站的数据。需要注意的是，不同的网站可能有不同的登录机制，因此在实际操作时需要根据具体情况进行调整。</p> 
<p style="text-align:center;"> </p> 
<p class="img-center"><img alt="" height="302" src="https://images2.imgbox.com/31/86/ks8eyH0S_o.gif" width="537"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c75e785bcfd1432618f420535d138d1a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">2023-10 最新jsonwebtoken-jjwt 0.12.3 基本使用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7d4dc52566797b9d2c11f5b147bcea1b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【Mysql】WITH AS 语法详解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程学习者.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>