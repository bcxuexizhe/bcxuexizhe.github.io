<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【大数据】HDFS、HBase操作教程（含指令和JAVA API） - 编程学习者</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcxuexizhe.github.io/posts/840fc4ce5a88b2ff7ce14315a42efdde/">
  <meta property="og:site_name" content="编程学习者">
  <meta property="og:title" content="【大数据】HDFS、HBase操作教程（含指令和JAVA API）">
  <meta property="og:description" content="目录
1.前言
2.HDFS
2.1.指令操作
2.2.JAVA API
3.HBase
3.1.指令操作
3.2.JAVA API
1.前言 本文是作者大数据专栏系列的其中一篇，前文中已经详细聊过分布式文件系统HDFS和分布式数据库HBase了，本文将会是它们的实操讲解。
HDFS相关前文：
【大数据】分布式文件系统HDFS-CSDN博客
【大数据】大数据概论与Hadoop_大数据导论与hadoop-CSDN博客
HBase相关前文：
【大数据】分布式数据库HBase-CSDN博客
【大数据】分布式数据库HBase下载安装教程-CSDN博客
2.HDFS 2.1.指令操作 创建目录：
hdfs dfs -mkdir /user/mydir
递归创建目录：
hdfs dfs -mkdir -p /user/mydir/subdir
上传文件到HDFS：
hdfs dfs -put localfile.txt /user/mydir/
下载文件到本地：
hdfs dfs -get /user/mydir/file.txt localdir/
删除文件：
hdfs dfs -rm /user/mydir/file.txt
递归删除目录：
hdfs dfs -rm -r /user/mydir
查看目录内容：
hdfs dfs -ls /user/mydir
递归查看目录内容：
hdfs dfs -lsr /user/mydir
查看文件详细信息：
hdfs dfs -stat /user/mydir/file.txt">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-12T14:20:16+08:00">
    <meta property="article:modified_time" content="2024-05-12T14:20:16+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程学习者" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程学习者</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【大数据】HDFS、HBase操作教程（含指令和JAVA API）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="%E2%80%8B%E7%BC%96%E8%BE%91"><img alt="" height="98" src="https://images2.imgbox.com/98/10/9oNLafpz_o.png" width="595"></p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="1.%E5%89%8D%E8%A8%80-toc" style="margin-left:0px;"><a href="#1.%E5%89%8D%E8%A8%80" rel="nofollow">1.前言</a></p> 
<p id="2.HDFS-toc" style="margin-left:0px;"><a href="#2.HDFS" rel="nofollow">2.HDFS</a></p> 
<p id="2.1.%E6%8C%87%E4%BB%A4%E6%93%8D%E4%BD%9C-toc" style="margin-left:40px;"><a href="#2.1.%E6%8C%87%E4%BB%A4%E6%93%8D%E4%BD%9C" rel="nofollow">2.1.指令操作</a></p> 
<p id="2.2.JAVA%20API-toc" style="margin-left:40px;"><a href="#2.2.JAVA%20API" rel="nofollow">2.2.JAVA API</a></p> 
<p id="3.HBase-toc" style="margin-left:0px;"><a href="#3.HBase" rel="nofollow">3.HBase</a></p> 
<p id="3.1.%E6%8C%87%E4%BB%A4%E6%93%8D%E4%BD%9C-toc" style="margin-left:40px;"><a href="#3.1.%E6%8C%87%E4%BB%A4%E6%93%8D%E4%BD%9C" rel="nofollow">3.1.指令操作</a></p> 
<p id="3.2.JAVA%20API-toc" style="margin-left:40px;"><a href="#3.2.JAVA%20API" rel="nofollow">3.2.JAVA API</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="1.%E5%89%8D%E8%A8%80">1.前言</h2> 
<p>本文是作者大数据专栏系列的其中一篇，前文中已经详细聊过分布式文件系统HDFS和分布式数据库HBase了，本文将会是它们的实操讲解。</p> 
<p>HDFS相关前文：</p> 
<p><a href="https://bugman.blog.csdn.net/article/details/137846603?spm=1001.2014.3001.5502" rel="nofollow" title="【大数据】分布式文件系统HDFS-CSDN博客">【大数据】分布式文件系统HDFS-CSDN博客</a></p> 
<p><a href="https://bugman.blog.csdn.net/article/details/137527825?spm=1001.2014.3001.5502" rel="nofollow" title="【大数据】大数据概论与Hadoop_大数据导论与hadoop-CSDN博客">【大数据】大数据概论与Hadoop_大数据导论与hadoop-CSDN博客</a></p> 
<p>HBase相关前文：</p> 
<p><a href="https://bugman.blog.csdn.net/article/details/138218730?spm=1001.2014.3001.5502" rel="nofollow" title="【大数据】分布式数据库HBase-CSDN博客">【大数据】分布式数据库HBase-CSDN博客</a></p> 
<p><a href="https://bugman.blog.csdn.net/article/details/138566986?spm=1001.2014.3001.5502" rel="nofollow" title="【大数据】分布式数据库HBase下载安装教程-CSDN博客">【大数据】分布式数据库HBase下载安装教程-CSDN博客</a></p> 
<h2 id="2.HDFS">2.HDFS</h2> 
<h3 id="2.1.%E6%8C%87%E4%BB%A4%E6%93%8D%E4%BD%9C">2.1.指令操作</h3> 
<p>创建目录：</p> 
<blockquote> 
 <p>hdfs dfs -mkdir /user/mydir</p> 
</blockquote> 
<p>递归创建目录：</p> 
<blockquote> 
 <p>hdfs dfs -mkdir -p /user/mydir/subdir</p> 
</blockquote> 
<p>上传文件到HDFS：</p> 
<blockquote> 
 <p>hdfs dfs -put localfile.txt /user/mydir/</p> 
</blockquote> 
<p>下载文件到本地：</p> 
<blockquote> 
 <p>hdfs dfs -get /user/mydir/file.txt localdir/</p> 
</blockquote> 
<p>删除文件：</p> 
<blockquote> 
 <p>hdfs dfs -rm /user/mydir/file.txt</p> 
</blockquote> 
<p>递归删除目录：</p> 
<blockquote> 
 <p>hdfs dfs -rm -r /user/mydir</p> 
</blockquote> 
<p>查看目录内容：</p> 
<blockquote> 
 <p>hdfs dfs -ls /user/mydir</p> 
</blockquote> 
<p>递归查看目录内容：</p> 
<blockquote> 
 <p>hdfs dfs -lsr /user/mydir</p> 
</blockquote> 
<p>查看文件详细信息：</p> 
<blockquote> 
 <p>hdfs dfs -stat /user/mydir/file.txt</p> 
</blockquote> 
<p>移动或重命名文件：</p> 
<blockquote> 
 <p>hdfs dfs -mv /user/mydir/file.txt /user/mydir/newfile.txt</p> 
</blockquote> 
<p>复制文件、目录：</p> 
<blockquote> 
 <p>hdfs dfs -cp /user/mydir/file.txt /user/mydir2/</p> 
</blockquote> 
<p>查看文件内容：</p> 
<p>hdfs dfs -cat /user/mydir/file.txt</p> 
<h3 id="2.2.JAVA%20API">2.2.JAVA API</h3> 
<p>首先这里有个巨坑：</p> 
<p>一定要把core-site.xml里面的fs.defaultFS换成真实IP地址，不能用localhsot</p> 
<pre><code class="language-XML">&lt;configuration
        &lt;property&gt;
                &lt;name&gt;hadoop.tmp.version&lt;/name&gt;
                &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
                &lt;name&gt;fs.defaultFS&lt;/name&gt;
                &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
        &lt;/property&gt;
&lt;/configuration&gt;</code></pre> 
<p>如果JAVA API的client端会先找HDFS拿到fs.defaultFS，然后再去访问拿到的地址上的HDFS，如果JAVA API的client端和HDFS不在一台机器上，JAVA API的Client就会去访问它本地的localhost的9000端口上的服务，会直接报错：</p> 
<p>Connection refused: no further information</p> 
<p>依赖：</p> 
<blockquote> 
 <pre>&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
    &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
    &lt;version&gt;3.1.3&lt;/version&gt;
&lt;/dependency&gt;</pre> 
</blockquote> 
<p>代码示例：</p> 
<pre><code class="language-java">import java.io.*;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;
​
public class HDFSSample {
​
    public static void main(String[] args) throws IOException {
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(conf);
​
        // 创建目录
        createDirectory(fs, "/user/hadoop/testdir");
​
        // 上传文件
        uploadFile(fs, "/user/hadoop/testfile.txt", "C:/localfile.txt");
​
        // 下载文件
        downloadFile(fs, "/user/hadoop/testfile.txt", "C:/downloadedfile.txt");
​
        // 列出目录内容
        listDirectory(fs, "/user/hadoop");
​
        // 删除文件
        deleteFile(fs, "/user/hadoop/testfile.txt");
​
        // 删除目录
        deleteDirectory(fs, "/user/hadoop/testdir");
​
        // 关闭文件系统
        fs.close();
    }
​
    private static void createDirectory(FileSystem fs, String dirPath) throws IOException {
        fs.mkdirs(new Path(dirPath));
        System.out.println("Directory created: " + dirPath);
    }
​
    private static void uploadFile(FileSystem fs, String hdfsPath, String localFilePath) throws IOException {
        Path hdfsPathObj = new Path(hdfsPath);
        Path localPathObj = new Path(localFilePath);
        fs.copyFromLocalFile(false, true, localPathObj, hdfsPathObj);
        System.out.println("File uploaded: " + localFilePath + " to " + hdfsPath);
    }
​
    private static void downloadFile(FileSystem fs, String hdfsPath, String localFilePath) throws IOException {
        Path hdfsPathObj = new Path(hdfsPath);
        Path localPathObj = new Path(localFilePath);
        fs.copyToLocalFile(true, hdfsPathObj, localPathObj);
        System.out.println("File downloaded: " + hdfsPath + " to " + localFilePath);
    }
​
    private static void listDirectory(FileSystem fs, String dirPath) throws IOException {
        for (FileStatus file : fs.listStatus(new Path(dirPath))) {
            System.out.println("File/Directory: " + file.getPath().toString());
        }
    }
​
    private static void deleteFile(FileSystem fs, String filePath) throws IOException {
        Path filePathObj = new Path(filePath);
        if (fs.exists(filePathObj)) {
            fs.delete(filePathObj, false);
            System.out.println("File deleted: " + filePath);
        } else {
            System.out.println("File not found: " + filePath);
        }
    }
​
    private static void deleteDirectory(FileSystem fs, String dirPath) throws IOException {
        Path dirPathObj = new Path(dirPath);
        if (fs.exists(dirPathObj)) {
            fs.delete(dirPathObj, true);
            System.out.println("Directory deleted: " + dirPath);
        } else {
            System.out.println("Directory not found: " + dirPath);
        }
    }
}</code></pre> 
<h2 id="3.HBase">3.HBase</h2> 
<h3 id="3.1.%E6%8C%87%E4%BB%A4%E6%93%8D%E4%BD%9C">3.1.指令操作</h3> 
<p>创建一个列族为info的student表：</p> 
<blockquote> 
 <p>create 'Student', 'info'</p> 
</blockquote> 
<p>往表里插数据：</p> 
<blockquote> 
 <p>put 'Student', '1', 'info:id', '1'</p> 
 <p>put 'Student', '1', 'info:name', 'Alice' put 'Student', '1', 'info:age', '20'</p> 
 <p>put 'Student', '1', 'info:major', 'Computer Science'</p> 
 <p>put 'Student', '2', 'info:id', '2'</p> 
 <p>put 'Student', '2', 'info:name', 'Bob' put 'Student', '2', 'info:age', '21'</p> 
 <p>put 'Student', '2', 'info:major', 'Mathematics'</p> 
</blockquote> 
<p>查询单个：</p> 
<p>get 'Student', '1'</p> 
<p style="text-align:center;"><img alt="" height="101" src="https://images2.imgbox.com/1f/ef/bV8xnz53_o.png" width="613"></p> 
<p>查询批量：</p> 
<p>scan 'Student'</p> 
<p style="text-align:center;"><img alt="" height="119" src="https://images2.imgbox.com/95/8d/1nGiT7bL_o.png" width="614"></p> 
<p>条件批量查询：</p> 
<p>scan 'Student', {FILTER =&gt; "SingleColumnValueFilter('info','age', &gt;=, 'binary:20')"}</p> 
<p style="text-align:center;"><img alt="" height="121" src="https://images2.imgbox.com/5f/f2/UcYbEo00_o.png" width="610"></p> 
<p>在HBase中，Scan对象用于定义在表上进行扫描时的参数，包括哪些行和列需要被检索，以及如何处理这些数据。Filter是Scan的一部分，用于在服务器端对返回的数据进行过滤，以减少网络传输的数据量，提高查询效率。 Filter类提供了一种方式来指定复杂的过滤逻辑，允许你基于行键（Row Key）、列族、列限定符和时间戳来筛选结果。以下是一些常见的Filter类型及其用法：</p> 
<ul><li> <p>RowFilter： 用于基于行键的比较，如RowFilter(=, 'binary:rowKey')，匹配特定的行键。</p> </li><li> <p>SingleColumnValueFilter： 用于基于列族和列限定符的值进行比较，如SingleColumnValueFilter('cf', 'qualifier', CompareOp.GREATER_OR_EQUAL,BinaryComparator.valueOf(Bytes.toBytes(20)))，匹配特定列族和列限定符的值大于或等于给定值的行。</p> </li><li> <p>PrefixFilter： 用于匹配以特定前缀开头的行键，如PrefixFilter(Bytes.toBytes('row-prefix'))。</p> </li><li> <p>RegexStringComparator： 用于基于正则表达式匹配行键，如RowFilter(CompareOp.EQUAL, RegexStringComparator('.<em>pattern.</em>'))。</p> </li><li> <p>MultipleColumnPrefixFilter： 用于匹配具有相同前缀的多个列，如MultipleColumnPrefixFilter(Bytes.toBytes('col-prefix'))。</p> </li><li> <p>PageFilter： 用于限制返回结果的数量，这对于大数据量的扫描很有用，如PageFilter(pageSize)，pageSize是你希望一次返回的最大行数。</p> </li><li> <p>TimestampsFilter： 用于指定返回的行必须包含特定时间戳范围内的版本，如TimestampsFilter(timestamps)，timestamps是一个包含多个时间戳的列表。</p> </li><li> <p>ValueFilter 和 QualifierFilter： 分别基于列值和列限定符进行过滤。</p> </li></ul> 
<p>使用不同类型的过滤器的指令示例：</p> 
<blockquote> 
 <p>RowFilter（基于行键过滤）</p> 
 <p>scan 'Student', {FILTER =&gt; "RowFilter(=, 'regexstring:^1')"}</p> 
 <p>SingleColumnValueFilter（基于特定列的值过滤）</p> 
 <p>scan 'Student', {FILTER =&gt; "SingleColumnValueFilter ('info', 'age', &gt;=, 'binary:20')"}</p> 
 <p>PrefixFilter（基于列前缀过滤）</p> 
 <p>scan 'Student', {FILTER =&gt; "PrefixFilter(Bytes.toBytes('info'))"}</p> 
 <p>RegexStringComparator（基于列值的正则表达式过滤）</p> 
 <p>scan 'Student', {FILTER =&gt; "RowFilter(=, 'regexstring:.<em>Alice.</em>')"}</p> 
 <p>MultipleColumnPrefixFilter（基于多列前缀过滤）</p> 
 <p>scan 'Student', {FILTER =&gt; "MultipleColumnPrefixFilter(Bytes.toBytes('info'))"}</p> 
 <p>ValueFilter（基于列值的比较过滤）</p> 
 <p>scan 'Student', {FILTER =&gt; "ValueFilter(=, 'binary:Alice')"}</p> 
 <p>QualifierFilter（基于列限定符的比较过滤）</p> 
 <p>scan 'Student', {FILTER =&gt; "QualifierFilter(=, 'binary:age')"}</p> 
</blockquote> 
<p>清理表：</p> 
<blockquote> 
 <p>delete 'Student', '1' delete 'Student', '2' delete 'Student', '3' disable 'Student' drop 'Student'</p> 
</blockquote> 
<h3 id="3.2.JAVA%20API">3.2.JAVA API</h3> 
<p><span style="background-color:#ffd900;">HBase也要注意和HDFS中相似的问题，hbase-site.xml中也要用真实的IP地址，不然JAVA API的Client端和HBase不在一台机器上的会，就会访问不到HBase，下面的代码中作为演示代码并没有用真实IP，仍然用的LocalHost，这点要注意。</span></p> 
<p>依赖：</p> 
<blockquote> 
 <p>&lt;dependency&gt;<br>     &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;<br>     &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;<br>     &lt;version&gt;2.2.2&lt;/version&gt;<br> &lt;/dependency&gt;<br>  </p> 
</blockquote> 
<p>代码示例：</p> 
<pre><code class="language-java">import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;
import org.apache.hadoop.hbase.client.Delete;
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.Table;
import org.apache.hadoop.hbase.util.Bytes;

public class HBaseExample {

    public static void main(String[] args) {
        Configuration config = HBaseConfiguration.create();
        config.set("hbase.zookeeper.quorum", "localhost"); // 设置ZooKeeper地址
        config.set("hbase.zookeeper.property.clientPort", "2181"); // 设置ZooKeeper端口

        try (Connection connection = ConnectionFactory.createConnection(config);
             Table table = connection.getTable(TableName.valueOf("students"))) {

            // 创建表
            table.createIfNotExists();

            // 插入数据
            Put put1 = new Put(Bytes.toBytes("student1"));
            put1.addColumn(Bytes.toBytes("info"), Bytes.toBytes("name"), Bytes.toBytes("Alice"));
            put1.addColumn(Bytes.toBytes("info"), Bytes.toBytes("age"), Bytes.toBytes("20"));
            put1.addColumn(Bytes.toBytes("info"), Bytes.toBytes("major"), Bytes.toBytes("CS"));
            table.put(put1);

            Put put2 = new Put(Bytes.toBytes("student2"));
            put2.addColumn(Bytes.toBytes("info"), Bytes.toBytes("name"), Bytes.toBytes("Bob"));
            put2.addColumn(Bytes.toBytes("info"), Bytes.toBytes("age"), Bytes.toBytes("21"));
            put2.addColumn(Bytes.toBytes("info"), Bytes.toBytes("major"), Bytes.toBytes("Math"));
            table.put(put2);

            // 查询数据
            Get get = new Get(Bytes.toBytes("student1"));
            Result result = table.get(get);
            System.out.println("Name: " + Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("name"))));
            System.out.println("Age: " + Bytes.toInt(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("age"))));
            System.out.println("Major: " + Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("major"))));

            // 根据条件删除数据
            Delete delete = new Delete(Bytes.toBytes("student1"));
            table.delete(delete);

        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}
</code></pre> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/679693459243869c5ce55c25a1da235d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【Python单点知识】深入理解与应用类多态</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9b9f791dc10c297c096542d6d7ebcc1a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">探索数据结构：树与二叉树</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程学习者.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>